[{"title":"Zhehao Wang","href":"/about","content":"Hi there My name is Zhehao My fulltime job is to develop software that trade stocks in a company based in New York City Ive a passion for everything engineering and love to ponder about knowledge conduct cultures and life in general duh I recently took an interest in photography and traveling like everyone else does I spent my spare time learning about philosophy and linguistics lately My resolution for 2020 is to get enough sleep"},{"title":"unprocessed","href":"/effectives/ecpp/it1","content":" View C as a federation of languages View C as a federation of languages The rules within each particular sublanguage tend to be straightforward however the rules change as you move from one sublanguage to another The primary sublanguages are C Statements blocks preprocessor builtin types arrays pointers etc C often offers superior approaches to the C counterparts OO C Classes encapsulation inheritance polymorphism dynamic binding etc Template C The aspect of generic programming in C The STL Containers iterators algorithms function objects that mesh beautifully Dont be surprised when moving from one sublanguage to another the strategy might switch For example pass builtin C types by value but pass user defined C classes by reference to const are often times more efficient Takeaways Rules for effective C programming vary depending on the part of C you are using"},{"title":"unprocessed","href":"/effectives/ecpp/it10","content":" Have assignment operators return a reference to this You can chain assignments together and they are right associative cpp int x y z x y z 15 chain of assignments becomes x y z 15 The way this is implemented is that assignment returns a reference to its lefthand argument and thats the convention you should follow when you implement assignment operators for your classes This convention applies to all assignment operators not just the standard but also This is only a convention code that doesnt follow it will compile However the convention is followed by all the builtin types as well as by all the types in the standard library eg string vector complex stdsharedptr etc Unless you have a good reason for doing things differently dont Takeaways Have assignment operators return a reference to this"},{"title":"unprocessed","href":"/effectives/ecpp/it11","content":" Handle assignment to self in operator Assignments to self are legal so rest assured clients will do it They may come in forms not easily recognizable eg cpp ai aj potential assignment to self if i and j have the same value px py also potential assignment to self If you follow the advice of Items 13 and 14 youll always use objects to manage resources and youll make sure that the resourcemanaging objects behave well when copied When thats the case your assignment operators will probably be selfassignmentsafe without your having to think about it If you try to manage resources yourself however which youd certainly have to do if you were writing a resourcemanaging class you can fall into the trap of accidentally releasing a resource before youre done using it Eg cpp class Bitmap class Widget Widget Widgetoperatorconst Widget rhs unsafe impl of operator delete pb stop using current bitmap pb new Bitmaprhspb start using a copy of rhss bitmap return this see Item 10 private Bitmap pb ptr to a heapallocated object The issue is that when assigning to self rhs and this point to the same object we would delete the Bitmap of rhs first then try to use a copy of the deleted Bitmap The traditional way to prevent self assignment is to add an identity test at the top Eg cpp Widget Widgetoperatorconst Widget rhs if this rhs return this identity test if a selfassignment do nothing delete pb pb new Bitmaprhspb return this This version works but its exceptionunsafe if new Bitmap yields an exception insufficient memory or copyctor of Bitmap throws the Widget will end up holding a pointer to the deleted Bitmap Making operator exceptionsafe typically renders it selfassignmentsafe too As a result its increasingly common to deal with issues of selfassignment by ignoring them focusing instead on achieving exception safety In this code to achieve exception safety we only have to reorder the statements cpp Widget Widgetoperatorconst Widget rhs Bitmap pOrig pb remember original pb pb new Bitmaprhspb point pb to a copy of rhss bitmap delete pOrig delete the original pb return this Now if the new throws pb would still point at the old Bitmap which is not yet deleted Self assignment would also be making a new copy pointing pb to that new copy and freeing the old Bitmap that pb used to point to This may not look the most efficient when selfassigning compared with the identity test but before you add that in consider how often selfassignment happens and the cost of the check think bigger code additional branch the effectiveness of prefetching caching and pipelining An alternative to this reordering approach is copyandswap discussed in more details in item 29 Like this cpp class Widget void swapWidget rhs exchange thiss and rhss data see Item 29 for details Widget Widgetoperatorconst Widget rhs Widget temprhs make a copy of rhss data swaptemp swap thiss data with the copys return this A variation of this could take advantage of passing by value is acceptable for implementing copy assignment opr and passing by value makes a copy by itself Like this cpp Widget WidgetoperatorWidget rhs rhs is a copy of the object passed in note pass by val swaprhs swap thiss data with the copys return this This may sacrifice clarity for cleverness Compilers may also generate more efficient code for this version passingbyvaluecopy over calling copy in function body Takeaways Make sure operator is wellbehaved when an object is assigned to itself Techniques include comparing addresses of source and target objects careful statement ordering and copyandswap Make sure that any function operating on more than one object behaves correctly if two or more of the objects are the same Snippet cpp handleassignmenttoselfincopyassignmentoprmcpp include include demonstrates selfassignmentunsafe and exceptionunsafe implementations of operator and ways of making it exceptionsafe and selfassignmentsafe class Bitmap public Bitmap dx10 int dx class Widget public Widget ddatanew Bitmap Widgetconst Widget rhs ddatanew Bitmaprhsddata Widget delete ddata Widget operatorconst Widget rhs void swapWidget rhs stdswapddata rhsddata Bitmap temp ddata ddata rhsddata rhsddata temp public Bitmap ddata Widget Widgetoperatorconst Widget rhs w w should break dereferencing nullptr in fact it does not delete ddata ddata new Bitmaprhsddata return this Widget Widgetoperatorconst Widget rhs selfassignment safe but exceptionunsafe if this rhs return this delete ddata ddata new Bitmaprhsddata return this Widget Widgetoperatorconst Widget rhs selfassignment safe exceptionsafe reorder statements Bitmap origData ddata ddata new Bitmaprhsddata delete origData return this Widget Widgetoperatorconst Widget rhs selfassignment safe exceptionsafe swap Widget temprhs swaptemp return this int main Widget w wddatadx 15 w w stdcout dx n Widget x x w stdcout dx n With the selfassignmentunsafe and exceptionunsafe impl the program does not break return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it12","content":" Copy all parts of an object Say you dont like compilers copy implementation and provided your own compiler in turn does not warn you if your impl is only copying a part of the object Eg when new data member gets added Similarly youll need to update the ctors other forms of assignment opr etc A particular insidious case can arise through inheritance Eg cpp class PriorityCustomer public Customer a derived class public PriorityCustomerconst PriorityCustomer rhs PriorityCustomer operatorconst PriorityCustomer rhs private int priority PriorityCustomerPriorityCustomerconst PriorityCustomer rhs priorityrhspriority logCallPriorityCustomer copy constructor PriorityCustomer PriorityCustomeroperatorconst PriorityCustomer rhs logCallPriorityCustomer copy assignment operator priority rhspriority return this The problem with this is the Customer part of PriorityCustomer will be default ctored in the copy ctor or in the copy assignment the Customer part is not assigned Any time you take it upon yourself to write copying functions for a derived class you must take care to also copy the base class parts Like this cpp PriorityCustomerPriorityCustomerconst PriorityCustomer rhs Customerrhs invoke base class copy ctor priorityrhspriority logCallPriorityCustomer copy constructor PriorityCustomer PriorityCustomeroperatorconst PriorityCustomer rhs logCallPriorityCustomer copy assignment operator Customeroperatorrhs assign base class parts priority rhspriority return this The meaning of copy all parts then becomes copy all local data members and invoke the appropriate copying function in all base classes In practice the two copying functions will often have similar bodies and this may tempt you to try to avoid code duplication by having one function call the other Your desire to avoid code duplication is laudable but having one copying function call the other is the wrong way to achieve it Instead if you find that your copy constructor and copy assignment operator have similar code bodies eliminate the duplication by creating a third member function that both call Such a function is typically private and is often named init This strategy is a safe proven way to eliminate code duplication in copy constructors and copy assignment operators Takeaways Copying functions should be sure to copy all of an objects data members and all of its base class parts Dont try to implement one of the copying functions in terms of the other copycon copy assignment opr Instead put common functionality in a third function that both call Snippet cpp copyallpartsmcpp include include demonstrates when not specified the copycon assignment opr of a derived class does not by default invoke the copycon assignment opr of the base class Parent public Parent dx10 stdcout Parent default ctorn Parentconst Parent rhs dxrhsdx stdcout Parent copyconn Parent operatorconst Parent rhs dx rhsdx stdcout Parent assignment oprn return this int x const return dx void setXint x dx x private int dx class Child public Parent public Child dy5 stdcout Child default ctorn Childconst Child rhs Parentrhs dyrhsdy stdcout Child copy ctorn Child operatorconst Child rhs stdcout Child assignment oprn remember to call Parents assignment operator Parentoperatorrhs dy rhsdy return this private void init to avoid duplication between assignment opr and copycon one could put shared code in a private init int dy int main Child c csetX20 Child dc d c stdcout dx n return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it13","content":" Use objects to manage resources Suppose we have this factory function cpp Investment createInvestment return ptr to dynamically allocated object in the Investment hierarchy the caller must delete it parameters omitted for simplicity For creating objects of different child classes of the Investment interface And we have this code that uses the above cpp void f Investment pInv createInvestment call factory function use pInv delete pInv release object There are several ways pInv wont be deleted a premature return statement or an exception before the delete Careful programming may work around those issues but think about how code changes over time relying on f getting to the delete statement isnt always viable So instead we put resource management inside an object and we can rely on Cs automatic dtor invocation to make sure that the resources are released In this case an autoptr is well suited for the job cpp void f stdautoptr pInvcreateInvestment call factory function use pInv as before automatically delete pInv via autoptrs dtor This example demonstrates two critical aspects of using objects to manage resources Resources are acquired and immediately turned over to resource managing objects This is where the name Resource Acquisition Is Initialization RAII comes from because its so common to acquire a resource and initialize a resourcemanaging object in the same statement Resource managing objects use their dtors to ensure resources are released Because destructors are called automatically when an object is destroyed eg when an object goes out of scope resources are correctly released regardless of how control leaves a block Things can get tricky when releasing can result in an exception which is addressed in Item 8 Because autoptr deletes what it points to when its deleted its important to never have two autoptrs pointing to the same object To prevent such problems autoptr has a copycon copy assignment that does not take in const rhs and sets rhs to null when copying from them so that the ownership is moved to the ctored or assigned autoptr An example cpp stdautoptr pInv1 points to the pInv1createInvestment object returned from createInvestment stdautoptr pInv2pInv1 pInv2 now points to the object pInv1 is now null pInv1 pInv2 now pInv1 points to the object and pInv2 is null STL containers require that their content exhibits normal copying behavior thus containers of autoptr are not allowed An alternative is a reference counting smart pointer stdsharedptr in C11 whose copying works as expected and can be used in STL containers Both autoptr and sharedptr use delete and not delete in their dtor which means using them to manage an array is a bad idea but one that will compile cpp stdautoptr bad idea the wrong apsnew stdstring10 delete form will be used stdsharedptr spinew int1024 same problem Dynamically allocated arrays in C dont have corresponding autoptr and sharedptr because vector and string can almost always replace dynamically allocated arrays If you want them look to boostscopedarray and boostsharedarray This item indicates if you are releasing resources manually using delete other than in resource managing classes you are doing something wrong For managing other resources you often have to craft your own resource managing classes think a lockguard in which case more subtleties to be considered are described in Item 14 and 15 As a final comment createInvestments raw pointer return type is an invitation to a resource leak because its so easy for callers to forget to call delete on the pointer they get back The interface change to createInvestment is discussed in Item 18 Takeaways To prevent resource leaks use RAII objects that acquire resources in their constructors and release them in their destructors Two commonly useful RAII classes are stdsharedptr and stduniqueptr stdsharedptr is usually the better choice because its behavior when copied is intuitive Copying an stdautoptr sets it to null For the complete uptodate argument refer to Items 18 and 19 in emcpp Snippet cpp useobjectstohandleresourcesmcpp include include demonstrates the pitfall of calling raw deletes class ObjectToBeCreatedWithFactoryMethod dont design your interface like this refer to item 18 ObjectToBeCreatedWithFactoryMethod createObject expects the client to delete the allocated object return new ObjectToBeCreatedWithFactoryMethod void iCouldThrow throw stdruntimeerrorblow up void func ObjectToBeCreatedWithFactoryMethod pObj createObject iCouldThrow using delete directly here is a bad idea if the above throws or returns prematurely think how the code evolves over time we leak resources delete pObj void func1 stduniqueptr pObjcreateObject iCouldThrow int main try func leaks func1 does not leak catch const stdruntimeerror e swallow return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it14","content":" Think carefully about copying behavior in resourcemanaging classes There are cases where you need to create your own resource managing classes say you are using C API mutex and creating an RAII class to wrap around it For example cpp class Lock public explicit LockMutex pm mutexPtrpm lockmutexPtr acquire resource Lock unlockmutexPtr release resource private Mutex mutexPtr to be used like this Mutex m define the mutex you need to use create block to define critical section Lock mlm lock the mutex perform critical section operations automatically unlock mutex at end of block But what should happen if a lock object is copied cpp Lock ml1m lock m Lock ml2ml1 copy ml1 to ml2what should happen here What should happen when copied is a question every RAII class author should confront The usual answers are Prohibit copying eg it rarely makes sense to copy a synchronization primitive like mutex Refer to emcpp item 11 This is the behavior of stduniqueptr Reference count the underlying resource eg a stdsharedptr Often an RAII class can implement the reference counting behavior with a stdsharedptr data member If we want to allow the mutex to be reference counted we could make the data member a stdsharedptr but with a custom deleter since when the count drops to 0 we dont want to destroy the mutex but rather unlock it With TR1 it looks something like this cpp class Lock public explicit LockMutex pm init sharedptr with the Mutex mutexPtrpm unlock to point to and the unlock func as the deleter lockmutexPtrget see Item 15 for info on get private stdtr1sharedptr mutexPtr use sharedptr instead of raw pointer In this case note the absence of a custom dtor Copy the underlying resource Sometimes you can have as many copies of the managed resource as you like in which case the copy operations perform a deep copy of the managed resource Some implementations of stdstring does this string class contains pointer to heap memory and both the pointer and the heap memory are copied when a string copy is made Transfer ownership of the managed resource Occasionally you may want to transfer ownership to the copied object when copying This is the behavior of stdautoptr Takeaways Copying an RAII object entails copying the resource it manages so the copying behavior of the resource determines the copying behavior of the RAII object Common RAII class copying behaviors are disallowing copying and performing reference counting but other behaviors deep copy and transfer ownership are possible"},{"title":"unprocessed","href":"/effectives/ecpp/it15","content":" Provide access to raw resources in resource managing classes In the perfect world you only need to interact with resourcemanaging objects but in reality many APIs refer to the managed resource directly Say a raw pointer is wanted but we have a sharedptr uniqueptr Both of those would offer explicit conversion via get and implicit conversion via operator and operator to access the raw pointer User custom resource management classes often demonstrate similar approaches explicit and implicit For example in this Font resource management class that wraps around an underlying FontHandle cpp given C API that works with FontHandle FontHandle getFont from C APIparams omitted for simplicity void releaseFontFontHandle fh from the same C API we have the following RAII class and to accommodate other APIs working directly with FontHandle we have the explicit and the implicit conversions approach class Font RAII class public explicit FontFontHandle fh acquire resource ffh use passbyvalue because the C API does Font releaseFontf release resource explicit FontHandle get const return f explicit conversion function downside is explicit calls to get function is required everywhere FontHandle is expected alternative implicit approach note the operator overload operator FontHandle const return f implicit conversion function downside is increased chance of error private FontHandle f the raw font resource Font f1getFont downside of the explicit approach useFontHandlef1get get has to be called explicitly downside of the implicit approach FontHandle f2 f1 oops meant to copy a Font object but instead implicitly converted f1 into its underlying FontHandle then copied that Now the program has a FontHandle being managed by the Font object f1 but the FontHandle is also available for direct use as f2 Thats almost never good For example when f1 is destroyed the font will be released and f2 will dangle The classs designer chooses whether an implicit or explicit approach is provided The guideline is in item 18 make interfaces easy to use correctly and hard to use incorrectly It may occur to you returning the resource being managed in resource management classes is contrary to encapsulation This is true but not as disastrous RAII classes dont exist to encapsulate something but rather to ensure a particular action resource release takes place Welldesigned classes hides what clients dont need to see but makes available those things that clients honestly need to access Takeaways APIs often require access to raw resources so each RAII class should offer a way to get at the resource it manages Access may be via explicit conversion or implicit conversion In general explicit conversion is safer but implicit conversion is more convenient for clients Snippet cpp provideaccesstorawresourcesinraiiclassesmcpp include include demonstrates explicit and implicit approaches to expose resources managed by RAII classes class FontHandle FontHandle getFontHandle stdcout allocate FontHandlen return FontHandle void releaseFontHandleFontHandle fh stdcout release FontHandlen class Font RAII class public explicit FontFontHandle fh acquire resource ffh use passbyvalue because the C API does Font releaseFontHandlef release resource explicit FontHandle get const stdcout explicit get calln return f explicit conversion function downside is explicit calls to get function is required everywhere FontHandle is expected alternative implicit approach note the operator overload operator FontHandle const stdcout implicit conversionn return f implicit conversion function downside is increased chance of error private FontHandle f the raw font resource void anotherAPIExpectingFontHandleFontHandle fh int main Font fontgetFontHandle implicit anotherAPIExpectingFontHandlefont explicit anotherAPIExpectingFontHandlefontget return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it16","content":" Use the same form in corresponding uses of new and delete Consider this code cpp stdstring stringArray new stdstring100 delete stringArray This is undefined behavior at least 99 of the 100 strings are unlikely to be properly destroyed because their dtor will probably never be called When you employ a new operation two things happen First memory is allocated via a function named operator newsee Items 49 and 51 Second one or more constructors are called for that memory When you employ a delete expression two things happen First one or more destructors are called for the memory Second the memory is deallocated via a function named operator deletesee Item 51 The big question for delete is how many objects reside in the memory being deleted Whose answer decides how many dtors should be called Actually the question is simpler does the pointer being deleted point to a single object or an array of objects This is important because layouts are usually different think of it like this compilers arent required to implement it like this but some do Single Object Object Array of Objects n Object Object When you call delete the only way for it to know whether the layout looks like the first or the second is for you to tell it whether you use delete or delete Like this cpp stdstring stringPtr1 new stdstring stdstring stringPtr2 new stdstring100 delete stringPtr1 delete an object delete stringPtr2 delete an array of objects Using delete on stringPtr1 or using delete on stringPtr2 would cause undefined behaviors eg too many or too few dtors called as the memory layout is not interpretted correctly This is particularly important to bear in mind when you write a class that deals with dynamic allocation and provides multiple versions of ctor all of them must use the same new or new X as otherwise dtor cannot be implemented Also for those using typedefs they should document which form of delete should be used cpp typedef stdstring AddressLines4 a persons address has 4 lines each of which is a string because AddressLines is an array underlying resource allocated with stdstring pal new AddressLines note that new AddressLines returns a string just like new string4 would should use delete pal fine To avoid such confusions abstain from typedefs for array types eg AddressLines could be a vector Takeaways If you use in a new expression you must use in the corresponding delete expression If you dont use in a new expression you mustnt use in the corresponding delete expression"},{"title":"unprocessed","href":"/effectives/ecpp/it17","content":" Store newed objects in smart pointers in standalone statements This item illustrates the same issue as that of emcpp item 21 Consider this code cpp processWidgetstdsharedptrnew Widget priority vs stdsharedptr pwnew Widget store newed object in a smart pointer in a standalone statement processWidgetpw priority this call wont leak the first version could leak due to compiler being given freedom to reorder 1 new Widget 2 stdsharedptr ctor 3 priority as long as 2 comes after 1 now if compiler orders it as 1 3 2 and 3 throws 2 wont get to manage the allocated resource from 1 and 1 causes a leak so always prefer the second version Takeaways Store newed objects in smart pointers in standalone statements Failure to do this can lead to subtle resource leaks when exceptions are thrown"},{"title":"unprocessed","href":"/effectives/ecpp/it18","content":" Make interfaces easy to use correctly and hard to use incorrectly Assuming your clients want to use your interface correctly then if they manage to use it incorrectly your interface would be partially to blame Ideally if an attempted use of an interface wont do what the client expects the code wont compile and if the code does compile it will do what the client wants Consider this interface cpp class Date public Dateint month int day int year What could go wrong Clients may pass parameters in wrong order or simply keyed in the wrong int meh Many client errors can be avoided with the introduction of new types the type system is your primary ally in preventing undesirable code from compiling In this case say we have this cpp struct Day struct Month struct Year explicit Dayint d explicit Monthint m explicit Yearint y vald valm valy int val int val int val class Date public Dateconst Month m const Day d const Year y Date d30 3 1995 error wrong types Date dDay30 Month3 Year1995 error wrong types Date dMonth3 Day30 Year1995 okay types are correct Making Day Month and Year fullfledged classes with encapsulated data would be better than the simple use of structs above see Item 22 Once the right types are in place it can sometimes be reasonable to restrict the values of those types For example there are only 12 valid month values so the Month type should reflect that One way to achieve that is to use enums with C11 enum classes due to typesafety Without using enums you could do the following cpp class Month public static Month Jan return Month1 functions returning all valid static Month Feb return Month2 Month values see below for why these are functions not static Month Dec return Month12 objects other member functions private explicit Monthint m prevent creation of new Month values monthspecific data Date dMonthMar Day30 Year1995 If the idea of using functions instead of objects to represent specific months strikes you as odd it may be because you have forgotten that reliable initialization of nonlocal static objects can be problematic Item 4 can refresh your memory And to remind you unless there is good reason not to have your types behave consistently with the builtin types The real reason can be phrased as have interfaces behave consistently with builtin types amongst themselves Think the STL their container interfaces are largely though not perfectly consistent and this helps make them fairly easy to use Eg every STL container has a member function named size that tells how many objects are in the container Another way to prevent client errors is to restrict what can be done with a type a common way is to add const qualifier Any interface that requires that clients remember to do something is prone to incorrect use because clients can forget to do it For example this one cpp Investment createInvestment from Item 13 parameters omitted for simplicity Returning a raw pointer means the client needs to remember to delete that pointer exactly once when they are done using it This would be error prone Instead the interface could return a stduniqueptr or a stdsharedptr which also has the benefit if you need a custom deleter behavior instead of relying on client calling a deleteInvestment you could bind that in the smart pointer instantiation Like this with custom deleter cpp stdtr1sharedptr createInvestment stdtr1sharedptr retValnullptr getRidOfInvestment make retVal point to the correct object return retVal The sharedptr version also works around the issue of crossDLL deletion where an object is created using new in one DLL and freed in another one The shared pointer deletion in this case would guarantee that the same DLL news and deletes this object Takeaways Good interfaces are easy to use correctly and hard to use incorrectly Your should strive for these characteristics in all your interfaces Ways to facilitate correct use include consistency in interfaces and behavioral compatibility with builtin types Ways to prevent errors include creating new types restricting operations on types constraining object values and eliminating client resource management responsibilities stdsharedptr supports custom deleters This prevents the crossDLL problem can be used to automatically unlock mutexes see Item 14 etc"},{"title":"unprocessed","href":"/effectives/ecpp/it19","content":" Treat class design like type design Defining a new class meant designing a new type meaning youre not just a class designer youre a type designer augmenting Cs type system You should therefore approach class design with the same care that language designers lavish on the design of the languages builtin types Good types have a natural syntax intuitive semantics and one or more efficient implementations How then do you design effective classes First you must understand the issues you face How should objects of your new type be created and destroyed this affects your ctor and dtor and potentially operator new delete new delete overload How should object initialization differ from object assignment this lets you decide how your ctors differ from assignment oprs What does it mean for objects of your new type to be passed by value copy ctor defines how passbyvalue is implemented for a type What are the restrictions on legal values for your new type usually only certain combinations of values for data members are valid invariants that your class has to maintain This determines the error checking you need to do and your exception specification Does your new type fit into an inheritance graph do you inherit from other classes what functions of theirs are declared virtual do you intend for other classes to inherit from yours what functions should your class declare virtual What kind of conversions are allowed for your type if you wish T1 to be implicitly convertible to T2 you will want either an operator T2 inside T1 or an nonexplicit ctor in T2 that takes T1 If you want explicit conversions only youll write the conversions but avoid the two implicit approaches What operators and functions make sense for the new type What standard functions should be disallowed Who should have access to the members of your new type public protected private members friend functions nest one class in another What is the undeclared interface of your new type what guarantees do your class offer in terms of performance exception safety and resource usage think locks and dynamic memory How general is your new type are you defining one new type or a family of types If its the latter you should probably define a class template Is the new type really what you need Takeaways Class design is type design Before defining a new type be sure to consider all the issues discussed in this item"},{"title":"unprocessed","href":"/effectives/ecpp/it2","content":" Prefer const enum inline to define Alternatively this item can be called prefer compilers to preprocessors define is substituted by the preprocessor thus the symbol is never seen by the compiler nor does it exist inside symbol table A language const instead will be seen by the compiler Advantages of using the latter include The compiler will be able to point out the symbol should an error occur If the const appears multiple times the memory footprint is smaller using a const only one copy in memory No way to create classspecific const with define because define does not respect scope When declaring const to replace define keep in mind that pointers should have both the pointer and the object it refers to declared const Eg cpp const char const author me Though item 3 would claim in this case a const stdstring is preferable cpp const stdstring author me Another case to keep in mind is classspecific consts to limit the scope of a constant to a class you should make it a member to ensure there is only one copy make it a static member cpp class GamePlayer private static const int NumTurns 5 constant declaration int scoresNumTurns use of constant If you need to take the address of NumTurns define it in the impl file of this class like this cpp include const int GamePlayerNumTurns definition This does not go in the header so that you dont double define when the header is included in multiple places and the definition does not have since an initial value is already given at the point of declaration Inclass initialization is allowed only for integral types and only for constants In cases where the above syntax cant be used you put the initial value at the point of definition Like this cpp class CostEstimate private static const double FudgeFactor declaration of static class constant goes in header file const double definition of static class CostEstimateFudgeFactor 135 constant goes in impl file If your compiler wrongfully forbid the inclass initialization of constant integral types which is required to for example declare an array of that size you could do the enum hack Like this cpp class GamePlayer private enum NumTurns 5 the enum hack makes NumTurns a symbolic name for 5 int scoresNumTurns fine The enum hack can also be used to forbid your client caller from taking the address or a reference of NumTurns or to enforce that compiler does not allocate memory for NumTurns due to its being unnecessary And enum hack is fundamental for TMP Another common misuse of define is to implement macros that look like functions but dont incur the cost of a function call This misuse can be confusing to reason eg cpp define CALLWITHMAXa b fa b a b int a 5 b 0 CALLWITHMAXa b a is incremented twice CALLWITHMAXa b10 a is incremented once Here the number of times that a is incremented before calling f depends on what it is being compared with Do this instead with a regular inline function using templates cpp template because we dont inline void callWithMaxconst T a const T b know what T is we pass by referenceto fa b a b const see Item 20 Given the availability of consts enums and inlines your need for the preprocessor especially define is reduced but not eliminated include ifdef ifndef continue to play important roles in controlling compilation Takeaways For simple constants prefer const objects or enums to defines For functionlike macros prefer inline functions to defines Snippet cpp headerwithstaticconsth ifndef INCLUDEDCONSTOBJECT define INCLUDEDCONSTOBJECT include include class ConstObject public ConstObject dc10 stdcout ConstObject default ctorn int dc void print const class MyClass public static const int dx 5 fine until you ask for its address in which case define it like dobj in impl file leave the initialization here and only do const int MyClassdx in the impl file static const ConstObject dobj needs definition correct way to define it is inside the impl file static const stdstring ds does not need definition due to no usage ConstObject MyClassdobj wrong double definition endif implwithstaticconstcpp include include include const ConstObject MyClassdobj fine invokes default ctor before main const int MyClassdx 5 void ConstObjectprint const stdcout dc n preferconstenuminlinetodefinemcpp include include include int main stdcout invoking mainn When replacing define with consts correct specification of const pointer and const pointed to const char name z const char const name2 h const int const i1 0 int const const i2 0 the second const does not matter const char const name3 e duplicate const declaration specifier int const const i3 0 duplicate const declaration specifier To test class member static const definitions MyClass mc mcdobjprint stdcout mcdx n const int const addrOfStaticConst mcdx return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it20","content":" Prefer passbyreferencetoconst to passbyvalue By default C passes by value function parameters are initialized with copies of the actual argument These copies are produced by the objects copy ctors This can make passbyvalue expensive Passing parameters by reference also avoids the slicing problem when a derived class object is passed by value as a base class object the part that only belongs to the derived part will be sliced off since only a base class copy ctor is called Like passing by pointer passing by reference does not have slicing problem If you peek under the hood of a C compiler youll find that references are typically implemented as pointers so passing something by reference usually means really passing a pointer As a result if you have an object of a builtin type eg an int its often more efficient to pass it by value than by reference This same advice applies to iterators and function objects in the STL because by convention they are designed to be passed by value Implementers of iterators and function objects are responsible for seeing to it that they are efficient to copy and are not subject to the slicing problem Just because an object is small doesnt mean that calling its copy constructor is inexpensive we pass builtin types by value not because they are small but because of the underlying compiler impl some compilers treat builtin types and even small user types differently objects containing only a bare int will not be put into registers while a bare int pointer will Another reason why small userdefined types are not necessarily good passbyvalue candidates is that being userdefined their size is subject to change They can get bigger in the next release or change as you switch to a different C impl like somes impl of stdstring can be 7 times as big as others Takeaways Prefer passbyreferencetoconst over passbyvalue Its typically more efficient and it avoids the slicing problem The rule doesnt apply to builtin types and STL iterator and function object types For them passbyvalue is usually appropriate"},{"title":"unprocessed","href":"/effectives/ecpp/it21","content":" Dont try to return a reference when you must return an object Once folks learn passing by reference some become so relenting with it that they start passing references to things that dont exist Consider this cpp class Rational public Rationalint numerator 0 see Item 24 for why this int denominator 1 ctor isnt declared explicit private int n d numerator and denominator friend const Rational see Item 3 for why the operatorconst Rational lhs return type is const const Rational rhs Rational resultlhsn rhsn lhsd rhsd return result Returning a reference to a local stackallocated object would bring undefined behavior to anyone calling operator What about a heap allocated version It wont be undefined behavior but whos in charge of calling delete Imagine its heap allocated with new instead and this code cpp Rational w x y z w x y z same as operatoroperatorx y z Here is a guaranteed leak as two objects will be heap allocated but there is only reference to one of them What about an even more exotic approach using static cpp const Rational operatorconst Rational lhs warning yet more const Rational rhs bad code static Rational result static object to which a reference will be returned result multiply lhs by rhs and put the product inside result return result client code bool operatorconst Rational lhs an operator const Rational rhs for Rationals Rational a b c d if a b c d do whatevers appropriate when the products are equal else do whatevers appropriate when theyre not think of the equality test as if operatoroperatora b operatorc d Not to mention the potentially undesirable lifetime threadsafety issue of static the above codes check will always be true The two operator calls would be returning reference to the same object so they are always equal The right way to write a function that must return a new object is to have that function return a new object For Rationals operator that means either the following code or something essentially equivalent cpp inline const Rational operatorconst Rational lhs const Rational rhs return Rationallhsn rhsn lhsd rhsd It all boils down to this when deciding between returning a reference and returning an object your job is to make the choice that offers correct behavior Let your compiler vendors wrestle with figuring out how to make that choice as inexpensive as possible Takeaways Never return a pointer or reference to a local stack object a reference to a heapallocated object or a pointer or reference to a local static object if there is a chance that more than one such object will be needed Item 4 provides an example of a design where returning a reference to a local static is reasonable at least in singlethreaded environments"},{"title":"unprocessed","href":"/effectives/ecpp/it22","content":" Declare data members private Why not public data members Syntactic consistency item 18 clients will know always to retrieve data members with getter functions instead of scratching their head trying to remember Member functions grant you more precise control With member functions you can control read write access but with public members you cant Most importantly encapsulation If you implement access to a data member with a function you can later replace the data member with a computation and no clients will be affected Hiding data members behind functional interfaces can offer all kinds of implementation flexibility Eg it makes it easy to notify other objects when data members are read or written to verify class invariants and function preand postconditions to perform synchronization in threaded environments etc If you hide your data members from your clients ie encapsulate them you can ensure that class invariants are always maintained because only member functions can affect them Furthermore you reserve the right to change your implementation decisions later Public means unencapsulated and practically speaking unencapsulated means unchangeable especially for classes that are widely used The argument against protected data members is similar Arent protected data members more encapsulated than public ones Practically speaking the surprising answer is that they are not Somethings encapsulation is inversely proportional to the amount of code that might be broken if that something changes Suppose we have a protected data member and we eliminate it How much code might be broken now All the derived classes that use it which is typically an unknowably large amount of code not unlike the case with public data members From an encapsulation point of view there are really only two access levels private which offers encapsulation and everything else which doesnt Takeaways Declare data members private It gives clients syntactically uniform access to data affords finegrained access control allows invariants to be enforced and offers class authors implementation flexibility protected is no more encapsulated than public"},{"title":"unprocessed","href":"/effectives/ecpp/it23","content":" Prefer nonmember nonfriend functions to member functions Often times youll find yourself facing the choice of having a function being a member of a class a function in this translation unit namespace Say you have a an object o with member functions a b c and there is an action abc that calls oa ob oc Should abc be a part of the class or not say being a part of the namespace that the class is in Objectoriented principles dictate that data and the functions that operate on them should be bundled together Objectoriented principles dictate that data should be as encapsulated as possible Start by inspecting encapsulation Encapsulating something means its hidden from view The more something is encapsulated the fewer see it and the greater flexibility we have to change it This is why we value encapsulation to be able to change something in a way that only affects a limited number of clients How encapsulated a data member in a class is can be evaluated by how many functions can access it The less the number of functions accessing it the more encapsulated it is Thus when given the choice of a member friend function vs a nonmember nonfriend option the preferred choice in terms of encapsulation is always the nonmember nonfriend function C doesnt require that all functions be a part of a class as Java C does so a natural approach in this case is to make the function abc a part of the same namespace that the class is in Namespace unlike classes can spread across multiple files and often times it only makes sense for some clients to know this abc and for those who dont care their compilation shouldnt require the declaration of abc at all To address this we could split these functions declarations into different headers This is how the std namespace is organized memory list algorithm vector etc Clients only need to include part of the std library headers where the required symbol is declared and in turn their compilation would only depend on those headers Partioning into different headers like described above is not possible for class member functions as they have to appear in one file This approach of putting abc in the namespace of the class also allows clients to easily extend the namespace with helper functions they need This is another feature the member function approach cannot offer classes are closed to extension by clients Takeaways Prefer nonmember nonfriend functions to member functions Doing so increases encapsulation packaging flexibility and functional extensibilit"},{"title":"unprocessed","href":"/effectives/ecpp/it24","content":" Declare nonmember functions when type conversions should apply to all parameters Having classes support implicit conversions is generally a bad idea but there are exceptions For example a numerical Rational class Having int and float being able to implicit convert to Rational is not a bad idea You may have this cpp class Rational public Rationalint numerator 0 ctor is deliberately not explicit int denominator 1 allows implicit inttoRational conversions int numerator const accessors for numerator and int denominator const denominator see Item 22 private You know youd like to support arithmetic operations like addition multiplication etc but how Which one to choose among member functions nonmember functions or nonmember functions that are friends Say you go with member functions cpp class Rational public const Rational operatorconst Rational rhs const This is fine cpp Rational oneEighth1 8 Rational oneHalf1 2 Rational result oneHalf oneEighth fine result result oneEighth fine But if you also want to support doing multiply with an int this breaks cpp result oneHalf 2 fine result 2 oneHalf error or if you rewrite the two it becomes more obvious result oneHalfoperator2 fine implicit conversion from 2 to Rational doable because Rational ctor is not explicit result 2operatoroneHalf error oneHalf has an operator so its fine int doesnt so compiler will look for nonmember functions that can be called like operator2 oneHalf ie functions that are global or in namespaces But in this example that search also fails It turns out that parameters are eligible for implicit conversion only if they are listed in the parameter list In terms of member function this is not eligible to become target of implicit conversion causing the second statement to fail So to support mixed mode operators consistently one approach is to make operator not a member Like this cpp const Rational operatorconst Rational lhs now a nonmember const Rational rhs function return Rationallhsnumerator rhsnumerator lhsdenominator rhsdenominator Rational oneFourth1 4 Rational result result oneFourth 2 fine result 2 oneFourth hooray it works The next question is should operator be a friend of Rational In this case no because operator can be implemented entirely on Rationals public interface Whenever you can avoid friend functions you should This item contains nothing but truth but not the whole truth When Rational is class template instead of a class there are new things to consider Takeaways If you need type conversions on all parameters to a function including the one pointed to by the this pointer the function must be a nonmember"},{"title":"unprocessed","href":"/effectives/ecpp/it25","content":" Consider support for a nonthrowing swap swap is originally included as part of STL and has since been a mainstay of exceptionsafe programming and used to cope with the possibility of assignment to self A typical implementation of stdswap is like cpp namespace std template typical implementation of stdswap void swapT a T b swaps as and bs values T tempa a b b temp As long as your type supports copycon and copy assignment opr the stdswap will work without additional effort But this default implementation can be slow three copy calls Eg think of a class following pimpl idiom item 31 cpp class WidgetImpl class for Widget data public details are unimportant private int a b c possibly lots of data stdvector v expensive to copy class Widget class using the pimpl idiom public Widgetconst Widget rhs Widget operatorconst Widget rhs to copy a Widget copy its WidgetImpl object For details on implementing pImpl rhspImpl operator in general see Items 10 11 and 12 private WidgetImpl pImpl ptr to object with this Widgets data To swap two Widget objects we only need to swap the pointers yet the stdswap has no way of knowing that Using total template specialization template with the later to specify this specialization is for Widget we could tell stdswap function template to swap only the implementation pointers when dealing with Widget objects cpp namespace std template this is a specialized version void swapWidget a of stdswap for when T is Widget b Widget this wont compile swapapImpl bpImpl to swap Widgets just swap their pImpl pointers This however wouldnt compile due to pImpl being private We could have this swap being a friend but the convention here is different we have swap being a public member of Widget which calls stdswap to swap the pointers Like this cpp class Widget same as above except for the public addition of the swap mem func void swapWidget other using stdswap the need for this declaration is explained later in this Item swappImpl otherpImpl to swap Widgets swap their pImpl pointers namespace std template revised specialization of void swapWidget a stdswap Widget b aswapb to swap Widgets call their swap member function This compiles and is compliant with how STL containers do it public swap member function and template specialization for each container What if Widget and WidgetImpl are instead class templates instead of classes The swap member function inside is fine but we cant partial specialize the swap function in std namespace like this cpp namespace std template void swapWidget Widget a error illegal code Widget b aswapb Due to C allowing partial specialization for classes but not functions When you want to partially specialize a function what you do instead is simply add an overload cpp namespace std template an overloading of stdswap void swapWidget a note the lack of after Widget b swap but see below for aswapb why this isnt valid code In general overloading function templates is fine but std is special Its ok to specialize templates in std but not ok to add new templates or classes or functions or anything else to std However programs that cross this line will compile but this will be undefined behavior Unfortunately this overload template in std will be seen as adding functions to std and this approach yields undefined behavior per above So what do we do We still declare an overload just not in std namespace Say Widgets in namespace WidgetStuff we could do cpp namespace WidgetStuff templatized WidgetImpl etc template as before including the swap class Widget member function template nonmember swap function void swapWidget a not part of the std namespace Widget b aswapb Now if code anywhere calls swap on two Widgets the name lookup rule argument dependent lookup or Koenig lookup will find this version in WidgetStuff which is what we want If you are not using namespaces the above would still work but why clog everything in global namespace Should we use this approach all the time then There is still a case for specializing stdswap In fact to have your class specific version of swap be called in as many places as possible you need to write both a nonmember version in the same namespace as your class and a specialization of stdswap Now from a clients perspective you want to call swap cpp template void doSomethingT obj1 T obj2 swapobj1 obj2 Which one do you want to call The one in std exist Its specialization may or may not exist A Tspecific swap may exist but not in std namespace Youll want to call a Tspecific one if it exists but fall back to general version in std if not To achieve this you do cpp template void doSomethingT obj1 T obj2 using stdswap make stdswap available in this function swapobj1 obj2 call the best swap for objects of type T With this name lookup rules dictate swap will find Tspecific swap at global scope or in the same namespace as type T argument dependent lookup If no Tspecific swap exists compilers will use swap in std thanks to the using statement that makes stdswap visible Even then compiler would still prefer a specialization if there is one over the general stdswap template Keep in mind the one thing you dont want to do is qualify the call like cpp stdswapobj1 obj2 the wrong way to call swap Youd force the compiler to consider only the std one and its specializations eliminating the possibility of using a more Tspecific one elsewhere Some programmers do call swap like the above thus the need for swaps writers to provide the fully specialized version to accommodate such clients such code is present even in stds implementation To summarize if the default swap offers acceptable efficiency for your class dont do anything if not do the following offer a public member function swap that should never throw offer a nonmember swap in the same namespace as your class have it call the member version if you are writing a class not a class template specialize stdswap for your class and have it call the member version Finally if you are using swap as a client be sure to do using stdswap and not qualify the swap call Now on exception safety the most useful application of swap is to help classes offer strong exception safety guarantee This constraint never throws only applies on the member version as the default version uses copycon and copy assignment both of which are allowed to throw in general When you write a custom version of swap you are typically offering more than just an efficient way to swap values youre also offering one that doesnt throw exceptions As a general rule these two swap characteristics go hand in hand because highly efficient swaps are almost always based on operations on builtin types such as the pointers underlying the pimpl idiom and operations on builtin types never throw exceptions Takeaways Provide a swap member function when stdswap would be inefficient for your type Make sure your swap doesnt throw exceptions If you offer a member swap also offer a nonmember swap that calls the member For classes not templates specialize stdswap too When calling swap employ a using declaration for stdswap then call swap without namespace qualification Its fine to totally specialize std templates for userdefined types but never try to add something completely new to std Snippet cpp supportnonthrowingswapmcpp include include include demonstrates what one should do when needing to provide a classspecific swap for efficiency purposes also how a client should call that swap namespace something class FooImpl public FooImpl dx10 dv FooImplint x const stdvector v dxx dvv int x const return dx stdvector v return dv private int dx stdvector dv class Foo public void doSomething Foo pimplnew FooImpl Fooconst Foo rhs note to self is there a better way to implement copycon pimpl new FooImplrhsx rhsv Foo operatorconst Foo rhs copy over all members of rhspimpl stdswap will invoke this twice making it inefficient in this pimpl class pimpl rhspimpl return this Foo delete pimpl due to default stdswap being inefficient we want to add class specific swap void swapFoo rhs stdcout x stdvector v const return pimplv private FooImpl pimpl you should also add a swap within the same namespace of your class void swapFoo lhs Foo rhs stdcout somethingswap calledn lhsswaprhs you should also specialize the one in std namespace for misguided clients who write swap like stdswap namespace std template void swapsomethingFoo lhs somethingFoo rhs stdcout stdswap specialization on Foo calledn lhsswaprhs namespace client void callswapright somethingFoo foo1 foo2 call swap like this using stdswap swapfoo1 foo2 void callswapwrong somethingFoo foo1 foo2 not like this stdswapfoo1 foo2 int main clientcallswapright clientcallswapwrong return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it26","content":" Postpone variable definitions as long as possible Unused variables come with the cost of ctors and dtors Like this function cpp this function defines the variable encrypted too soon stdstring encryptPasswordconst stdstring password using namespace std string encrypted if passwordlength MinimumPasswordLength throw logicerrorPassword is too short do whatever is necessary to place an encrypted version of password in encrypted return encrypted The object encrypted isnt completely unused only if an exception is thrown Why not move it after the length check instead With that the code is still not as tight as it might be encrypted is defined without initialization Say you are going to want encrypted to hold the value of password initially then given to an encrypt call cpp void encryptstdstring s encrypts s in place instead of stdstring encrypted defaultconstruct encrypted encrypted password assign to encrypted encryptencrypted just do stdstring encryptedpassword encryptencrypted Not only should you postpone a variables definition until right before you have to use the variable you should also try to postpone the definition until you have initialization arguments for it What about loops cpp Approach A define outside loop Approach B define inside loop Widget w for int i 0 i n i for int i 0 i n i w some value dependent on i Widget wsome value dependent on i 1 ctor 1 dtor n assignments n ctor n dtor A is generally more efficient if you know an assignment costs less than a ctordtor pair A also makes w visible in larger scope something thats contrary to program comprehensibility and maintainability As a result you should default to B unless you know assignment is less expensive than ctordtor pair and you are dealing with a performance sensitive part of your code Takeaways Postpone variable definitions as long as possible It increases program clarity and improves program efficiency"},{"title":"unprocessed","href":"/effectives/ecpp/it27","content":" Minimize casting In theory if your program compiles cleanly its not trying to perform any unsafe or nonsensical operations on any objects This is a valuable guarantee you dont want to forego it lightly Unfortunately casts can subvert the type system potentially leading to some subtle issues In C Java C casting is generally more necessary and less dangerous in C Casts can look like these cpp C style T expression cast expression to be of type T Texpression cast expression to be of type T no difference between the two purely where you put the parentheses C style constcastexpression dynamiccastexpression reinterpretcastexpression staticcastexpression Each serves a distinct purpose constcast casts away the constness and is the only cast that can achieve this dynamiccast is primarily used to perform safe downcasting ie to determine whether an object is of a particular type in the inheritance hierarchy It is the only cast that cannot be performed with old style casts and the only cast that may incur a significant runtime cost reinterpretcast is intended for low level casts that yield implementationdependent ie unportable results Eg casting pointer to int Such casts should be rare outside lowlevel code staticcast can be used to force implicit conversion eg nonconst object to const int to double etc also the reverse of many such conversions eg void to typed pointers pointertobase to pointertoderived though it cannot cast const to nonconst C style casts are preferred over C style ones They are easier to identify in code and the more narrowly specified purpose makes it possible for compilers to detect errors About the only time this book uses old style cast is when calling an explicit ctor to pass an object to a function Like this cpp class Widget public explicit Widgetint size void doSomeWorkconst Widget w doSomeWorkWidget15 create Widget from int with Cstyle cast doSomeWorkstaticcast15 create Widget from int with Cstyle cast The belief that casts do nothing but tell compilers to treat one type as another is wrong Type conversion of any kind explicit via casts or implicit by compilers often leads to code that is executed at runtime Eg this cpp int x y double d staticcastxy divide x by y but use floating point division Converting from int to double will most likely generate code due to different underlying representations Now what about this cpp class Base class Derived public Base Derived d Base pb d implicitly convert Derived Base Here were just creating a base class pointer to a derived class object but sometimes the two pointer values will not be the same When thats the case an offset is applied at runtime to the Derived pointer to get the correct Base pointer value When do such cases happen It cant happen in C C or Java but it does happen in C with multiple inheritance It can happen in single inheritance too so avoid making assumptions on how things are laid out and performing casts based on such assumptions Eg casting object addresses to char pointers and then using pointer arithmetic on them almost always yields undefined behavior Object layout is compiler specific so by making the above assumptions you are making your code unportable An interesting thing about casts is that they may seem to do the right thing but in fact they dont Consider this example of calling base method first in a derived classs method impl cpp class Window base class public virtual void onResize base onResize impl class SpecialWindow public Window derived class public virtual void onResize derived onResize impl staticcastthisonResize cast this to Window then call its onResize this doesnt work do SpecialWindow specific stuff This code does call the base classs onResize method but not on this object Instead the cast creates a new temporary copy of the base class part of this then invokes onResize on the copy This can easily lead to object being in an invalid state The solution is not to use cast but express what you really want to say cpp class SpecialWindow public Window public virtual void onResize WindowonResize call WindowonResize on this This suggests wanting to cast especially dynamiccast could be a sign that you are doing things the wrong way Before diving into details about dynamiccast know that many implementations of dynamiccast can be slow Eg an implementation based on string comparison of class names think an object in an inheritance hierarchy four levels deep each dynamiccast in this chain would be doing 4 strcmps The need for dynamiccast generally arises because you want to perform derived class operations on what you believe to be a derived class object but you only have a pointer or reference to base to work with To avoid this one way is to have containers contain the derived object pointers instead of base object pointers though this would mean having separate containers for different objects in the inheritance chain an alternative is to declare this virtual function in base as well just that its noop in base Neither of these approaches type safe containers or moving virtual functions up the hierarchy are universally applicable but they offer alternatives to dynamiccast One thing you definitely want to avoid is cascading dynamiccast like in this inheritance hierarchy of Window SpecialWindow1 SpecialWindow2 SpecialWindow3 cpp typedef stdvectorstdsharedptr VPW VPW winPtrs for VPWiterator iter winPtrsbegin iter winPtrsend iter if SpecialWindow1 psw1 dynamiccastiterget else if SpecialWindow2 psw2 dynamiccastiterget else if SpecialWindow3 psw3 dynamiccastiterget Such code generates code that is big and slow and brittle in the sense that each time the inheritance hierarchy changes the code needs to be reinspected Good C uses few casts but its generally not practical to completely get rid of them The static cast from int to double could be considered fine but still avoidable eg creating a double variable from the int Like most suspicious constructs casts should be isolated as much as possible typically hidden inside functions whose interfaces shield callers from the grubby work being done inside Takeaways Avoid casts whenever practical especially dynamiccasts in performancesensitive code If a design requires casting try to develop a castfree alternative When casting is necessary try to hide it inside a function Clients can then call the function instead of putting casts in their own code Prefer Cstyle casts to oldstyle casts They are easier to see and they are more specific about what they do Snippet cpp minimizecastingmcpp include include include demonstrates behavior of dynamiccast in an inheritance hierarchy wrong ways to use dynamiccast and cases where static casting base pointer to derived pointer may result in pointer values being different class Base1 public virtual Base1 virtual void someBase1Call class Base2 public virtual Base2 virtual void someBase2Call class Derived1 public Base1 public void someDerived1Call class Derived2 public Derived1 public void someDerived2Call class MultiDerived public Base1 public Base2 int main Derived1 pd1 new Derived1 Derived2 pd2 new Derived2 Base1 pbd1 pd1 all the wrong way to use dynamiccast if Derived2 temp dynamiccastpd1 stdcout pointer to base is casted downn fails if Derived1 temp dynamiccastpd1 stdcout pointer isnt castedn passes if Derived1 temp dynamiccastpbd1 stdcout realized pointer to base can be casted as it actually points to derivedn passes if Derived2 temp dynamiccastpbd1 stdcout base pointer further casted down albeit it doesnt point to an object of that typen fails MultiDerived md new MultiDerived reinterpretcast is implementation dependent using it makes your code not portable Base2 pbmd staticcastmd note how the value of pbmd differs from that of md assigning pointer to derived to pointer to base causes the two to have different values this is an effect of multiple inheritance where the Base2 pointer points to only a part of what MultiDerived pointer points to 8 bytes after stdcout reinterpretcastmd n stdcout reinterpretcastpbmd n delete pd1 delete pd2 delete md return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it28","content":" Avoid returning handles to object internals Suppose you are working on a rectangle class represented by its upper left and lower right corners To keep a Rectangle object small you decided to keep the extents of a rectangle in a class pointed to by a member in the Rectangle object cpp class Point class for representing points public Pointint x int y void setXint newVal void setYint newVal struct RectData Point data for a Rectangle Point ulhc ulhc upper lefthand corner Point lrhc lrhc lower righthand corner class Rectangle private stdsharedptr pData see Item 13 for info on tr1sharedptr Now you want to add functions in Rectangle to expose its points cpp class Rectangle public Point upperLeft const return pDataulhc Point lowerRight const return pDatalrhc This design is selfcontradictory const member functions meant for readonly use exposed internal private data that can be modified by client In this case ulhc and lrhc are effectively public Since ulhc and lrhc are stored outside the Rectangle class const member functions of Rectangle can return references to them limitation of bitwise const Returning pointers iterators demonstrates the same problem breaking encapsulation they are handles whose modification will affect internal members Similarly for private member functions you should never have a public member function return a pointer to a private member function since if you do the access level of that private member function is practically public What about this cpp class Rectangle public const Point upperLeft const return pDataulhc const Point lowerRight const return pDatalrhc Now clients cannot modify the returned and readonly is conveyed but it can lead to dangling references What if the referred object disappears Like this cpp class GUIObject const Rectangle returns a rectangle by boundingBoxconst GUIObject obj value see Item 3 for why return type is const GUIObject pgo make pgo point to some GUIObject const Point pUpperLeft get a ptr to the upper boundingBoxpgoupperLeft left point of its bounding box boundingBoxpgo returns a temporary object that will be destroyed at the end of the statement In turn pUpperLeft will be dangled at the end of the statement that created it This is why returning a reference iterator pointer to an internal part of the object is dangerous what if the reference outlives the object This doesnt mean you should never return a handle sometimes you have to eg operator of string and vector But such functions are exceptions not the rule Takeaways Avoid returning handles references pointers or iterators to object internals This increases encapsulation helps const member functions act const and minimizes the creation of dangling handles Snippet cpp avoidreturninghandletoobjectinternalmcpp include include demonstrates undefined behavior of dangling reference caused by temporary objects returning a handle to object internal class MyClass public MyClassconst stdstring data ddatadata const stdstring data const return ddata private stdstring ddata MyClass createMyClassconst stdstring data return MyClassdata int main stdstring datadata const stdstring rdcreateMyClassdatadata rd will be dangling at this point MyClass ccreateMyClassdata const stdstring rd1cdata rd1 will be fine stdcout rd rd1 n clang on osx does not demonstrate noticeable behavior for this UB return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it29","content":" Strive for exceptionsafe code Consider this code where we want to have this menu change background images in a threaded environment cpp class PrettyMenu public void changeBackgroundstdistream imgSrc change background image private Mutex mutex mutex for this object Image bgImage current background image int imageChanges of times image has been changed We could implement changeBackground like this cpp void PrettyMenuchangeBackgroundstdistream imgSrc lockmutex acquire mutex as in Item 14 delete bgImage get rid of old background imageChanges update image change count bgImage new ImageimgSrc install new background unlockmutex release mutex From the perspective of exception safety this function is bad as it violates Leak no resources if this throws before unlock mutex is locked forever Dont allow data structures to become corrupted if new ImageimgSrc throws bgImage is left pointing at a deleted object and imageChanges is incremented Item 14 introduces a lockguard like RAII object to tackle mutex locking forever cpp void PrettyMenuchangeBackgroundstdistream imgSrc Lock mlmutex from Item 14 acquire mutex and ensure its later release delete bgImage imageChanges bgImage new ImageimgSrc To address data corruption first we defines the terms Exceptionsafe functions offer one of the three guarantees The basic guarantee promises if an exception is thrown everything in the program is left in valid state All objects are in internally consistent state though the state of the entire program may not be predictable The strong guarantee promises if an exception is thrown the state of the program is unchanged Calls to such functions are atomic transactional in the sense that if they succeed they succeed completely and if they fail its like theyve never been called The nothrow guarantee promises never to throw exceptions the function always does what they promise to do All operations on builtin types int pointer etc guarantee no throw This is a critical building block of exception safe code Empty throw specification does not mean a function is not going to throw int doSomething throw indicates if an exception is thrown its a serious error and the unexpected function should be called Exception safe must offer one of the three guarantees the choice is then to decide which guarantee is practical for the code you write As a general rule you want to offer the strongest guarantee thats practical Anything dynamically allocating memory all STL containers typically throw a badalloc if it cannot find enough memory to satisfy the allocation Offer no throw when you can but most of the time the choice is between strong exception guarantee and basic exception guarantee Now if we change the bgImage pointer to a smart pointer good idea from resource management perspective as well as exception safety perspective and increment imageChanges after the reset usually a good idea to change a status to reflect a state update only after the state update actually happens we end up with cpp class PrettyMenu stdsharedptr bgImage void PrettyMenuchangeBackgroundstdistream imgSrc Lock mlmutex bgImageresetnew ImageimgSrc replace bgImages internal pointer with the result of the new Image expression imageChanges Now there is no need for manual delete and delete only happens if the reset succeeds Now this is almost a strong guarantee except that imgSrc marker might moved in case of an exception Its important to know a general strategy that typically leads to exception safe code copyandswap In principle if you want to change something make a copy of it change the copy and swap the original with the copy If any of the operation throws the original object is not affected And after the operations are done use a nothrow swap to swap the copy and the original This is usually implemented with a pimpl Like this cpp struct PMImpl PMImpl PrettyMenu stdtr1sharedptr bgImage Impl see below for int imageChanges why its a struct class PrettyMenu private Mutex mutex stdtr1sharedptr pImpl void PrettyMenuchangeBackgroundstdistream imgSrc using stdswap see Item 25 Lock mlmutex acquire the mutex stdtr1sharedptr copy obj data pNewnew PMImplpImpl pNewbgImageresetnew ImageimgSrc modify the copy pNewimageChanges swappImpl pNew swap the new data into place release the mutex Copyandswap is excellent in making all or nothing changes to an object though it doesnt guarantee strong exception safety Say if you make some calls inside changeBackground changeBackground will only be as exception safe as those calls Say changeBackground makes two calls f1 and f2 even if both offer strong exception safe guarantee changeBackground may not f1 modifies some states f2 then throws states modified in f1 will not be rolled back The problem is side effect if a function is sideeffect say update a DB free or operate only on local data then its easy to guarantee strong exception safety Otherwise itll be hard there is no general way to undo a DB operation considering other clients might have updated it in between Another issue with copyandswap is efficiency you have to make a copy a cost you may not want to pay Strong exception safety is desirable but you should offer it only when practical When its not youll have to offer the basic guarantee Things are different if you offer no exception safety guarantee in which case its like guilty until proven innocent All of its callers would be unable to offer exception safety guarantee and the system in turn offers no exception safe guarantee A functions exceptionsafety guarantee is a visible part of its interface so you should choose it as deliberately as you choose all other aspects of a functions interface Dont use interaction with legacy code as your excuse to not write exception safe code Forty years ago gotoladen code was considered perfectly good practice Now we strive to write structured control flows Twenty years ago globally accessible data was considered perfectly good practice Now we strive to encapsulate data Ten years ago writing functions without thinking about the impact of exceptions was considered perfectly good practice Now we strive to write exceptionsafe code Time goes on We live We learn Takeaways Exceptionsafe functions leak no resources and allow no data structures to become corrupted even when exceptions are thrown Such functions offer the basic strong or nothrow guarantees The strong guarantee can often be implemented via copyandswap but the strong guarantee is not practical for all functions A function can usually offer a guarantee no stronger than the weakest guarantee of the functions it calls Snippet cpp copyandswapstrongexceptionsafetymcpp include include include include demonstrates offering strong exception safety using copyandswap and objects to manage resources along with a pimpl struct MyClassImpl this pimpl class offers no encapsulation perceived as unnecessary MyClassImplconst stdstring data int counter ddatadata dcountercounter MyClassImplconst MyClassImpl rhs default MyClassImpl operatorconst MyClassImpl rhs default stdstring ddata int dcounter class MyClass public MyClassconst stdstring data int counter dimplnew MyClassImpldata counter MyClass operatorconst MyClass rhs dimpl rhsdimpl return this MyClassconst MyClass rhs dimplstdmakeuniquerhsdimpl void updateDataconst stdstring data this function offers strong exception safety transactional the whole thing either happened or it didnt using stdswap stdlockguard guarddmutex stduniqueptr pCopy stdmakeuniquedata dimpldcounter swapdimpl pCopy dimpldcounter 1 return int counter const return dimpldcounter stdstring data const return dimplddata private stduniqueptr dimpl stdmutex dmutex any preference for mutex going into impl vs not Book had it this way tho due to copying concerns potentially int main stdstring datadata MyClass mydata 0 stdstring data1data1 myupdateDatadata1 myupdateDatadata1 stdcout mydata mycounter n return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it3","content":" Use const whenever possible const is how you can communicate to the compiler and other programmers that a value should not be altered and the compiler will enforce it Use it whenever this constraint holds Using const with pointers cpp char greeting Hello char p greeting nonconst pointer nonconst data const char p greeting nonconst pointer const data char const p greeting const pointer nonconst data const char const p greeting const pointer const data If the word const appears to the left of the asterisk whats pointed to is constant if the word const appears to the right of the asterisk the pointer itself is constant if const appears on both sides both are constant If a const appears on the left of the asterisk whether its const type or type const makes no difference An STL iterator is modeled on a pointer declaring an iterator itself const will be analogous to declaring the pointer const If with STL you want to declare the data const use constiterator cpp stdvector vec const stdvectoriterator iter iter acts like a T const vecbegin iter 10 OK changes what iter points to iter error iter is const stdvectorconstiterator cIter cIter acts like a const T vecbegin cIter 10 error cIter is const cIter fine changes cIter const can be used to specify a function return value its parameters and the function itself if it is a member function Having a function return a constant value is generally inappropriate but sometimes doing so can reduce the incidence of client errors without giving up safety or efficiency For example cpp class Rational const Rational operatorconst Rational lhs const Rational rhs why should the operator for Rationals return const because if not clients can inadvertently do Rational a b c a b c invoke operator on the result of ab and all it takes is a typo for the above code to happen if a b c oops meant to do a comparison One of the hallmarks of good userdefined types is that they avoid gratuitous incompatibilities with the builtins The above where the product of two can be assigned is to is pretty gratuitous const member functions are important as they make the interface easier to understand which functions can change the object in question and they make it possible to work with const objects Member functions differing only in their constness can be overloaded Eg cpp class TextBlock public const char operator for operatorconst stdsizet position const const objects return textposition char operator for operatorconst stdsizet position const nonconst objects return textposition private stdstring text Its never legal to modify the return value of a function that returns a builtin type and when modify the return value of a function note that itd be done on a copy of the source inside that function Semanticswise there is bitwise constness and logical constness C uses bitwise constness where a const member function is not allowed to modify any of the bits inside the object Bitwise const would mean if the objects data member is a pointer inside a const member function where the pointer looks at cannot be changed but the content of what it points to can be For example cpp class CTextBlock public char operatorstdsizet position const inappropriate but bitwise return pTextposition const declaration of operator private char pText this compiles without issues const CTextBlock cctbHello declare constant object char pc cctb0 call the const operator to get a pointer to cctbs data pc J cctb now has the value Jello in the book this should allow you to change the value though in my code sample the assignment call results in bus error Logical constness suggest that a const member function may modify parts of the object only if its clients cannot detect This notion can be achieved with mutable keyword Eg cpp class CTextBlock public stdsizet length const private char pText mutable stdsizet textLength these data members may mutable bool lengthIsValid always be modified even in const member functions stdsizet CTextBlocklength const if lengthIsValid textLength stdstrlenpText now fine lengthIsValid true also fine return textLength Now suppose you have boundary check logging etc in TextBlocks operator These logic would be duplicated in both the const and the nonconst version To avoid the duplication we could do a cast instead which is usually a bad idea in other circumstances like this cpp class TextBlock public const char operatorstdsizet position const same as before some shared operations return textposition char operatorstdsizet position now just calls const op return constcast cast away const on ops return type staticcastthis add const to thiss type position call const version of op This is safe as when we are given a nonconst TextBlock we can safely invoke the const version and then cast its result The other way round of having the const version call the nonconst version is not something you want to do Takeaways Declaring something const helps compilers detect usage errors const can be applied to objects at any scope to function parameters and return types and to member functions as a whole Compilers enforce bitwise constness but you should program using conceptual constness When const and nonconst member functions have essentially identical implementations code duplication can be avoided by having the nonconst version call the const version Snippet cpp useconstwheneverpossiblemcpp include include class StringWrapper public StringWrapperconst char in ddatain const char operatorstdsizet idx const stdcout call on const versionn return ddataidx char operatorstdsizet idx stdcout call on nonconst versionn return ddataidx private stdstring ddata class CTextBlock public CTextBlockchar in ddatain char operatorstdsizet position const inappropriate but bitwise return ddataposition const declaration of operator char ddata class TextBlock public TextBlockconst char in ddatain const char operatorstdsizet position const same as before stdcout const version of operatorn return ddataposition char operatorstdsizet position now just calls const op return constcast cast away const on ops return type staticcastthis add const to thiss type position call const version of op stdstring ddata int main StringWrapper swHello stdcout sw0 n calls nonconst const StringWrapper cswWorld stdcout csw0 n calls const sw0 Y stdcout sw0 n calls nonconst char content Hello const CTextBlock cctbcontent declare constant object stdcout cctbddata n calls nonconst char pc cctb0 call the const operator to get a pointer to cctbs data pc J cctb now has the value Jello this results in bus error on OSX stdcout cctbddata n TextBlock textBlockGood textBlock0 f stdcout textBlockddata n return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it30","content":" Understand the ins and outs of inlining Inline replaces a function call with the body of the function code which saves function call overhead but results in larger object code size in turn additional page reduced instruction cache hits etc On the other hand if an inline function is very short the code generated for the function body may be smaller than that generated for a function call and the effects of larger object size would be reversed Bear in mind inline is a request to the compiler not a command Inline can be implicit or explicit cpp implicit defining a function inside a class definition class Person public int age const return theAge an implicit inline request age is defined in a class definition private int theAge explicit eg the definition of stdmax template an explicit inline inline const T stdmaxconst T a const T b request stdmax is return a b b a preceded by inline Inline functions must typically be in header files because most compilers do inline during compilation compilers need to know what the function looks like in order to inline Some compilers can inline at linking or even at runtime Net but inlining in C is mostly compile time Templates are typically in header files because compiler needs to know what a template looks like in order to instantiate it Some compilers can do instantiation at linking Its not true that function templates must be inline they are independent Most compilers refuse to inline a function deemed too complicated eg loops or recursion Virtual function calls cannot be inlined if what to call is only known at runtime then compiler cannot replace the function call with function body Whats inlined would end up depending on the compiler who usually emits a warning when it refuses to inline something you told it to If your program takes the address of a function compiler will need to generate a function body anyway Compilers generally dont perform inlining over calls made through function pointers Thus if something ends up being inlined or not sometimes depends on how its called Like cpp inline void f assume compilers are willing to inline calls to f void pf f pf points to f f this call will be inlined because its a normal call pf this call probably wont be because its through a function pointer Even if you dont use function pointers compilers may do In fact ctors and dtors are often worse candidates for inlining than a casual examination would indicate For example cpp class Base public private stdstring bm1 bm2 base members 1 and 2 class Derived public Base public Derived Deriveds ctor is empty or is it private stdstring dm1 dm2 dm3 derived members 13 Deriveds ctor may contain no user code but in order to make Base class construction happen string members construction happen and rollback if a part of it is done then exception happens compiler has to generate code Eg like the following cpp DerivedDerived conceptual implementation of empty Derived ctor BaseBase initialize Base part try dm1stdstringstring try to construct dm1 catch if it throws BaseBase destroy base class part and throw propagate the exception try dm2stdstringstring try to construct dm2 catch if it throws dm1stdstringstring destroy dm1 BaseBase destroy base class part and throw propagate the exception try dm3stdstringstring construct dm3 catch if it throws dm2stdstringstring destroy dm2 dm1stdstringstring destroy dm1 BaseBase destroy base class part and throw propagate the exception Considering all the code added by the compiler inlining a ctor or dtor might lead to excessive bloat in object code Library compiler must also consider that inlining a function makes it get compiled with clients code Should the library decide to change that function clients would now need to recompile as opposed to relink which is often times undesirable And if the library is dynamically this change may be absorbed in a way transprent to clients Most debuggers have trouble with inline functions how do you set a breakpoint to a function thats not there Some debuggers do support it while others may simply disable inlining for debug builds To summarize the strategy with regard to inlining initially dont inline anything or limit to those that have to be inline or trivial Then figure out the right functions to inline Remember the 8020 rule of 80 of time might be spent executing 20 of the code thus finding out the right functions to inline is important Takeaways Limit most inlining to small frequently called functions This facilitates debugging and binary upgradability minimizes potential code bloat and maximizes the chances of greater program speed Dont declare function templates inline just because they appear in header files"},{"title":"unprocessed","href":"/effectives/ecpp/it31","content":" Minimize compilation dependencies between files When you change the private parts implementation of a class sometimes you have to recompile the entire project Why Arguably C doesnt do a good job splitting interface from implementation Like this cpp personh class Person public Personconst stdstring name const Date birthday const Address addr stdstring name const stdstring birthDate const stdstring address const private stdstring theName implementation detail Date theBirthDate implementation detail Address theAddress implementation detail To be able to compile personh it has to include cpp include include dateh include addressh Meaning if any of these change the files including the changed have to recompile meaning a compile dependency is set up You might think why doesnt C do this instead cpp namespace std class string forward declaration an incorrect one see below class Date forward declaration class Address forward declaration class Person public Personconst stdstring name const Date birthday const Address addr stdstring name const stdstring birthDate const stdstring address const Two problems first the forward declaration for string is not correct string is not a class but rather a typedef basicstring The proper forward declaration is substantially more complicated and involves more templates That said you shouldnt forward declare anything from the standard library they shouldnt be compilation bottlenecks especially if your build environment takes advantage of precompiled headers Second problem is compiler having to know the size of objects during compilation cpp int main int x define an int Person p params define a Person When compiler sees this definition of p it has to know how much space to allocate for p Java and Smalltalk doesnt have this issue since compilers only allocate enough space for a pointer to the object as if cpp int main int x define an int Person p define a pointer to a Person You could play the hide the implementation behind a pointer yourself pimpl idiom like this cpp include standard library components shouldnt be forwarddeclared include for tr1sharedptr see below class PersonImpl forward decl of Person impl class class Date forward decls of classes used in class Address Person interface class Person public Personconst stdstring name const Date birthday const Address addr stdstring name const stdstring birthDate const stdstring address const private ptr to implementation stdtr1sharedptr pImpl see Item 13 for info on stdtr1sharedptr This is a true separation of interface and implementation whose key is replacement of dependencies on definitions with dependencies on declarations Thus make headers selfsufficient whenever practical when not depend on the declarations in other files not definitions The rest flows from this strategy Avoid using objects when object references and pointers will do Defining an object necessitates the presence of the types definition Depend on class declarations instead of class definitions whenever you can Note that you dont need a class definition to declare a function using that class not even if the function passes or returns the class type by value Eg cpp class Date class declaration Date today fine no definition void clearAppointmentsDate d of Date is needed We dont need the definition of Date in order to declare today or clearAppointments since whoever calls those functions must have Date defined Provide separate header files for declarations and definitions And if a declaration is changed in one place it must be changed in both Library owners declare both header files so that clients dont forward declare but instead include the declaration header file Like cpp include datefwdh header file declaring but not defining class Date Date today as before void clearAppointmentsDate d Think of datefwdh as from C which contains forward declarations of iostream components whose definitions are in and also demonstrates the suggestions of this item applies to templates as well Although templates are often defined in headers for those build systems that support header definition in implementation it still makes sense to provide declarationonly headers for templates C has export keyword though support in real world is scanty Here is what an impl of the pimpl Person handle class could look like cpp include Personh were implementing the Person class so we must include its class definition include PersonImplh we must also include PersonImpls class definition otherwise we couldnt call its member functions note that PersonImpl has exactly the same member functions as Person their interfaces are identical PersonPersonconst stdstring name const Date birthday const Address addr pImplnew PersonImplname birthday addr stdstring Personname const return pImplname An alternative to Person class being a pimpl is to make it an interface which could look like note that C does not impose restrictions on interfaces like Java and net does cpp class Person public virtual Person virtual stdstring name const 0 virtual stdstring birthDate const 0 virtual stdstring address const 0 Clients of Person interface has to program in terms of pointers and references due to being not possible to instantiate classes containing pure virtual methods Clients of an interface class need a way to create new objects typically via static functions called factory methods or virtual ctors Like this cpp class Person public static stdtr1sharedptr return a tr1sharedptr to a new createconst stdstring name Person initialized with the const Date birthday given params see Item 18 for const Address addr why a tr1sharedptr is returned clients call this like stdstring name Date dateOfBirth Address address create an object supporting the Person interface stdtr1sharedptr ppPersoncreatename dateOfBirth address And well need a concrete class to handle the actual work cpp class RealPerson public Person public RealPersonconst stdstring name const Date birthday const Address addr theNamename theBirthDatebirthday theAddressaddr virtual RealPerson stdstring name const implementations of these stdstring birthDate const functions are not shown but stdstring address const they are easy to imagine private stdstring theName Date theBirthDate Address theAddress and persons create function works like this stdtr1sharedptr Personcreateconst stdstring name const Date birthday const Address addr return stdtr1sharedptrnew RealPersonname birthdayaddr A more realistic implementation of Personcreate would create different types of derived class objects depending on function parameters environment etc A second way to implement an Interface class involves multiple inheritance a topic explored in Item 40 Now what does handle classes like this cost you It costs you some speed at runtime plus some additional memory per object Additional level of indirection size of additional pointer have to use heap allocation for pimpl Needing to go through vptr and look at vtable for Interface class method call Finally neither pimpl of interface classes cant get much use out of inline functions inline needs to see function bodies in the headers but the point of pimpl or interface in this instance is to hide such details away Dont be scared off by the runtime cost of pimpl interface functions use them in development where propoer Takeaways The general idea behind minimizing compilation dependencies is to depend on declarations instead of definitions Two approaches based on this idea are Handle classes and Interface classes Library header files should exist in full and declarationonly forms This applies regardless of whether templates are involved Snippet cpp myclasscpp include include include include include void MyClassImpldoSomething stdcout data content ddata doSomething stdstring MyClassdata const return dimplddata MyClassMyClass dimplstdmakeshared MyClassImplMyClassImpl ddatadefault clientutilscpp include include include void printFromMyClassconst MyClass myClass stdcout a client util whose compilation does not depend on that of MyClass myClassdata n MyClass buildMyClassMyClass rhs return MyClassrhs concreteotherclasscpp include include ConcreteOtherClassConcreteOtherClass ddatagood void ConcreteOtherClassdoSomething stdcout other class uses inheritance to hide definition ddata n stdstring ConcreteOtherClassdata const return ddata stduniqueptr OtherClasscreateOtherClass return stdmakeunique myclassh ifndef INCLUDEDMYCLASS define INCLUDEDMYCLASS include class MyClassImpl class MyClass public MyClass MyClass default MyClass operatorconst MyClass default MyClassconst MyClass default void doSomething stdstring data const private stdsharedptr dimpl endif testdrivermcpp demonstrates using pimpl to reduce compile time dependency depend on declarations as opposed to definitions include client caller is not aware of the impl header using pimpl include client caller is not aware of derived impl class header using inheritance include include include include int main MyClass obj objdoSomething MyClass obj1buildMyClassobj printFromMyClassobj1 stduniqueptr obj2 OtherClasscreateOtherClass obj2doSomething return 0 myclassimplh ifndef INCLUDEDMYCLASSIMPL define INCLUDEDMYCLASSIMPL include struct MyClassImpl MyClassImpl void doSomething stdstring ddata endif otherclassh ifndef INCLUDEDOTHERCLASS define INCLUDEDOTHERCLASS include include this interface exists to achieve the same thing pimpl achieves making compilation depend on declaration rather than definition in its definition the client is locked in with the ConcreteOtherClass making it not a typical Strategypatternlike implementation class OtherClass public virtual OtherClass virtual void doSomething 0 virtual stdstring data const 0 static stduniqueptr createOtherClass endif concreteotherclassh ifndef INCLUDEDCONCRETEOTHERCLASS define INCLUDEDCONCRETEOTHERCLASS include class ConcreteOtherClass public OtherClass public ConcreteOtherClass virtual void doSomething virtual stdstring data const private stdstring ddata endif clientutilsh ifndef INCLUDEDCLIENTUTILS define INCLUDEDCLIENTUTILS note how myclass header is not included class MyClass void printFromMyClassconst MyClass myClass MyClass buildMyClassMyClass rhs this doesnt make sense but just for demo sake endif "},{"title":"unprocessed","href":"/effectives/ecpp/it32","content":" Make sure public inheritance models isa Public inheritance means isa commit this to memory If D publicly inherits from B then you are telling the compiler and every reader of your code that D is an instance of B not vice versa B is a more general concept than D and D is more specific than B Anywhere B can be used a D can be used as well because every object of type D is an object of type B Within the realm of C anything that expects an argument of type B say B const B B a D would suffice This sounds simple but sometimes intuition can be misleading a penguin is a bird a bird can fly but a penguin cannot In this case our language isnt precise To model that not all birds can fly and a penguin is one such bird we can do cpp class Bird no fly function is declared class FlyingBird public Bird public virtual void fly class Penguin public Bird no fly function is declared If your software does not model flying of a bird but rather just beaks and wings then we dont need FlyingBird Say your software does concern flying what about this cpp void errorconst stdstring msg defined elsewhere class Penguin public Bird public virtual void fly errorAttempt to make a penguin fly This can only be tested at runtime and item 18 would suggest good interfaces prevent invalid code from compiling so you should prefer rejecting at compile time as opposed to at runtime What about having a square publicly inherit from a rectangle From geometry sure but consider this code cpp class Rectangle public virtual void setHeightint newHeight virtual void setWidthint newWidth virtual int height const return current values virtual int width const void makeBiggerRectangle r function to increase rs area int oldHeight rheight rsetWidthrwidth 10 add 10 to rs width assertrheight oldHeight assert that rs height is unchanged class Square public Rectangle Square s assertswidth sheight this must be true for all squares makeBiggers by inheritance s isa Rectangle so we can increase its area assertswidth sheight this must still be true for all squares The issue here is that something applicable to a rectangle is not applicable to a square changing its width independent of the height This means using public inheritance which suggests everything applicable to a rectangle is applicable to a square is not suitable for this case Isa relationship is not the only one that can exist between classes Two other common relationships are hasa and isimplementedintermsof which will be discussed in later items Takeaways Public inheritance means isa Everything that applies to base classes must also apply to derived classes because every derived class object is a base class object Snippet cpp publicinheritancemodelsisamcpp include include demonstrates class Base class Derived public Base void expectBaseBase x int main Derived d expectBased return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it33","content":" Avoid hiding inherited names Name hiding shadowing looks like this cpp int x global variable void someFunc double x local variable stdcin x read a new value for local x hides global x Now in an inheritance scenario cpp class Base private int x public virtual void mf1 0 simple virtual virtual void mf2 simple virtual void mf3 non virtual class Derived public Base public virtual void mf1 void mf4 Note that we are talking about names here that includes names of types enums nested classes and typedefs Suppose mf4 is implemented like this cpp void Derivedmf4 mf2 Compiler figures out what mf2 refers to they begin by looking at the local scope that of mf4 then the containing scope that of Derived then the scope of the base class and it finds mf2 so the search stops if not search continues to the namespaces containing Base if any and finally to the global scope Now suppose we have this cpp class Base private int x public virtual void mf1 0 virtual void mf1int virtual void mf2 void mf3 void mf3double class Derived public Base public virtual void mf1 void mf3 void mf4 Derived d int x dmf1 fine calls Derivedmf1 dmf1x error Derivedmf1 hides Basemf1 dmf2 fine calls Basemf2 dmf3 fine calls Derivedmf3 dmf3x error Derivedmf3 hides Basemf3 From the perspective of name lookup Basemf1 and Basemf3 are no longer inherited by Derived This applies even though the functions in the base and derived classes take different parameter types and it also applies regardless of whether the functions are virtual or nonvirtual The rationale behind this behavior is that it prevents you from accidentally inheriting overloads from distant base classes when you create a new derived class in a library or application framework In fact if you are using public inheritance and you dont inherit the overloads you are violating the isa relationship between base and derived classes that Item 32 explains is fundamental to public inheritance That being the case youll almost always want to override Cs default hiding of inherited names You do it with using declarations cpp class Base private int x public virtual void mf1 0 virtual void mf1int virtual void mf2 void mf3 void mf3double class Derived public Base public using Basemf1 make all things in Base named mf1 and mf3 using Basemf3 visible and public in Deriveds scope virtual void mf1 void mf3 void mf4 Now inheritance will work as expected cpp Derived d int x dmf1 still fine still calls Derivedmf1 dmf1x now okay calls Basemf1 dmf2 still fine still calls Basemf2 dmf3 fine calls Derivedmf3 dmf3x now okay calls Basemf3 This means if you inherit from a base class with overloaded functions and you want to redefine or override only some of them you need to include a using declaration for each name youd otherwise be hiding If you dont some of the names youd like to inherit will be hidden Its conceivable that you sometimes wont want to inherit all the functions from your base class Under public inheritance this should never be the case Under private inheritance however it can make sense Suppose Derived privately inherits from Base and the only version of mf1 Derived wants to inherit is the one taking no parameters using declaration wont do the trick here as using makes all inherited functions with a given name visible in the derived class This is the case for a different technique a simple forwarding function cpp class Base public virtual void mf1 0 virtual void mf1int as before class Derived private Base public virtual void mf1 forwarding function implicitly Basemf1 inline see Item 30 Derived d int x dmf1 fine calls Derivedmf1 dmf1x error Basemf1 is hidden When inheritance is combined with templates an entirely different form of the inherited names are hidden issue arises Takeaways Names in derived classes hide names in base classes Under public inheritance this is never desirable To make hidden names visible again employ using declarations or forwarding functions Snippet cpp avoidhidinginheritednamesmcpp include include demonstrates overriding and overloading at the same time Instead of the default hiding behavior say inheriting all overloads or some overloads is desirable arguably overloading and overriding at the same time is not a good idea class Base public Base ddatabase data void f stdcout f void overload basen void fint x stdcout f int overload base x n void gdouble d stdcout g double overload base d n private stdstring ddata class Derived public Base public using Basef such that both overloads of f are visible in this scope and considered as overload candidates with the same preference as the f defined here Derived ddataderived data void fconst stdstring data stdcout f string overload derived data n void gconst stdstring data stdcout g string overload derived data n private stdstring ddata class ImplementationInheritance private Base public now say if you only want the void version of Basef and a string overload defined in this class Note that this request does not make sense for public inheritance as with public inheritance every function in base class should be applicable to the derived void fconst stdstring data stdcout f string overload private inherited data n void f Basef simple forwarding function as using would import all overloads of f snippet from the book whose idea is shown above ImplementationInheritance but the particular impl is calling a pure virtual overload class Base1 public virtual Base1 default virtual void mf1 0 virtual void mf1int class Derived1 private Base1 public virtual void mf1 forwarding function implicitly Base1mf1 inline see Item 30 void func Derived1 d int x dmf1 fine calls Derivedmf1 well linker error actually dmf1x error Basemf1 is hidden int main func stdstring datagood int x 3 double y 40 Derived d dfdata works dfx works due to the present of using declaration df ditto dgy compile error ImplementationInheritance ii iifdata works iif works iifx compile error return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it34","content":" Differentiate between inheritance of interface and inheritance of implementation Public inheritance turns out to have two different notions inheritance of interface like a declaration and inheritance of impl like a definition Sometimes you want the derived to inherit only the declaration sometimes you want both but derived can override the impl and sometimes you want both but derived cannot override Consider cpp class Shape public virtual void draw const 0 virtual void errorconst stdstring msg int objectID const class Rectangle public Shape class Ellipse public Shape Member function interfaces are always inherited Shape is an instance and cannot be instantiated Taking a look at each function draw is pure virtual and has to be redeclared by any concrete class that inherits them and they typically have no definition in the abstract class The purpose of a pure virtual is to have derived classes inherit an interface only From base to derived you have to provide a draw impl but Ive no idea of your impl details Incidentally it is possible to provide a definition for pure virtual functions but the only way to call them would be with the class qualifiers Like cpp Shape ps1 new Rectangle fine ps1Shapedraw error is a simple virtual function whose purpose is to have derived classes inherit from a function interface as well as an impl From base to derived youve got to support an error call but if you dont write your own you can fall back on the default one in Shape This is potentially dangerous in that if a default impl is provided and a client needing to override the default impl forgot to do so Say we have an Airplane base class which used to have a simple virtual fly function since all children derived from it flew the same way Now we have another model deriving from Airplane and its intended to overwrite fly but might forget to do so We want to maintain the behavior of having a default impl but also want to make sure the clients are aware they may have to override fly We could make fly pure virtual like this and provide a defaultFly cpp class Airplane public virtual void flyconst Airport destination 0 protected void defaultFlyconst Airport destination void AirplanedefaultFlyconst Airport destination default code for flying an airplane to the given destination The derived classes that can use the default behavior do this cpp class ModelA public Airplane public virtual void flyconst Airport destination defaultFlydestination class ModelB public Airplane public virtual void flyconst Airport destination defaultFlydestination And for derived classes that cannot use the default behavior they would be forced to consider if they can use defaultFly when implementing fly override cpp class ModelC public Airplane public virtual void flyconst Airport destination void ModelCflyconst Airport destination code for flying a ModelC airplane to the given destination This is not fool proof but safer than just a simple virtual function defaultFly is protected as its implementation detail of Airplane and its derived and should be of no concern to the clients of Airplane defaultFly also should not be virtual because no derived classes should override one such default behavior If its virtual you open yourself up to this question again what if its meant to be overridden by some clients but they forget to do it To achieve this you may instead do the trick of providing an impl to a pure virtual function cpp class Airplane public virtual void flyconst Airport destination 0 void Airplaneflyconst Airport destination an implementation of a pure virtual function default code for flying an airplane to the given destination class ModelA public Airplane public virtual void flyconst Airport destination Airplaneflydestination class ModelB public Airplane public virtual void flyconst Airport destination Airplaneflydestination class ModelC public Airplane public virtual void flyconst Airport destination void ModelCflyconst Airport destination code for flying a ModelC airplane to the given destination This is almost exactly the same as above except that Airplanefly takes the place of AirplanedefaultFly In essence this splits Airplanefly into two parts the declaration which derived has to use and the impl which derived has the option to use only if they explicitly request it In merging fly and defaultFly though you lose the ability to specify the protection level of defaultFly Now to Shapes non virtual function objectID which is an invariant over specialization because it identifies behavior that is not supposed to change no matter how specialized a derived class becomes As such a non virtual function specifies both the interface and impl should be inherited From base to derived you must support an objectId that is always computed the same way Because this is meant to be an invariant derived should not redefine this function which is covered in Item 36 The pure virtual simple virtual and non virtual allow you to specify whether just an interface or impl as well should be inherited Avoid mistakes like declaring no functions virtual as almost any classes intended to be used as base will have virtual methods dtor at least If you are worried about the performance think about the 8020 rule Spend your effort optimizing where it matters Also avoid the mistake of declaring all functions virtual in a base class where some invariant functions should be preserved in the inheritance chain Takeaways Inheritance of interface is different from inheritance of implementation Under public inheritance derived classes always inherit base class interfaces Pure virtual functions specify inheritance of interface only Simple impure virtual functions specify inheritance of interface plus inheritance of a default implementation Nonvirtual functions specify inheritance of interface plus inheritance of a mandatory implementation Snippet cpp differentiateinheritinterfaceandinheritimplmcpp include include demonstrates pure virtual with a definition simple virtual and non virtual class Shape public Shape default virtual void draw 0 pure virtual child only inherits interface and have to provide impl virtual void error stdcout Shapeerrorn simple virtual child inherits interface and could choose to inherit impl void objectId stdcout ShapeobjectId invariantn non virtual invariant in the chain and should not be redefined Child inherits both interface and impl void Shapedraw pure virtual functions can have a default impl which can be used to convey to child clients you may inherit this impl only by explicitly specifying so stdcout Shapedraw default impl for a pure virtual functionn class Rectangle public Shape public virtual void draw stdcout Rectangledrawn class Circle public Shape public virtual void draw Shapedraw virtual void error stdcout draw pserror psobjectId Shape ps1 new Circle ps1draw ps1error ps1objectId return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it35","content":" Consider alternatives to virtual functions Suppose that you are designing a game where different game characters have different ways of calculating health values cpp class GameCharacter public virtual int healthValue const return characters health rating derived classes may redefine this The fact that healthValue is not pure virtual indicates there is a default way of calculating it While this approach might seem obvious there are possible alternatives Snippet cpp alternativestopublicvirtualmcpp include include include demonstrates alternatives to public virtual methods in particular passing a stdfunction for flexibility in what can be given in ctor to allow perobject customization but restrict yourself to using only public information in the function object class GameCharacter public HealthCalcFunc is any callable entity that can be called with anything compatible with a GameCharacter and that returns anything compatible with an int see below for details GameCharacter default using HealthCalcFunc stdfunction explicit GameCharacterHealthCalcFunc hcf healthFunchcf int healthValue const return healthFuncthis private HealthCalcFunc healthFunc like a strategy pattern but instead of virtual healthCalc inside GameCharacter object we pass the function in it can be a free function note the nonint return types implicit conversion short calcHealthconst GameCharacter stdcout calcHealth free functionn return 0 it can be a function object struct HealthCalculator int operator const GameCharacter stdcout HealthCalculatoroperator function objectn return 1 it can be a member function struct GameLevel float healthconst GameCharacter const stdcout GameLevelhealth member functionn return 20 class EvilBadGuy public GameCharacter public EvilBadGuyGameCharacterHealthCalcFunc hf GameCharacterhf class EyeCandyCharacter public GameCharacter public EyeCandyCharacterGameCharacterHealthCalcFunc hf GameCharacterhf int main EvilBadGuy ebg1calcHealth character using a health calculation function HealthCalculator hcObj EyeCandyCharacter ecc1hcObj character using a health calculation function object GameLevel currentLevel EvilBadGuy ebg2 character using a stdbindGameLevelhealth health calculation currentLevel member function stdplaceholders1 see below for details ebg1healthValue ecc1healthValue ebg2healthValue return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it36","content":" Non virtual interface idiom Lets begin with the interesting school of thought that virtual functions should almost always be private Adherents to this school would make healthValue a nonvirtual public method that calls into a virtual private method doHealthValue like this cpp class GameCharacter public int healthValue const derived classes do not redefine this see Item 36 do before stuff see below int retVal doHealthValue do the real work do after stuff see below return retVal private virtual int doHealthValue const derived classes may redefine this default algorithm for calculating characters health This is called NVI idiom nonvirtual interface for having a nonvirtual public function call a private virtual method A particular manifestation of a more general pattern called Template Method which has nothing to do with C templates An advantage of this comes from the do before stuff and do after stuff comments some code can be guaranteed to be executed before and after the virtual function does the work eg context setup and teardown lock a mutex making a log entry making sure invariants are satisfied NVI means clients redefine something private something they cant call But there is no design contradiction there redefining a virtual function specifies how something is to be done Calling a virtual function specifies when it will be done These concerns are independent NVI opens up the how but not the when The NVI idiom does not mandate the function to be overridden is private protected is also common should its functionality be exposed to derived classes"},{"title":"unprocessed","href":"/effectives/ecpp/it37","content":" The strategy pattern via function pointers A more dramatic change suggests healthValue need not be part of a GameCharacter class Like this cpp class GameCharacter forward declaration function for the default health calculation algorithm int defaultHealthCalcconst GameCharacter gc class GameCharacter public typedef int HealthCalcFuncconst GameCharacter explicit GameCharacterHealthCalcFunc hcf defaultHealthCalc healthFunchcf int healthValue const return healthFuncthis private HealthCalcFunc healthFunc This can be seen as a strategy design pattern it offers the flexibility of having different healthValue calculation for different instances of characters not just different types of characters also healthValue can be changed at runtime On the other hand healthValue not being a member means it does not have access to internal parts of the object whose health its calculating and this becomes an issue when not all the info needed to calculated health is public As a general rule the only way to work around this is to weaken encapsulation either by declaring the method a friend or expose public getters to parts that it would otherwise have kept hidden"},{"title":"unprocessed","href":"/effectives/ecpp/it38","content":" Strategy pattern via stdfunction stdfunction can hold any callable entity function pointer function object member function pointer etc whose signature is compatible with whats expected It would look something like this cpp class GameCharacter as before int defaultHealthCalcconst GameCharacter gc as before class GameCharacter public HealthCalcFunc is any callable entity that can be called with anything compatible with a GameCharacter and that returns anything compatible with an int see below for details typedef stdfunction HealthCalcFunc explicit GameCharacterHealthCalcFunc hcf defaultHealthCalc healthFunchcf int healthValue const return healthFuncthis private HealthCalcFunc healthFunc In this case by compatible we mean the stdfunction can contain any functions whose parameter can implicitly convert to const GameCharacter and return value can implicitly convert to int The difference with the function pointer approach is mininal except that the client now has slightly more flexibility cpp short calcHealthconst GameCharacter health calculation function note nonint return type struct HealthCalculator class for health objects class GameLevel public float healthconst GameCharacter const health calculation mem function note nonint return type class EvilBadGuy public GameCharacter as before class EyeCandyCharacter public GameCharacter another character type assume same constructor as EvilBadGuy EvilBadGuy ebg1calcHealth character using a health calculation function EyeCandyCharacter ecc1HealthCalculator character using a health calculation function object GameLevel currentLevel EvilBadGuy ebg2 character using a stdbindGameLevelhealth health calculation currentLevel member function stdplaceholders11 see below for details stdfunction offers a lots of flexibility in the form of what can be given For example the stdbind allows you to specify a particular function in this case a member function with 2 parameters first one being an implicit this pointer to call and adapt that to the number of parameters expected by the stdfunction object"},{"title":"unprocessed","href":"/effectives/ecpp/it39","content":" The classic strategy pattern With just strategy pattern you could end up with something like this cpp class GameCharacter forward declaration class HealthCalcFunc public virtual int calcconst GameCharacter gc const HealthCalcFunc defaultHealthCalc class GameCharacter public explicit GameCharacterHealthCalcFunc phcf defaultHealthCalc pHealthCalcphcf int healthValue const return pHealthCalccalcthis private HealthCalcFunc pHealthCalc This becomes more like a standard strategy pattern implementation where HealthCalcFunc class can be derived from to customize how health calculation is done Snippet cpp useprivateinheritancejudiciouslymcpp include include demonstrates the rare cases where you might want to consider private inheritance if you want to model isimplementedintermsof and you want access to protected parts or redefine virtual functions or if you care about empty base optimization class Timer public Timer explicit Timerint tickFrequency dtickFrequencytickFrequency virtual void onTick const 0 automatically called for each tick we want to redefine this private int dtickFrequency class Widget private Timer public explicit Widget Timer10 void measuredFunc const onTick private virtual void onTick const stdcout ticksn EBO class Empty class Composition public int ddata Empty e class PrivateInheritance private Empty public int ddata int main Widget w wmeasuredFunc stdcout size of int sizeofint n stdcout size of empty class sizeofEmpty n stdcout size of Composition class sizeofComposition n stdcout size of PrivateInheritance class sizeofPrivateInheritance n return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it4","content":" Make sure objects are initialized before they are used If you see this cpp int x in some contexts x will be initialized with 0 in others x wont class Point int x y Point point in some contexts point will be initialized with 0s in others point wont Reading uninitialized values yields undefined behavior More often reading them will result in semirandom bits The rules for when they are and when they arent could be too complicated to be worth remembering in general if youre in the C part of C see Item 1 and initialization would probably incur a runtime cost its not guaranteed to take place If you cross into the nonC parts of C things sometimes change This explains why an array from the C part of C isnt necessarily guaranteed to have its contents initialized but a vector from the STL part of C is The best way to deal with this seemingly indeterminate state of affairs is to always initialize your objects before you use them For nonmember objects of builtin types do this manually cpp int x 0 manual initialization of an int const char text A Cstyle string manual initialization of a pointer see also Item 3 double d initialization by reading from stdcin d an input stream For almost everything else do this in ctors Dont confuse member initialization list with assignments Member initialization list will often be more efficient as they do a default ctor call wasted copy assignment as opposed to just one copycon call Also sometimes member initialization list has to be used eg data members that are const or references Sometimes when multiple ctors have to duplicate the same member initialization lists and are undesirable in which case we could use the assignment approach and make them call one function that does the assignment But in general always prefer the member initialization list The order in which an objects data is initialized is defined base class before derived class and within a class data members are initialized in the order they are declared This is true even if member initialization list has a different order To avoid confusion always list the declaration and the member initialization list in the same order And now on to the order of initialization of nonlocal static objects defined in different translation units Firstly a static object is one that exists from the time its constructed until the end of the program Stack and heapbased objects are thus excluded Included are global objects objects defined at namespace scope objects declared static inside classes objects declared static inside functions and objects declared static at file scope Static objects inside functions are known as local static objects because theyre local to a function and the other kinds of static objects are known as nonlocal static objects Static objects are automatically destroyed when the program exits ie their destructors are automatically called when main finishes executing The relative order of initialization of nonlocal static objects defined in different translation units is undefined What if you want to control the order of their initialization then Make them local instead have a function call that instantiates them and return the static objects like a singleton pattern Eg cpp we used to have two global static variables in different translation units file system and a temp directory the temp directory depends on the file system global static variable being instantiated first now since the sequence would be undefined we make both functionlocal static instead and make sure file system function is always called before temp directory function class FileSystem as before FileSystem tfs this replaces the tfs object it could be static in the FileSystem class static FileSystem fs define and initialize a local static object return fs return a reference to it class Directory as before DirectoryDirectory params as before except references to tfs are now to tfs stdsizet disks tfsnumDisks Directory tempDir this replaces the tempDir object it could be static in the Directory class static Directory td defineinitialize local static object return td return reference to it tempDir and tfs are both good candidates for inlining another concern is asis there is the possibility of multithreaded call on functions that instantiate static variables to avoid this we could call both functions in main first Thus to avoid using objects before they are initialized you need to do first manually initialize nonmember objects of builtin types then use member initialization lists to initialize all parts of an object finally design around the initialization order uncertainty that afflicts nonlocal static objects defined in separate translation units Takeaways Manually initialize objects of builtin type because C only sometimes initializes them itself In a constructor prefer use of the member initialization list to assignment inside the body of the constructor List data members in the initialization list in the same order theyre declared in the class Avoid initialization order problems across translation units by replacing nonlocal static objects with local static objects Snippet cpp dependsonmyclasscpp include DependsOnMyClass getGlobalDependsOnMyClass static DependsOnMyClass dependsOnMyClassgetGlobalMyClassdx return dependsOnMyClass myclassh ifndef INCLUDEDMYCLASS define INCLUDEDMYCLASS include include we want a translationunit scope instance of MyClass and DependsOnMyClass and we want to make sure MyClass is always instantiated first class MyClass public MyClass dx10 stdcout MyClass ctorn int dx class DependsOnMyClass public DependsOnMyClassint y dyy stdcout DependsOnMyClass ctorn int dy DependsOnMyClass getGlobalDependsOnMyClass MyClass getGlobalMyClass endif myclasscpp include MyClass getGlobalMyClass static MyClass myClass return myClass initializeobjectbeforeusemcpp include include include demonstrates enforcing certain order in initializing nonlocal static variables in different translation units int main stdcout main calledn stdcout getGlobalDependsOnMyClassdy n return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it40","content":" To recap We introduced the following alternatives to public virtual methods NVI idiom private virtual being called into by public nonvirtual Holding a function pointer data member per object customization allowed but restricted to working with common parts Holding a stdfunction data member similar with above but more flexibility in what can be given Holding a HealthCalcFunc pointer data member full fledged Strategy pattern where HealthCalcFunc can be inherited to customize per object calc behavior There are lots of alternatives in OO design explore them often Takeaways Alternatives to virtual functions include the NVI idiom and various forms of the Strategy design pattern The NVI idiom is itself an example of the Template Method design pattern A disadvantage of moving functionality from a member function to a function outside the class is that the nonmember function lacks access to the classs nonpublic members stdfunction objects act like generalized function pointers Such objects support all callable entities compatible with a given target signature Snippet cpp multipleinheritancemcpp include include include demonstrates a case where arguably multiple inheritance is useful inherit an interface from a class and implementation from another because we want to override its virtual functions class IPerson this class specifies the public interface to be implemented virtual IPerson default virtual stdstring name const 0 class PersonInfo this class has functions public useful in implementing the IPerson interface virtual PersonInfo default virtual const char theName const stdcout valueDelimOpen theName called valueDelimClose n return theName virtual const char valueDelimOpen const 0 made abstract just for the sake of demonstration virtual const char valueDelimClose const 0 class CPerson public IPerson private PersonInfo note use of MI inherit interface from one class implementation from another due to wanting to override some of the latters virtual functions public virtual stdstring name const implementations return PersonInfotheName of the required IPerson member functions private redefinitions of const char valueDelimOpen const return inherited virtual const char valueDelimClose const return delimiter functions int main stduniqueptr pp stdmakeunique ppname return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it41","content":" Never redefine an inherited nonvirtual function Consider this code cpp class B public void mf class D public B D x x is an object of type D youd be quite surprised if the following two behave differently B pB x get pointer to x pBmf call mf through pointer D pD x get pointer to x pDmf call mf through pointer How can these two differ They differ if mf is a non virtual function in B but redefined in D Like this cpp class D public B public void mf hides Bmf see Item 33 pBmf calls Bmf pDmf calls Dmf Reasoning for this is that nonvirtual functions are statically bound decided at compile time according to the type the pointer points to This kind of behavior is undesirable in that an object may behave as B or D which is decided by compiletime type as opposed to what the object really is References demonstrate similar behaviors as pointers Whats more item 34 described declaring a nonvirtual function in base class conveys the idea that the function is invariant over specialization for that class Now if D redefines this function there is a contradiction in the design This echos with item 7s declare dtors virtual in base classes if they are not virtual youll hide base classess dtor And should an object of the derived type be referred to using pointer to base type only the base part will be dtored afterwards Takeaways Never redefine an inherited nonvirtual function Snippet cpp templateinstiationmcpp include include include demonstrates implicit interface and compile time polymorphism in template instantiation template void largerThanTenT w if wsize 10 stdcout larger than 10n else stdcout const C5 lhs double rhs return true int main C2 obj using C3 here wont work as compiler expects the implicit interface to directly contain a call size rather than anything that this type can implicitly convert to contains a call size largerThanTenobj return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it42","content":" Never redefine a functions inherited default parameter value Virtual functions are dynamically bound but default parameters are statically bound Reminder on dynamic type and static type cpp a class for geometric shapes class Shape public enum ShapeColor Red Green Blue all shapes must offer a function to draw themselves virtual void drawShapeColor color Red const 0 class Rectangle public Shape public notice the different default parameter value bad virtual void drawShapeColor color Green const class Circle public Shape public virtual void drawShapeColor color const Shape ps static type Shape dynamic type doesnt refer to anything yet Shape pc new Circle static type Shape dynamic type Circle Shape pr new Rectangle static type Shape dynamic type Rectangle dynamic type changes as assignments go This is all fine but imagine invoking a virtual function with a default parameter the default parameter will be statically bound while the function is dynamically bound Now with the above example if you call draw with no arguments on pr itll actually use ColorRed as opposed to Rectangles default ColorGreen Similarly with references Why does C statically bind default parameters for dynamically bound functions virtual functions For efficiency Now what about keep using the same default parameters cpp class Shape public enum ShapeColor Red Green Blue virtual void drawShapeColor color Red const 0 class Rectangle public Shape public virtual void drawShapeColor color Red const Code duplication and dependency should something change in either the other has to be updated as well If you find yourself doing this consider alternatives pointed out by item 35 like NVI cpp class Shape public enum ShapeColor Red Green Blue void drawShapeColor color Red const now nonvirtual set up doDrawcolor calls a virtual tear down private virtual void doDrawShapeColor color const 0 the actual work is done in this func class Rectangle public Shape public private virtual void doDrawShapeColor color const note lack of a default param val This makes it clear that draw being an invariant how can be substituted but not the when its default parameter should always be Red Takeaways Never redefine an inherited default parameter value because default parameter values are statically bound while virtual functions the only functions you should be overriding are dynamically bound Snippet cpp twomeaningsoftypenamemcpp include include include demonstrates the rule of prepending typename before nested dependent type names except when such names are those of base classes in inheritance or appear in a member initialization list int x 10 class MyClass public class Nested public int ddata void doStuff const stdcout nested do stuffn Nested dnested compiler thought what if you have this This would become ambiguous static int Nested static int member template void templateFuncconst T t typename TNested n typename is required here nested dependent type name Tmember x compiler may think you have this instead member is a static data member and x is something to multiply it with So this is a multiplication whose result is not used instead of a pointer declaration this generates a compiler warning tdoStuff template class TemplateBase public class Nested T data template class Derived public TemplateBaseNested public in the declaration of derived you dont need a typename to specify TemplateBaseNested being a nested dependent type name Derived TemplateBaseNested stdcout derived ctorn similarly with member initialization list int main MyClass m templateFuncm Derived d return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it43","content":" Model hasa or isimplementedintermsof through composition Composition is the relationship between types that arises when objects of one type contain objects of another type Example cpp class Address where someone lives class PhoneNumber class Person public private stdstring name composed object Address address ditto PhoneNumber voiceNumber ditto PhoneNumber faxNumber ditto Person objects are composed of string Address and PhoneNumber objects Composition means either hasa or isimplementedintermsof Most people have little difficulty differentiating hasa and isa but how about differentiating implementedintermsof and isa Consider you want to implement a set no duplicated elements based on stdlist since you dont want to pay the space cost of a binary search tree of stdset three pointers How about having this set derive from stdlist cpp template the wrong way to use list for Set class Set public stdlist This may seem fine but something is quite wrong a set is not a list as a list allow duplicated elements but a set does not The right way here is to suggest a set is implemented in terms of a list cpp template the right way to use list for Set class Set public bool memberconst T item const void insertconst T item void removeconst T item stdsizet size const private stdlist rep representation for Set data and the implementation could look like template bool Setmemberconst T item const return stdfindrepbegin repend item repend template void Setinsertconst T item if memberitem reppushbackitem template void Setremoveconst T item typename stdlistiterator it see Item 42 for info on stdfindrepbegin repend item typename here if it repend reperaseit template stdsizet Setsize const return repsize One can argue this sets interface would be easier to use correctly and harder to use incorrectly if it conforms with STL containers interface but that would require a lot more stuff to the code and better not added for the sake of clarity here Takeaways Composition has meanings completely different from that of public inheritance In the application domain composition means hasa In the implementation domain it means isimplementedintermsof Snippet cpp accessnameintemplatedbasemcpp include include include demonstrates how compiler will not look at functions in templated base by default and three ways of making them do it template class Base public void log stdcout logn void log1 stdcout log1n private T ddata template class Base template class Derived public Base public void doStuff log use of undeclared identifier log thislog fine Baselog fine but unrecommended if log is virtual this wouldnt employ dynamic binding using Baselog1 void doStuff1 log1 fine too int main Derived d ddoStuff return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it44","content":" Use private inheritance judiciously Lets look at a private inheritance example cpp class Person class Student private Person inheritance is now private void eatconst Person p anyone can eat void studyconst Student s only students study Person p p is a Person Student s s is a Student eatp fine p is a Person eats error a Student isnt a Person How does private inheritance behave Compilers will generally not convert a derived class object into a base class object Members inherited from a private base class become private members of the derived class even if they were protected or public in the base class What does private inheritance mean It means isimplementedintermsof You do so because you want the derived to take advantage of some of the features available in the base not because of there is any conceptual relationship between objects of the base and the derived As such private inheritance is an implementation technique and means nothing during software design Using the terms from Item 34 private inheritance means that implementation only should be inherited interface should be ignored Item 38 suggests composition also can mean isimplementedintermsof what to choose between the two Use composition whenever you can and use private inheritance only when you must When must you Primarily when protected members andor virtual functions enter the picture Lets say we have a Widget class and we now want to keep track of how many times each member function is called Suppose we already have this Timer class cpp class Timer public explicit Timerint tickFrequency virtual void onTick const automatically called for each tick Timer can be configured to tick with whatever frequency we need and on each tick it calls a virtual function We can redefine the virtual function so that it examines the current state of the Widget world In order for Widget to redefine a virtual function in Timer Widget must inherit from Timer but public inheritance is not appropriate in this case not isa encourages misuses We then have cpp class Widget private Timer private virtual void onTick const look at Widget usage data etc Making onTick to private to not expose to client misusage of this implementation detail This is a nice design but achievable without private inheritance make a Widget that publicly inherit from Timer redefine Timer there and put an object of that type inside Widget Like this cpp class Widget private class WidgetTimer public Timer public virtual void onTick const nested within Widget WidgetTimer timer This may seem more complicated but two reasons you might want to go with this You might want to allow Widget to be derived from but you might want to prevent derived classes from redefining onTick like final in Java If Widget inherits from Timer thats not possible not even if the inheritance is private You might want to minimize Widgets compilation dependencies If Widget inherits from Timer Timers definition must be available when Widget is compiled so Widget header has to be included If WidgetTimer is moved out of Widget and Widget contains only a pointer to WidgetTimer Widget can get by with a simple declaration for the WidgetTimer class There is other very edgy case where private inheritance might save space when you are dealing with a class that has no data in it Freestanding empty class in C has non0 size If you do this cpp class Empty has no data so objects should use no memory class HoldsAnInt should need only space for an int private int x Empty e should require no memory Youll find that sizeofHoldsOfInt sizeofint A char is usually silently inserted into Empty and compilers add padding to HoldsAnInt Now if we have a Empty as base and HoldsAnInt derive from it you are almost sure to find that sizeofHoldsAnInt sizeofint An object of HoldsAnInt is not freestanding and the base part of it neednt have a non0 size This is known as empty base optimization EBO EBO is generally only viable under single inheritance In practice empty classes arent truly empty They never have nonstatic data members they often contain typedefs enums static data members or nonvirtual functions The STL has many such unaryfunction binaryfunction are too But lets go back to the basics both private inheritance and composition mean isimplementedintermsof but composition is easier to understand so use it whenever you can Takeaways Private inheritance means isimplementedinterms of Its usually inferior to composition but it makes sense when a derived class needs access to protected base class members or needs to redefine inherited virtual functions Unlike composition private inheritance can enable the empty base optimization This can be important for library developers who strive to minimize object sizes Snippet cpp factorparameterindependentcodeoutoftemplatesmcpp include include include demonstrates cases where one can should factor parameter code out of templates to avoid code bloat by parameterizing a nontype parameter required to instantiate a template bloat version template objects of type T a nontype parameter class SquareMatrixBloat on the sizet parameter public void invert invert the matrix in place some business logic stdcout SquareMatrixBloat invertn unbloat version template class SquareMatrixBase protected SquareMatrixBasestdsizet n T pMem store matrix size and a sizen pDatapMem ptr to matrix values void setDataPtrT ptr pData ptr reassign pData void invertstdsizet matrixSize invert matrix of the given some business logic size Work with pData and stdcout SquareMatrixBase invert size matrixSize n private stdsizet size size of matrix T pData pointer to matrix values template class SquareMatrix private SquareMatrixBase private using SquareMatrixBaseinvert avoid hiding base version of invert item 33 public SquareMatrix send matrix size and SquareMatrixBasen data data ptr to base class derived class is in charge actual allocation but gives a pointer of its data to the base so that invert knows what to work with void invert thisinvertn does this violate item 37 arguably not as the inheritance relationship is not public you cant assign SquareMatrix to SquareMatrixBase and call invert on the same pointer and expect different behaviors due to static binding SquareMatrixBase is not meant to be exposed by this design private T datann int main SquareMatrixBloat smb1 smb1invert SquareMatrixBloat smb2 smb2invert business logic in SquareMatrixBloatinvert will be generated twice SquareMatrix sm1 sm1invert SquareMatrix sm2 sm2invert business logic in SquareMatrixBaseinvert will not be generated twice the parameterized version may run slower eg less opportunity for compiletime optimization such as constant propagation but it also results in smaller binary size which may better leverage instruction cache locality return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it45","content":" Use multiple inheritance judiciously When multiple inheriting it becomes possible to inherit the same name eg function typedef etc from more than one base class This could lead to ambiguity cpp class BorrowableItem something a library lets you borrow public void checkOut check the item out from the library class ElectronicGadget private bool checkOut const perform selftest return whether test succeeds class MP3Player note MI here public BorrowableItem some libraries loan MP3 players public ElectronicGadget class definition is unimportant MP3Player mp mpcheckOut ambiguous which checkOut Note that in this case even if only BorrowableItems checkOut is accessible this call is still ambiguous in that C first identifies the best match for the call and then check its accessibility In this case both are equally good matches so its ambiguous To resolve ambiguity you could do cpp mpBorrowableItemcheckOut ah that checkOut Multiple inheritance could lead to diamond of doom Like this cpp class File class InputFile public File class OutputFile public File class IOFile public InputFile public OutputFile Now suppose File has a data member fileName how many copies of fileName should IOFile have C allows both By default it provides the replication If thats not what you want you mut make File a virtual base class To do that you have all classes that immediately inherit from it use virtual inheritance cpp class File class InputFile virtual public File class OutputFile virtual public File class IOFile public InputFile public OutputFile The std library contains multiple inheritance just like this one except classes are class templates and names are basicios basicistream basicostream and basiciostream From the viewpoint of correct behavior public inheritance should always be virtual But correctness is not the only the perspective due to internal implementation virtual inheritance costs It costs in other ways too the rules governing the initialization of virtual base classes are more complicated and less intuitive than are those for nonvirtual bases the responsibility for initializing a virtual base is borne by the most derived class in the hierarchy Given the above the advice on virtual inheritance is simple dont use it unless you need to if you must use them avoid putting data into virtual base classes in this sense its similar with Java Net interfaces which are not allowed to contain any data Now to demonstrate a case where multiple inheritance can be useful cpp class IPerson this class specifies the public interface to be implemented virtual IPerson virtual stdstring name const 0 virtual stdstring birthDate const 0 class DatabaseID used below details are unimportant class PersonInfo this class has functions public useful in implementing explicit PersonInfoDatabaseID pid the IPerson interface virtual PersonInfo virtual const char theName const virtual const char theBirthDate const virtual const char valueDelimOpen const virtual const char valueDelimClose const class CPerson public IPerson private PersonInfo note use of MI public explicit CPerson DatabaseID pid PersonInfopid virtual stdstring name const implementations return PersonInfotheName of the required IPerson member virtual stdstring birthDate const functions return PersonInfotheBirthDate private redefinitions of const char valueDelimOpen const return inherited virtual const char valueDelimClose const return delimiter functions The idea is that we want to implement a concrete class that public inherits from IPerson interface and we found another class PerrsonInfo that has useful functions to help with implementing this concrete class but those functions are declared virtual To override them we privately inherit from PersonInfo Takeaways Multiple inheritance is more complex than single inheritance It can lead to new ambiguity issues and to the need for virtual inheritance Virtual inheritance imposes costs in size speed and complexity of initialization and assignment Its most practical when virtual base classes have no data Multiple inheritance does have legitimate uses One scenario involves combining public inheritance from an Interface class with private inheritance from a class that helps with implementation"},{"title":"unprocessed","href":"/effectives/ecpp/it46","content":" Define nonmember functions inside templates when type conversions are desired Recall that item 24 explained why nonmember functions are eligible for implicit type conversions on all arguments operator on Rational class why it should not be a member function instead have a nonmember so that lhs is eligible for implicit conversion Consider this code goal is to support the same mixed mode operation item 24 pointed out cpp template class Rational public Rationalconst T numerator 0 see Item 20 for why params const T denominator 1 are now passed by reference const T numerator const see Item 28 for why return const T denominator const values are still passed by value Item 3 for why theyre const template const Rational operatorconst Rational lhs const Rational rhs Now if we have this cpp Rational oneHalf1 2 this example is from Item 24 except Rational is now a template Rational result oneHalf 2 error wont compile The template suggested lhs and rhs are of the same type so we cant multiply a Rational with a int Having deduced lhs and T int you might want compilers to use implicit conversion and convert 2 to Rational and succeed but implicit type conversions are never considered during template argument deduction Such conversions are used during function calls but before you can call a function you have to know which functions exist This instead will work cpp template class Rational public friend declare operator const Rational operatorconst Rational lhs function see const Rational rhs below for details template define operator const Rational operatorconst Rational lhs functions const Rational rhs We relieve compilers the work of having to do type deduction by leveraging the fact that a friend declaration in a template class can refer to a specific function Class templates dont depend on template argument deduction that process only applies to function templates so T is always known at the time the class Rational is instantiated So what happens here is oneHalf will cause Rational to be instantiated when that happens the friend function declaration happens and as a declared function compilers no longer need to do type deduction on it just try to generate code to call it and apply implicit conversion when needed which in this case will be able to turn int into Rational Note the syntax for declaring the friend function without just saves typing the following is the same cpp template class Rational public friend const Rational operatorconst Rational lhs const Rational rhs Now this will compile but will not link since compiler knows we want to call operatorRational Rational that function is declared in Rational but not defined there We want the template function outside to provide the definition but things dont work that way if we declare this friend function we are also responsible for defining it The simplest approach is this cpp template class Rational public friend const Rational operatorconst Rational lhs const Rational rhs return Rationallhsnumerator rhsnumerator same impl lhsdenominator rhsdenominator as in Item 24 An interesting observation about this technique is that the use of friendship has nothing to do with needing to access nonpublic parts of the class In order to make type conversions possible on all arguments we need a nonmember function Item 24 still applies And in order to have the proper function automatically instantiated we need to declare the function inside the class The only way to declare a nonmember function inside a class is to make it a friend So thats what we do Or you can do this instead with a helper function call say you want to avoid the inline cpp template class Rational declare Rational template template declare const Rational doMultiplyconst Rational lhs helper const Rational rhs template template class Rational public friend const Rational operatorconst Rational lhs const Rational rhs Have friend return doMultiplylhs rhs call helper doMultiply impl template define const Rational doMultiplyconst Rational lhs helper const Rational rhs template in header file return Rationallhsnumerator rhsnumerator if necessary lhsdenominator rhsdenominator doMultiply does not need to support mixedmode multiplication but it doesnt need to It will only be called by operator and operator does support mixed mode In essence operator makes sure implicit conversion happens and when both become the same Rational doMultiply does the action Takeaways When writing a class template that offers functions related to the template that support implicit type conversions on all parameters define those functions as friends inside the class template Snippet cpp definenonmemberfunctionsinsidetemplateswhentypeconversiondesiredmcpp include include include demonstrates how to support mixedmode operations implicit conversion item 24 when template classes come into play when writing a class template that offers functions related to the template that support implicit type conversions on all parameters define those functions as friends inside the class template template class Rational public RationalT p 1 T q 1 dpp dqq T p const return dp T q const return dq friend Rational operatorconst Rational lhs const Rational rhs return Rationallhsp rhsp lhsq rhsq note that the friend here has nothing to do with being able to access private parts of Rational rather to make this a nonmember function such that essentially implicit conversions on this pointer would be considered And if we declare this friend we have to define it either do it like this or have this call a templated version defined outside You cant forego the definition here a call since the implementation of the templated version outside wont be instantiated automatically private T dp T dq int main Rational s1 2 Rational t s 2 stdcout tp tq n t 2 s stdcout tp tq n return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it47","content":" Uses traits classes for information about types STL has templates for containers algorithms iterators etc but also utilities Among its utilities templates there is advance cpp template move iter d units void advanceIterT iter DistT d forward if d 0 move iter backward Conceptually advance is iter d but only random access iterators support such Less powerful iterators need iteratively d times There are five categories of STL iterators Input iterators can only move forward one step at a time read what they point to only once like a read pointer to an input file eg istreamiterators Output iterators can only move forward one step at a time write what they point to only once eg ostreamiterators Forward iterator is more powerful They can move forward read or write what they point to multiple times The STL offers no singly linked list but if one were offered it would come with a forward iterator Bidirectional iterators adds the ability to move backward STL list iterator is in this category as for iterators for set multiset map and multimap Random access iterator adds to bidirectional iterator the ability to perform iterator arithmetic to jump forward or backward a distance in constant time Iterators for vector dequeu and string are random access iterators For each of the five categories C has a tag struct that serves to identify it cpp struct inputiteratortag struct outputiteratortag struct forwarditeratortag public inputiteratortag struct bidirectionaliteratortag public forwarditeratortag struct randomaccessiteratortag public bidirectionaliteratortag Now back to advance what we really want to do is this cpp template void advanceIterT iter DistT d if iter is a random access iterator iter d use iterator arithmetic for random access iters else if d 0 while d iter use iterative calls to else while d iter or for other iterator categories We need this information about iter during compilation which traits let you do Traits is not a keyword and needs to work for builtin types as well The standard technique is to put it into a template and one or more specializations of the template Like this cpp template template for information about struct iteratortraits iterator types The way iteratortraits works is that for each type IterT a typedef named iteratorcategory is declared in the struct iteratortraits This typedef identifies the iterator category of IterT iteratortraits implements this in two parts Any user defined iterator type must contain a nested typedef named iteratorcategory that identifies the appropriate tag struct Like for deque and list cpp template template params elided class deque public class iterator public typedef randomaccessiteratortag iteratorcategory template class list public class iterator public typedef bidirectionaliteratortag iteratorcategory iteratortraits just parrots back the iterator classs nested typedef the iteratorcategory for type IterT is whatever IterT says it is see Item 42 for info on the use of typedef typename template struct iteratortraits typedef typename IterTiteratorcategory iteratorcategory This works well for user defined types but not for iterators that are pointers since theres no such thing as a pointer with a nested typedef Thus the second part of the iteratortraits implementation handles iterators that are pointers by offering a partial template specialization for pointer types cpp template partial template specialization struct iteratortraits for builtin pointer types typedef randomaccessiteratortag iteratorcategory So to design a traits class identify some information youd like to make available for iterators their category choose a name to identify that information eg iteratorcategory provide a template and set of specializations eg iteratortraits taht contain the information for the types you want to support And advance looks like cpp template void advanceIterT iter DistT d if typeidtypename stditeratortraitsiteratorcategory typeidstdrandomaccessiteratortag But typeid is runtime while at compile time we have all the information We need an ifelse for types that is evaluated during compilation we can achieve this via overloading Like this cpp template use this impl for void doAdvanceIterT iter DistT d random access stdrandomaccessiteratortag iterators iter d template use this impl for void doAdvanceIterT iter DistT d bidirectional stdbidirectionaliteratortag iterators if d 0 while d iter else while d iter template use this impl for void doAdvanceIterT iter DistT d input iterators stdinputiteratortag if d 0 throw stdoutofrangeNegative distance see below while d iter Because forwarditeratortag inherits from inputiteratortag the version taking inputiteratortag will also work for forwarditerators And code for advance looks like cpp template void advanceIterT iter DistT d doAdvance call the version iter d of doAdvance typename that is stditeratortraitsiteratorcategory appropriate for iters iterator category So how to use a traits class create a set of overloaded worker functions that differ in a traits parameter Implement each function in accord with the traits information passed create a master or function template that calls the workers passing information provided by a traits class Traits are widely used in standard library Theres iteratortraits which offers iteratorcategory valuetype etc Theres also chartraits numericlimits And TR1 introduces isfundamental isarray isbaseof Takeaways Traits classes make information about types available during compilation Theyre implemented using templates and template specializations In conjunction with overloading traits classes make it possible to perform compiletime ifelse tests on types Snippet cpp traitsmcpp include include include demonstrates using traits to represent information about classes with iteratortraits as an example tag as type names struct myrandomaccessiterator struct mybidirectionaliterator we introduce iteratortraits class template such that we have a unified way to refer to the Iters iteratorcategory no matter if the Iters a builtin type pointer type partial specialization below or if Iters a user defined type the parrot back typedef here template struct iteratortraits typedef typename IterIteratoriteratorcategory iteratorcategory template partial template specialization struct iteratortraits for builtin pointer types typedef myrandomaccessiterator iteratorcategory MyVector wants a random access iterator Here we give iteratortraits something to parrot back template class MyVector public struct Iterator typedef myrandomaccessiterator iteratorcategory MyList wants a bidirectional iterator template class MyList public struct Iterator typedef mybidirectionaliterator iteratorcategory this shows using type as a parameter through overload with type as a parameter we at compile time knows which impl to call for different traits template void advanceIter iter D d doAdvanceiter d typename iteratortraitsiteratorcategory note how we use iteratortraits instead of IterIterator to refer to the iteratorcategory this is the unified way described in iteratortraits definition If we do use IterIteratoriteratorcategory we wouldnt need iteratortraits template but we also wouldnt be able to accommodate pointers being randomaccessiterators template void doAdvanceIter iter D d myrandomaccessiterator stdcout random access iterator advancen template void doAdvanceIter iter D d mybidirectionaliterator stdcout bidirectional iterator advancen int main MyVector myv advancemyv 5 MyList myl advancemyl 10 void p nullptr advancep 2 return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it48","content":" Be aware of template meta programming TMP is the technique that writes templatebased C programs that execute during compilation TMP makes some things easy that would otherwise be hard or impossible They also shift the work from runtime to compile time Like the example with advance from previous item this is a version where work is done at runtime cpp template void advanceIterT iter DistT d if typeidtypename stditeratortraitsiteratorcategory typeidstdrandomaccessiteratortag iter d use iterator arithmetic else for random access iters if d 0 while d iter use iterative calls to else while d iter or for other iterator categories Item 47 shows how traits can be more efficient in fact the traits approach is TMP If advance looks like the above consider this code cpp stdlistiterator iter advanceiter 10 move iter 10 elements forward wont compile with above impl This wont compile as iter d doesnt work on lists bidirectional iterator We know the line iter d will never be executed but compiler doesnt know that as typeid is a runtime check The traits TMP approach doesnt have the same problem Boosts mpl offers a higher level TMP syntax something that looks very different from ordinary C TMP uses recursive template instantiations to realize recursion loops Eg this TMP factorial cpp template general case the value of struct Factorial Factorial is n times the value of Factorial enum value n Factorialvalue template special case the value of struct Factorial Factorial is 1 enum value 1 You get the factorial of n by referring to Factorialvalue This uses enum hack described in item 2 Why is TMP worth knowing about some examples ensuring dimensional unit correctness like a variable representing mass cannot be assigned to a variable representing velocity but can be divided by such optimizing matrix operations This code cpp typedef SquareMatrix BigMatrix BigMatrix m1 m2 m3 m4 m5 create matrices and give them values BigMatrix result m1 m2 m3 m4 m5 compute their product The normal way calls for four temporary matrices and independent for loops using TMP expression templates its possible to avoid temporaries and merge the loops Generating custom design pattern implementations policybased design generative programming Takeaways Template metaprogramming can shift work from runtime to compiletime thus enabling earlier error detection and higher runtime performance TMP can be used to generate custom code based on combinations of policy choices and it can also be used to avoid generating code inappropriate for particular types Snippet cpp tmpmcpp include include demonstrates the classic template meta programming example Factorial a recursion based factorial unsigned int factorialunsigned int n return n 0 1 n factorialn 1 a tmp based factorial template struct factorialtmp enum value n factorialtmpvalue template struct factorialtmp enum value 1 int main stdcout factorial4 n computed at runtime stdcout factorialtmpvalue n computed at compile time return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it49","content":" Understand the behavior of the new handler When operator new cannot satisfy a memory allocation request it throws an exception Before throwing an exception new calls a client specifiable newhandler Client can specify this handler via setnewhandler in std cpp include namespace std typedef void newhandler newhandler setnewhandlernewhandler p throw A new handler takes nothing returns nothing and setnewhandler takes a handler and returns a handler a pointer to the handler function in effect The throw at the end suggests this function throws no exceptions Use setnewhandler like this function to call if operator new cant allocate enough memory void outOfMem stdcerr Unable to satisfy request for memoryn stdabort int main stdsetnewhandleroutOfMem int pBigDataArray new int100000000L When operator new cannot find enough memory it calls new handler until it can find enough memory So a well designed new handler should do the following Make more memory available Eg allocating a chunk at the program start and release when new handler is called Install a different new handler If the current handler doesnt know how to get more memory maybe it knows a different handler that can Deinstall the new handler ie pass null pointer to setnewhandler With no new handler installed next time when memory allocation is not successful operator new throws Throw exception Of badalloc or something derived from badalloc Such exceptions will not be captured by operator new and will propagate to the site originating the memory request Not return Typically abort or exit Sometimes you want different classes to have different behaviors when running out of memory cpp class X public static void outOfMemory class Y public static void outOfMemory X p1 new X if allocation is unsuccessful call XoutOfMemory Y p2 new Y if allocation is unsuccessful call YoutOfMemory You can achieve this behavior by having each class provide their own setnewhandler and operator new where the classs setnewhandler allows clients to specify new handlers for the class and the classs custom operator new ensures the classspecific handler is called instead of the global one Itll look something like this cpp class Widget public static stdnewhandler setnewhandlerstdnewhandler p throw static void operator newstdsizet size throwstdbadalloc private static stdnewhandler currentHandler stdnewhandler WidgetcurrentHandler 0 init to null in the class impl file static class members must be defined outside the class definition unless theyre const and integralsee Item 2 save and return the old set the new stdnewhandler Widgetsetnewhandlerstdnewhandler p throw stdnewhandler oldHandler currentHandler currentHandler p return oldHandler And to implement operator new we have an RAII class for holding the current handler cpp class NewHandlerHolder public explicit NewHandlerHolderstdnewhandler nh acquire current handlernh newhandler NewHandlerHolder release it stdsetnewhandlerhandler private stdnewhandler handler remember it NewHandlerHolderconst NewHandlerHolder prevent copying NewHandlerHolder see Item 14 operatorconst NewHandlerHolder And operator new cpp void Widgetoperator newstdsizet size throwstdbadalloc NewHandlerHolder install Widgets hstdsetnewhandlercurrentHandler newhandler return operator newsize allocate memory or throw restore global newhandler And client code cpp void outOfMem decl of func to call if mem alloc for Widget objects fails WidgetsetnewhandleroutOfMem set outOfMem as Widgets newhandling function Widget pw1 new Widget if memory allocation fails call outOfMem stdstring ps new stdstring if memory allocation fails call the global newhandling function if there is one Widgetsetnewhandler0 set the Widgetspecific newhandling function to nothing ie null Widget pw2 new Widget if mem alloc fails throw an exception immediately There is no new handling function for class Widget Then if we want to reuse this piece we make it a templated base cpp template mixinstyle base class for class NewHandlerSupport classspecific setnewhandler public support static stdnewhandler setnewhandlerstdnewhandler p throw static void operator newstdsizet size throwstdbadalloc other versions of op new see Item 52 private static stdnewhandler currentHandler template stdnewhandler NewHandlerSupportsetnewhandlerstdnewhandler p throw stdnewhandler oldHandler currentHandler currentHandler p return oldHandler template void NewHandlerSupportoperator newstdsizet size throwstdbadalloc NewHandlerHolder hstdsetnewhandlercurrentHandler return operator newsize this initializes each currentHandler to null template stdnewhandler NewHandlerSupportcurrentHandler 0 With this base to add classspecific setnewhandler support we make Widget inherit from NewHandlerSupport cpp class Widget public NewHandlerSupport as before but without declarations for setnewhandler or operator new This may look weird of Widget inheriting from Template but note that NewHandlerSupport does not use T all we need is another copy of NewHandlerSupport in particular its static data member currentHandler and the template only differentiates different classes This pattern of Widget inheriting from Template is called curiously recurring template pattern CRTP This pattern could easily lead to multiple inheritance about which youll want to consult Item 40 In the standards before 1993 new returns null when its unable to allocate the requested memory The standardization committee does not want to abandon the testfornull codebase before throwing badalloc is standardized so they provided alternative forms that does failure yields null These are the nothrow forms where they employ nothrow objects in cpp class Widget Widget pw1 new Widget throws badalloc if allocation fails if pw1 0 this test must fail Widget pw2 new stdnothrow Widget returns 0 if allocation for the Widget fails if pw2 0 this test may succeed In new stdnothrow Widget two things happen the nothrow version of operator new is called to allocate enough memory for Widget object and if that fails operator new returns null pointer If it succeeds Widget ctor is called and it may decide to allocate more memory itself and that allocation is not constrained to use nothrow new and if that allocation ctor throws this expression new stdnothrow Widget still throws In all likelihood you wont need to use new stdnothrow Widget Takeaways setnewhandler allows you to specify a function to be called when memory allocation requests cannot be satisfied nothrow new is of limited utility because it applies only to memory allocation subsequent constructor calls may still throw exceptions Snippet cpp newhandlermcpp include include include include demonstrates customizing setnewhandler the function new calls before throwing upon being unable to satisfy a memory allocation request and making a template that would allow easy customization of newhandler for that class using curiously recurring template pattern void outOfMem demonstrates how this is called repeatedly in an attempt to free up memory using this call static int globalCounter 0 if globalCounter 3 stdcerr Custom unable to satisfy request for memoryn else stdsetnewhandlernullptr globalCounter void clientOutOfMem static int customGlobalCounter 0 if customGlobalCounter 3 stdcerr Custom object allocation out of memory object customized n else abort customGlobalCounter RAII class for resetting newhandler class NewHandlerHolder public explicit NewHandlerHolderstdnewhandler nh acquire current handlernh newhandler NewHandlerHolder release it stdsetnewhandlerhandler NewHandlerHolderconst NewHandlerHolder delete NewHandlerHolder operatorconst NewHandlerHolder delete private stdnewhandler handler remember it templated base to support a class that enables customizing classspecific new handler template mixinstyle base class for class NewHandlerSupport classspecific setnewhandler public support static stdnewhandler setnewhandlerstdnewhandler p throw static void operator newstdsizet size throwstdbadalloc other versions of op new see Item 52 private static stdnewhandler currentHandler template stdnewhandler NewHandlerSupportsetnewhandlerstdnewhandler p throw stdnewhandler oldHandler currentHandler currentHandler p return oldHandler template void NewHandlerSupportoperator newstdsizet size throwstdbadalloc NewHandlerHolder hstdsetnewhandlercurrentHandler return operator newsize this initializes each currentHandler to null template stdnewhandler NewHandlerSupportcurrentHandler 0 client class this cannot be private inheritance since otherwise everything inside NewHandlerSupport setnewhandler included will be private We wont be able to do Widgetsetnewhandler We can use a different syntax NewHandlerSupportsetnewhandler but arguably more obscure class Widget public NewHandlerSupport public Widget default private int pdata100000000000000L int main stdsetnewhandleroutOfMem try int pBigDataArray new int100000000000000L catch const stdbadalloc ex swallow Widgetsetnewhandler0 set the Widgetspecific newhandling function to nothing ie null try Widget pw2 new Widget if mem alloc fails throw an exception immediately There is no new handling function for class Widget catch const stdbadalloc ex swallow WidgetsetnewhandlerclientOutOfMem set outOfMem as Widgets newhandling function Widget pw1 new Widget if memory allocation fails call outOfMem return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it5","content":" Know what functions C silents writes and calls If you dont declare a copy constructor copy assignment operator or destructor C declares and defines one for you If you dont declare any constructors C declares a default constructor for you Note that the generated dtor is not virtual unless the class is derived from a class with a virtual dtor Consider this example cpp template class NamedObject public NamedObjectconst char name const T value NamedObjectconst stdstring name const T value private stdstring nameValue T objectValue NamedObject no1Smallest Prime Number 2 NamedObject no2no1 calls compilersupplied copy constructor this calls copycon of string to copy nameValue and copies the bits of int instantiated type in this case a primitive type to copy objectValue no2 no1 copy assignment calls the copy assignment of string and copies over the bits of int in fact compiler generated copy assignment behaves as described above only when resulting code is both legal and has a reasonable chance of making sense suppose we have this instead template class NamedObject public this ctor no longer takes a const name because nameValue is now a referencetononconst string The char constructor is gone because we must have a string to refer to NamedObjectstdstring name const T value as above assume no operator is declared private stdstring nameValue this is now a reference const T objectValue this is now const then consider this code stdstring newDogPersephone stdstring oldDogSatch NamedObject pnewDog 2 NamedObject soldDog 36 p s what should happen to the data members in p Should the reference in p itself be modified to refer to s instead C doesnt allow that Should the referredto string be modified In this case C refuses to compile the code Similarly if the class contains const members copy assignment opr wont be generated Or if the class derives from a base class whose copy assignment is made private the classs copy assignment wont work Takeaways Compilers may implicitly generate a classs default constructor copy constructor copy assignment operator and destructor Snippet cpp defaultctordtorassignmentmcpp include include demonstrates a case where compiler will not generate copy assignment operator for you class MyClass public MyClassint x int y dxx dyy MyClass operatorconst MyClass rhs default this would still compile but does nothing it appears private const int dx int dy int main MyClass m13 4 MyClass m25 6 m1 m2 implicit copy assignment opr not defined return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it50","content":" Understand when it makes sense to replace new and delete Why would anybody want to replace the default new and delete To detect usage errors if new and delete keeps track of allocated addresses itd be easier to tell double free To detect overruns writing beyond the end of a block and underruns writing before the start of a block which you can do via new overallocating a block to start with to accommodate certain byte patterns and delete checks if those patterns are violated To improve efficiency the default needs to accommodate all patterns large small frequent infrequent and they need to worry about heap fragmentation They work reasonably well for everybody but really well for nobody If you understand the memory usage pattern of your program a custom new and delete can be much faster and use less memory To collect usage statistics to understand the memory usage pattern of your program How big are the sizes how long are their lifetimes FIFO LIFO or mostly random How the pattern changes over time High watermark Here is a new that has byte patterns to check for underrun overrun cpp static const int signature 0xDEADBEEF typedef unsigned char Byte this code has several flawssee below void operator newstdsizet size throwstdbadalloc using namespace std sizet realSize size 2 sizeofint increase size of request so2 signatures will also fit inside void pMem mallocrealSize call malloc to get theactual if pMem throw badalloc memory write signature into first and last parts of the memory staticcastpMem signature reinterpretcaststaticcastpMemrealSizesizeofint signature return a pointer to the memory just past the first signature return staticcastpMem sizeofint One issue is that this operator new does not conform with the convention eg it doesnt call newhanlde in a loop in case of badalloc Item 51 discusses that in more details Here look at a more subtle issue alignment many architectures require data of particular types be placed at particular kinds of addresses Eg an architecture might require that pointers occur at addresses that are a multiple of four fourbytealigned or doubles appear at a multiple of eight eightbytealigned Failure to follow such conventions may lead to hardware exceptions at runtime or slower execution C requires that all operator news return pointers that are suitably aligned for any data type malloc labors under the same requirement so returning a pointer allocated by malloc is safe however in our sample we are returning the result of malloc offset by an int On a machine where int is 4 bytes and double needs to be 8bytes aligned wed probably return a pointer with improper alignment tr1 adds support for discovering typespecific alignment requirements Writing a custom memory manager that almost works is pretty easy Writing one that works well is a lot harder As a general rule I suggest you not attempt it unless you have to In many cases you dont compilers tools may have a flag to facilitate debugging and logging functionality in their memory management functions Or you can use open source memory managers Pool from boost eg Pool deals with a large number of small objects well To summarize when you might want to customize new and delete in more details To detect usage errors To collect statistics about the use of dynamically allocated memory To increase the speed of allocation and deallocation profile before you do such To reduce the space overhead of default memory management To compensate for suboptimal alignment in the default allocator To cluster objects near one another to reduce page faults placement new in item 52 To obtain unconventional behaviors eg you want to work with shared memory but only have a C API to do it you could provide a C wrapper via custom new delete that call into the C API Takeaways There are many valid reasons for writing custom versions of new and delete including improving performance debugging heap usage errors and collecting heap usage information Snippet cpp replacenewmcpp include include include include demonstrates a custom albeit unideal operator new static const int signature 0xDEADBEEF typedef unsigned char Byte this code has several flaws not conforming to convention eg call handler in a loop and alignment may be broken returning a pointer from malloc is safe but offset this by an int may break alignment assumption for doubles void operator newstdsizet size throwstdbadalloc using namespace std cout using custom newn sizet realSize size 2 sizeofint increase size of request so2 signatures will also fit inside void pMem mallocrealSize call malloc to get theactual if pMem throw badalloc memory write signature into first and last parts of the memory staticcastpMem signature reinterpretcaststaticcastpMem realSize sizeofint signature return a pointer to the memory just past the first signature return staticcastpMem sizeofint int main double pData new double pData 100 stdcout pData n return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it51","content":" Adhere to conventions when writing new and delete Implementing an operator new requires having the right return value calling new handle when there isnt sufficient memory and coping with requests for no memory Youll also want to avoid the normal form of new Pseudocode for a nonmember new then looks like cpp void operator newstdsizet size throwstdbadalloc your operator new might using namespace std take additional params if size 0 handle 0byte requests size 1 by treating them as 1byte requests simple and effective while true attempt to allocate size bytes if the allocation was successful return a pointer to the memory allocation was unsuccessful find out what the current newhandling function is unfortunately there is no way to get at the newhandling function pointer directly so you have to call setnewhandler to find out what it is this is no longer the case in C11 C11 has getnewhandler newhandler globalHandler setnewhandler0 setnewhandlerglobalHandler in a multithreaded context you probably need some form of lock for these two operations if globalHandler globalHandler else throw stdbadalloc Now given this infinite loop in conventional operator new Item 49s requirement on new handler is clear new handler needs to make more memory available install a different newhandler deinstall the newhandler throw an exception of or derived from badalloc or fail to return Member operator new is inherited by derived classes This may have interesting implications Consider this code cpp class Base public static void operator newstdsizet size throwstdbadalloc class Derived public Base Derived doesnt declare operator new Derived p new Derived calls Baseoperator new Its possible that Bases operator new is geared towards allocating sizeofBase and to avoid allocating an unexpected amount of memory with Bases operator new we could do this cpp void Baseoperator newstdsizet size throwstdbadalloc if size sizeofBase if size is wrong return operator newsize have standard operator new handle the request otherwise handle the request here If youd like to control memory allocation for arrays on a perclass basis you need to implement array new operator new as well You cant assume inside operator new the size of each object is sizeofBase and you cant assume the number of objects is requestedSize sizeofBase And the size passed to operator new may ask for more memory than filled with objects as dynamically allocated arrays may include extra space to store number of elements allocated Item 16 For operator delete C guarantees it needs to be safe to delete nullptr NULL as before 11 and you could honor this guarantee with pseudocode like this cpp void operator deletevoid rawMemory throw if rawMemory 0 return do nothing if the null pointer is being deleted deallocate the memory pointed to by rawMemory The member version of this is simple too except that you need to check the size of the deleted object in case a Baseoperator delete tries to delete an object of Derived cpp class Base same as before but now public operator delete is declared static void operator newstdsizet size throwstdbadalloc static void operator deletevoid rawMemory stdsizet size throw void Baseoperator deletevoid rawMemory stdsizet size throw if rawMemory 0 return check for null pointer if size sizeofBase if size is wrong operator deleterawMemory have standard operator return delete handle the request deallocate the memory pointed to by rawMemory return Interestingly the sizet C pass to delete may not be correct if the object being deleted was derived from a class lacking a virtual dtor Item 7 Takeaways operator new should contain an infinite loop trying to allocate memory should call the new handler if it cant satisfy a memory request and should handle requests for zero bytes Classspecific versions should handle requests for larger blocks than expected operator delete should do nothing if passed a pointer that is null Classspecific versions should handle blocks that are larger than expected Snippet cpp adheretoconventionswhencustomizingnewanddeletemcpp include include include include demonstrates global and classspecific new that conforms to the conventional behaviors a conformant operator new needs to account for zeroallocation call newhandler in a loop throw stdbadalloc if null handler or calling a conformant one void operator newstdsizet size throwstdbadalloc stdcout custom global operator newn if size 0 size 1 while true void handle mallocsize if handle return handle else stdnewhandler currentHandler stdgetnewhandler since c11 if currentHandler currentHandler else throw stdbadalloc class Base public Base ddata10 virtual Base default static void operator newstdsizet size throwstdbadalloc stdcout Base class operator newn if size sizeofBase stdcout Allocation not of sizeofBase falling back to defaultn return operator newsize in this custom new we dont do anything in addition return operator newsize void print const stdcout ddata n private int ddata class Derived public Base public private char dchar int main double pData new double delete pData Base pd new Derived delete pd Base pb new Base delete pb it would seem without an operator new overload the global default is used with no issues Base pds new Base4 pds3print delete pds return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it52","content":" Write placement delete if you write placement new Recall that when you call this cpp Widget pw new Widget Two functions are called one to operator new to allocate memory a second to Widgets default constructor If step 1 succeeds but 2 fails C runtime system needs to deallocate the memory from step 1 as pw is never assigned The runtime system needs to figure out which operator delete to call since there may be many suppose you have global new and delete and also classspecific nonnormal forms of new and delete Like cpp class Widget public static void operator newstdsizet size nonnormal stdostream logStream form of new throwstdbadalloc static void operator deletevoid pMemory normal class stdsizet size throw specific form of delete When an operator new takes extra parameters other than the mandatory sizet the operator is known as placement new A particularly useful version of placement new is one that takes a pointer specifying where the object should be placed Like this cpp void operator newstdsizet void pMemory throw placement new This version is in stds and its used inside vector to create objects in vectors unused capacity The term placement new is overridden when people talk about it they are usually referring to this particular function Now if we go back to check how Widgets placement new can be problematic say we have cpp Widget pw new stdcerr Widget call operator new passing cerr as the ostream this leaks memory if the Widget constructor throws When deleting pw due to exception in step 2 as described above step 1 is operator new and step 2 is ctor the runtime system looks for an operator delete taking the same number and type of extra arguments as operator new In this case the runtime system would settle on this a placement delete cpp void operator deletevoid stdostream throw But Widgets placement delete has a different interface meaning in this case no operator delete is called if Widget throws an exception To eliminate this possible leak Widget needs to be like cpp class Widget public static void operator newstdsizet size stdostream logStream throwstdbadalloc static void operator deletevoid pMemory throw static void operator deletevoid pMemory stdostream logStream throw Now if step 2 throws the 2nd delete handles it But if no exceptions in step 2 and we get a delete pw in client code this call never calls the placement version meaning to forestall memory leaks associated with placement new you need to provide both the placement delete and the normal version without providing the normal version the placement delete would hide the global regular version And because of hiding if you want the client to be able to use the normal operator new as well as the placement version youll need to have both cpp class Base public static void operator newstdsizet size this new hides stdostream logStream the normal throwstdbadalloc global forms Base pb new Base error the normal form of operator new is hidden Base pb new stdcerr Base fine calls Bases placement new Similarly operator news in derived classes hide both global and inherited versions of operator new cpp class Derived public Base inherits from Base above public static void operator newstdsizet size redeclares the normal throwstdbadalloc form of new Derived pd new stdclog Derived error Bases placement new is hidden Derived pd new Derived fine calls Deriveds operator new By default C offers the following forms of new at global scope cpp void operator newstdsizet throwstdbadalloc normal new void operator newstdsizet void throw placement new void operator newstdsizet nothrow new const stdnothrowt throw see Item 49 If you declare any operator news in your class youll hide all three Unless you mean to forbid the clients from using these make sure to make these available in addition to versions you declare For each operator new you make available of course be sure to offer the corresponding operator delete too If you want these functions to behave in the usual way have your classspecific versions call the global versions Like this base class cpp class StandardNewDeleteForms public normal newdelete static void operator newstdsizet size throwstdbadalloc return operator newsize static void operator deletevoid pMemory throw operator deletepMemory placement newdelete static void operator newstdsizet size void ptr throw return operator newsize ptr static void operator deletevoid pMemory void ptr throw return operator deletepMemory ptr nothrow newdelete static void operator newstdsizet size const stdnothrowt nt throw return operator newsize nt static void operator deletevoid pMemory const stdnothrowt throw operator deletepMemory And a derived class of this can avoid hiding via using statements cpp class Widget public StandardNewDeleteForms inherit std forms public using StandardNewDeleteFormsoperator new make those using StandardNewDeleteFormsoperator delete forms visible static void operator newstdsizet size add a custom stdostream logStream placement new throwstdbadalloc static void operator deletevoid pMemory add the corres stdostream logStream ponding place throw ment delete Takeaways When you write a placement version of operator new be sure to write the corresponding placement version of operator delete If you dont your program may experience subtle intermittent memory leaks When you declare placement versions of new and delete be sure not to unintentionally hide the normal versions of those functions Snippet cpp writeplacementdeleteifyouwriteplacementnewmcpp include include include include include demonstrates placement news and deletes the necessity of pairing them and how not to have them hide the global default versions class MyClass public MyClass throw stdruntimeerrorthis will leak with placement new this is a placement new in the sense that it takes extra arguments than the global one there is a global placement new as well taking a pointer of where the object should be located the term placement new is overloaded in the sense that when people refer to it they usually mean the latter static void operator newstdsizet size stdostream out throwstdbadalloc out MyClass class placement newn always fall back to global one no need to check if the size matches here return operator newsize without a placement delete as this ctor throws the runtime system will not be able to free memory allocated by the placement new private int ddata class Base public Base throw stdruntimeerrorthis will not leak with placement new static void operator newstdsizet size stdostream out throwstdbadalloc out Base class placement new customn return operator newsize a matching operator delete needs to be present static void operator deletevoid ptr stdostream out throw out Base class placement delete called by C runtime systemn return operator deleteptr well need a nonplacement operator delete as well as the placement one hides the global one user calls on delete pb will hit this version static void operator deletevoid ptr throw stdcout Base class regular dtor customn return operator deleteptr similarly to not hide the global operator new due to the presence of our our placement new we need to declare a global version here unless you want to limit your clients to use the placement new you provided static void operator newstdsizet size throwstdbadalloc stdcout Base class regular new customn return operator newsize the other two versions of new that exist globally are the placement new with void parameter and nothrow new for brevity we dont show them here private char dchar int main try MyClass ptr new stdcout MyClass catch const stdruntimeerror ex stdcout Object of MyClass would leak if no matching placement delete n valgrind definitely lost 4 bytes due to missing matching placement delete try Base pb new stdcout Base if pb delete pb catch const stdruntimeerror ex swallow try Base pb1 new Base if pb1 delete pb1 catch const stdruntimeerror ex swallow return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it53","content":" Pay attention to compiler warnings In C its a good bet the compiler writer konws better about whats going on than you do Consider this code cpp class B public virtual void f const class D public B public virtual void f While the intention is to redefine Bf in Df what actually happens is Df hides Bf Some compilers would emit a warning for this Its generally best practice to write code that compiles warning free at the highest warning level Warnings are compiler implementation dependent so dont get sloppy in your coding and hope that compilers spot the error for you Takeaways Take compiler warnings seriously and strive to compile warningfree at the maximum warning level supported by your compilers Dont become dependent on compiler warnings because different compilers warn about different things Porting to a new compiler may eliminate warning messages youve come to rely on Snippet cpp compilerwarningunexpectedhidingmcpp include include include include include demonstrates a case where hiding happens instead of overriding and compiler would emit a warning about it class Base public virtual Base default virtual void f const stdcout Basefn class Derived public Base public with Wall clang warns about this hiding behavior virtual void f stdcout f hiding not override delete pd return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it54","content":" Familiarize yourself with the standard library including tr1 The standard for this book was ratified in 1998 and 2003 saw a minor bugfix update C0x was meant to be a followup to 2003 tr1 heralds the new features in a new release of C Major parts of standard library as specified by C98 STL containers iterators algorithms function objects and their adapters Iostreams Support for internationalization unicode chars wstring wchart Support for numeric processing complex numbers arrays of pure values valarray An exception hierarchy exception from which logicerror runtimeerror etc derive C89s standard library TR1 of C98 has sharedptr weakptr function to represent any callable entity eg function bind Hash tables unorderedmap unorderedmultimap unorderedset unorderedmultiset Regular expressions tuple array fixed size during compilation no dynamic memory memfun adapting member function pointers referencewrapper as if containers hold references Random number generation Mathematical special functions C99 compatibility extensions And to better support templates tmp Type traits given T this can reveal if T is a builtin type offers a virtual dtor is an empty class is implicitly convertible so some U proper alignment etc resultof deduce the return type of function calls As pure additions not to replace anything in standard Boost offers these functionality and more sometimes not the same as specified by the standards Takeaways The primary standard C library functionality consists of the STL iostreams and locales The C99 standard library is also included TR1 adds support for smart pointers eg sharedptr generalized function pointers function hashbased containers regular expressions and 10 other components TR1 itself is only a specification To take advantage of TR1 you need an implementation One source for implementations of TR1 components is Boost"},{"title":"unprocessed","href":"/effectives/ecpp/it55","content":" Familiarize yourself with Boost Searching for a collection of highquality open source platform and compilerindependent libraries Look to Boost Interested in joining a community of ambitious talented C developers working on stateoftheart library design and implementation Look to Boost Want a glimpse of what C might look like in the future Look to Boost Boost as of the time of this book contains libraries of these categories String and text processing typesafe printf regular expressions tokenizing and parsing Containers fixedsized arrays variablesized bitsets multidimensional arrays Function objects and higher order programming like lambda Generic programming with extensive traits TMP MPL library Math and numerics rational numbers octonions and quaternions greatest common divisor and least common multiple computations and random numbers Correctness and testing Data structures like typesafe unions Interlanguage support like C with python Memory like pool Miscellaneous CRC checking traversing file system datetime etc Boost does not cover GUI DB communication etc Takeaways Boost is a community and web site for the development of free open source peerreviewed C libraries Boost plays an influential role in C standardization Boost offers implementations of many TR1 components but it also offers many other libraries too"},{"title":"unprocessed","href":"/effectives/ecpp/it6","content":" Explicitly disallow the use of compiler generated functions you dont want As of C03 the key to the solution is that all the compiler generated functions are public To prevent these functions from being generated you must declare them yourself but there is nothing that requires that you declare them public Instead declare the copy constructor and the copy assignment operator private By declaring a member function explicitly you prevent compilers from generating their own version and by making the function private you keep people from calling it And you dont implement it so that member functions or friends cannot call these private functions Or you could declare a base class Uncopyable whose copycon and assignment opr are made private and make your class private inherit from Uncopyable in which case when trying to call your classs copycon or assignment opr within a friend youd get compiler error instead of linker error which is preferable This would be obsolete by EMC item 11 Takeaways To disallow functionality automatically provided by compilers declare the corresponding member functions private and give no implementations Using a base class like Uncopyable is one way to do this"},{"title":"unprocessed","href":"/effectives/ecpp/it7","content":" Declare dtors virtual in polymorphic base classes Declare a virtual destructor in a class if and only if that class contains at least one virtual function if it does not it usually indicates the class is not meant to be inherited from The bottom line is that gratuitously declaring all destructors virtual is just as wrong as never declaring them virtual It is possible to get bitten by the nonvirtual destructor problem even in the complete absence of virtual functions For example the standard string type contains no virtual functions but misguided programmers sometimes use it as a base class anyway cpp class SpecialString public stdstring bad idea stdstring has a nonvirtual destructor stdstring ptr new SpecialString undefined behavior delete ptr The same analysis applies to any class lacking a virtual destructor including all the STL container types eg vector list set stdunorderedmap Dont inherit from STL container types Occasionally it can be convenient to give a class a pure virtual destructor if you want an abstract class one that cannot be instantiated but you dont have any pure virtual functions for it You could give it a pure virtual dtor but remember you have to provide a definition for this pure virtual dtor cpp class AWOV AWOV Abstract wo Virtuals public virtual AWOV 0 declare pure virtual destructor AWOVAWOV definition of pure virtual dtor Reason for needing a definition being when destroying an object of a derived class the base classs dtor will be called after that of the derived class and you provide a definition to base classs dtor since otherwise the linker would complain Some classes are designed to be used as base classes yet are not designed to be used polymorphically Such classes examples include Uncopyable from Item 6 are not designed to allow the manipulation of derived class objects via base class interfaces As a result they dont need virtual destructors Takeaways Polymorphic base classes should declare virtual destructors If a class has any virtual functions it should have a virtual destructor Classes not designed to be base classes or not designed to be used polymorphically should not declare virtual destructors Snippet cpp declaredtorvirtualpolymorphicbaseclassmcpp include include demonstrates pure virtual dtors a minor use case and undefined behavior in deleting ChildClass object using a BaseClass pointer whose dtor is not virtual class PureVirtualBase public virtual PureVirtualBase 0 PureVirtualBasePureVirtualBase class ChildPureVirtualBase public PureVirtualBase public void print const stdcout ChildPureVirtualBasen class PolymorphicBase public PolymorphicBase dx10 PolymorphicBase stdcout base class dtorn virtual void print stdcout dx n private int dx class Child public PolymorphicBase public Child dy20 Child stdcout child class dtorn virtual void print stdcout dy print undefined behavior dtor of child will not be called delete c it would appear that nothing happens but expect heap to be corrupted return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it8","content":" Prevent exceptions from leaving dtors Consider this code cpp class Widget public Widget assume this might emit an exception void doSomething stdvector v v is automatically destroyed here If an exception is thrown when the first element in v is destroyed C has to finish destroying all the remaining Widgets in v say another exception is thrown then well have two exceptions at the same time which is not allowed in C and causes undefined behavior or program termination What if your dtor needs to perform an action that might fail and cause an exception to be thrown Suppose you have this class for DBConnection cpp class DBConnection public static DBConnection create function to return DBConnection objects params omitted for simplicity void close close connection throw an exception if closing fails and you want to have an RAII class that closes connection on dtor DBConn make sure database connections are always closed dbclose There are two primary ways to solve this issue One is to terminate the program if close throws by calling abort cpp DBConnDBConn try dbclose catch make log entry that the call to close failed stdabort This is reasonable if the program cannot continue to run after an exception is encountered during destruction Calling abort forestalls undefined behavior The other is to swallow it cpp DBConnDBConn try dbclose catch make log entry that the call to close failed In general suppressing exception is a bad idea since it suppresses the important information that something failed However in this case this would still be preferable to undefined behavior or premature termination For this to be viable the program must be able to continue execution even if an error occurred and has been ignored Neither way is ideal In this case we could expose close to clients so that they have a chance of handling exceptions from close operation And if the client doesnt do it we still fall back to aborting or swallowing cpp class DBConn public void close new function for client use dbclose closed true DBConn if closed try close the connection dbclose if the client didnt catch if closing fails make log entry that call to close failed note that and terminate or swallow private DBConnection db bool closed Does this violate the principle of making interfaces easy to use We would argue not as in this example telling clients to call close themselves gives them an opportunity to deal with errors they would otherwise have no chance to react to And if they dont want to deal with it theyd still fall back on the dtors default action Takeaways Destructors should never emit exceptions If functions called in a destructor may throw the destructor should catch any exceptions then swallow them or terminate the program If class clients need to be able to react to exceptions thrown during an operation the class should provide a regular ie nondestructor function that performs the operation Snippet cpp preventexceptionsfromleavingdtormcpp include include demonstrates an RAII class design where dtor may call an operation that throws but dtor handles it by swallowing That is only if client does not first call this operation that could throw in order to handle the exception class DBConn public DBConn dhandle1 disOpenfalse void openint handle dhandle handle disOpen true void close if disOpen dhandle 10 throw stdruntimeerrorblow up disOpen false DBConn if disOpen try close catch stdcout exception swallowed in dtorn private int dhandle bool disOpen int main DBConn conn connopen11 const bool clientCares true if clientCares client is given the opportunity to handle this exception try connclose catch const stdexception e stdcout Client handles ewhat n else or if client does not care exception wont leave dtor return 0 "},{"title":"unprocessed","href":"/effectives/ecpp/it9","content":" Never call virtual functions during ctor or dtor Suppose you have this code cpp class Transaction base class for all public transactions Transaction virtual void logTransaction const 0 make typedependent log entry TransactionTransaction implementation of base class ctor logTransaction as final action log this transaction class BuyTransaction public Transaction derived class public virtual void logTransaction const how to log trans actions of this type class SellTransaction public Transaction derived class public virtual void logTransaction const how to log trans actions of this type consider what happens when this is executed BuyTransaction b Base class ctor would have to be called first The version of logTransaction thats called is the one in Transaction not the one in BuyTransaction even though the type of object being created is BuyTransaction During base class construction virtual functions never go down into derived classes Instead the object behaves as if it were of the base type The reason for such is that while base class ctor is being called derived class members arent initialized yet If we do call into derived classs overriden functions then wed run into undefined behavior when those functions refer to members of the derived class Actually during base class construction of a derived class object the type of object is that of the base class Not only do virtual functions resolve to those of the base class but also parts of the language using runtime type information eg dynamiccast and typeid treat the object as a base class type during base class ctor the derived part does not exist yet so its best to treat the objects type as that of the base An object doesnt become a derived class object until execution of a derived class constructor begins The same reasoning applies during destruction Once a derived class destructor has run the objects derived class data members assume undefined values so C treats them as if they no longer exist Upon entry to the base class destructor the object becomes a base class object and all parts of C virtual functions dynamiccasts etc treat it that way In the above example codes case it shouldnt link as logTransaction in base class is pure virtual Some compilers would also issue warnings about this This more insidious version however will likely compile and link cpp class Transaction public Transaction init call to nonvirtual virtual void logTransaction const 0 private void init logTransaction that calls a virtual The only way to avoid this problem is to make sure that none of your constructors or destructors call virtual functions on the object being created or destroyed and that all the functions they call obey the same constraint How do we achieve what we wanted to do then One way is to not make logTransaction virtual but rather parameterize the information it needs Like this cpp class Transaction public explicit Transactionconst stdstring logInfo void logTransactionconst stdstring logInfo const now a non virtual func TransactionTransactionconst stdstring logInfo logTransactionlogInfo now a non virtual call class BuyTransaction public Transaction public BuyTransaction parameters TransactioncreateLogStringparameters pass log info to base class constructor private static stdstring createLogString parameters Note the private static function createLogString using a helper function like this is often more readable and making it avoids accidentally using the data members of BuyTransaction in createLogString whose uninitialized state is the reason why we parameterize the message in the first place Takeaways Dont call virtual functions during construction or destruction because such calls will never go to a more derived class than that of the currently executing constructor or destructor Snippet cpp nevercallvirtualfunctionsinsidectordtormcpp include include include demonstrates a way to avoid calling virtual functions in ctor class Transaction public Transactionconst stdstring msg you may be tempted to make logTransaction virtual and overridden in both children below to print their corresponding messages Dont As if you do here itll always call the base classs logTransaction logTransactionmsg virtual Transaction 0 private void logTransactionconst stdstring msg stdcout msg n TransactionTransaction class BuyTransaction public Transaction public BuyTransaction TransactiongetTypeMessage private static stdstring getTypeMessage return one buy transaction int dsize class SellTransaction public Transaction public SellTransactionint size TransactiongetTypeMessagesize private static stdstring getTypeMessageint size stdstringstream ss ss one sell transaction of size size return ssstr int dsize int main BuyTransaction bt SellTransaction st20 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it1","content":" Understand type deduction cpp template void fT param param is a reference same case goes for param is a pointer int x 27 x is an int const int cx x cx is a const int const int rx x rx is a reference to x as a const int fx T is int params type is int fcx T is const int params type is const int frx T is const int params type is const int the refness of rx is dropped cpp template void fT param param is now a universal reference int x 27 as before const int cx x as before const int rx x as before fx x is lvalue so T is int params type is also int fcx cx is lvalue so T is const int params type is also const int frx rx is lvalue so T is const int params type is also const int f27 27 is rvalue so T is int params type is therefore int cpp template void fT param param is now passed by value int x 27 as before const int cx x as before const int rx x as before fx Ts and params types are both int fcx Ts and params types are again both int frx Ts and params types are still both int cpp Array declaration in function params decays into pointer void myFuncint param void myFuncint param same function as above cpp template void fT param param is now passed by value const char name J P Briggs names type is const char13 fname name is array but T deduced as const char If instead template void fT param template with byreference parameter fname pass array to f and T is actually deduced to be an array const char 13 cpp Functions too can decay into pointer void someFuncint double someFunc is a function type is voidint double template void f1T param in f1 param passed by value template void f2T param in f2 param passed by ref f1someFunc param deduced as ptrtofunc type is void int double f2someFunc param deduced as reftofunc type is void int double Takeaway During template type deduction arguments that are references are treated as nonreferences ie their referenceness is ignored When deducing types for universal reference parameters lvalue arguments get special treatment When deducing types for byvalue parameters const andor volatile arguments are treated as nonconst and nonvolatile During template type deduction arguments that are array or function names decay to pointers unless theyre used to initialize references Snippet cpp typedeductionmcpp include include demonstates rule 1 and rule 2 in template type deduction and that a function template parameter expecting an lvalue reference cannot work with a given rvalue template void funclvaluereferenceT param stdcout param n template void funcuniversalreferenceT param stdcout param n int main int x 42 cannot pass compile template expects a lvalue reference funclvaluereference27 works fine funclvaluereferencex funcuniversalreference27 funcuniversalreferencex const int crx x funclvaluereferencecrx funcuniversalreferencecrx return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it10","content":" Item 10 prefer scoped enums to unscoped enums As a general rule declaring a name inside curly braces limits the visibility of that name to the scope defined by the braces Not so for the enumerators declared in C98style enums The names of such enumerators belong to the scope containing the enum definition is leaked into the enclosing scope thus unscoped enums and that means that nothing else in that scope may have the same name cpp C03 enum Color black white red black white red are in same scope as Color auto white false error white already declared in this scope C11 enum class Color black white red black white red are scoped to Color auto white false fine no other white in scope Color c white error no enumerator named white is in this scope Color c Colorwhite fine auto c Colorwhite also fine and in accord with Item 5s advice Scoped enums are declared via enum class They are referred to as enum classes as well Enum classes Reduce namespace pollution Are strongly typed no implicit conversion to other types while unscoped enum implicitly convert to integral types Enum classes can be forward declared by default the underlying type for scoped enums in int so the compiler knows the size of a forward declared enum You can override the default underlying type Unscoped enum can be forward declared only if the underlying type is specified cpp enum class Status underlying type is int enum class Status stduint32t underlying type for Status is stduint32t from enum Color stduint8t fwd decl for unscoped enum underlying type is stduint8t Underlying type specifications can also go on an enums definition enum class Status stduint32t good 0 failed 1 incomplete 100 corrupt 200 audited 500 indeterminate 0xFFFFFFFF There is a case where unscoped enums may be useful due to its implicit conversion to integral types say you have the following cpp using UserInfo type alias see Item 9 stdtuple reputation elsewhere you see this UserInfo uInfo object of tuple type auto val stdgetuInfo get value of field 1 You probably dont want to remember what fields 1 2 3 are so you could have this instead enum UserInfoFields uiName uiEmail uiReputation UserInfo uInfo as before auto val stdgetuInfo ah get value of email field And the corresponding code with scoped enums is substantially more verbose enum class UserInfoFields uiName uiEmail uiReputation UserInfo uInfo as before auto val stdgetstaticcastUserInfoFieldsuiEmail uInfo You could get rid of some of this verbosity using a function but that function has to be evaluated at compile time since stdget is a template template C14 constexpr auto toUTypeE enumerator noexcept return staticcaststdunderlyingtypetenumerator And youd be able to do auto val stdgetuInfo Takeaways C98style enums are now known as unscoped enums Enumerators of scoped enums are visible only within the enum They convert to other types only with a cast Both scoped and unscoped enums support specification of the underlying type The default underlying type for scoped enums is int Unscoped enums have no default underlying type implementationdependent integral type that can represent all enumerator values Scoped enums may always be forwarddeclared Unscoped enums may be forwarddeclared only if their declaration specifies an underlying type Snippet cpp scopedenumsmcpp include include demonstrates how unscoped enum names are leaked to the scope containing its definition int main enum State good bad int good 2 compiler error redefinition enum class ScopedState char good good1 bad int good1 2 MyList list2 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it11","content":" Item 11 prefer deleted functions to private undefined ones You often want to suppress the special member functions that the compiler generates for you like copycon and assignment operator In C03 you do that with private undefined copycon and assignment opr In C11 you do that with declaration with delete cpp C03 template class charT class traits chartraits class basicios public iosbase public private basiciosconst basicios not defined basicios operatorconst basicios not defined C11 template class charT class traits chartraits class basicios public iosbase public basiciosconst basicios delete basicios operatorconst basicios delete these are public since compiler checks deleted status before accessibility The advantages of delete delete will result in better error messages always at compile time as opposed to friends members seeing undefined symbols any functions can be deleted while only member functions can be made private You can use this to get rid of unwanted implicit conversions or unwanted template instantiation Eg cpp bool isLuckyint number if isLuckya is a a lucky number if isLuckytrue is true if isLucky35 should we truncate to 3 before checking for luckiness Such implicit conversions to int are undesirable you could do bool isLuckyint number original function bool isLuckychar delete reject chars bool isLuckybool delete reject bools bool isLuckydouble delete reject doubles and floats and youll have if isLuckya error call to deleted function if isLuckytrue error if isLucky35f error And for unwanted template instantiations template void processPointerT ptr template void processPointervoid delete template void processPointerchar delete template void processPointerconst void delete template void processPointerconst char delete Similarly for template class member functions class Widget public template void processPointerT ptr template still void WidgetprocessPointervoid delete public but deleted Takeaways Prefer deleted functions to private undefined ones Any function may be deleted including nonmember functions and template instantiations Snippet cpp deletedfunctionsmcpp include include demonstrates delete can be used to get rid of unwanted implicit conversions and template instantiations bool isLuckyint number return true original function bool isLuckychar delete reject chars bool isLuckybool delete reject bools bool isLuckydouble delete reject doubles and floats class Widget public template void processPointerT ptr template void WidgetprocessPointervoid delete int main isLucky1 isLuckytrue compiler calling explicitly deleted functions Widget w1 int i 1 void ptr void i w1processPointerptr compiler calling explicitly deleted functions return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it12","content":" Item 12 declare overriding functions override override has nothing to do with overload override made it possible to invoke a derived class function through a base class interface For override to occur Base class function must be virtual Base and derived function names must be identical except in dtor Parameter types must be identical constness of the base and derived functions must be identical return types and exception sepcifications must be compatible C11 functions reference qualifier must be identical Reference qualifiers can make a member function available to lvalues or rvalues only cpp class Widget public void doWork this version of doWork applies only when this is an lvalue void doWork this version of doWork applies only when this is an rvalue Widget makeWidget factory function returns rvalue Widget w normal object an lvalue wdoWork calls WidgetdoWork for lvalues ie WidgetdoWork makeWidgetdoWork calls WidgetdoWork for rvalues ie WidgetdoWork How is this useful Consider this code cpp class Widget public using DataType stdvector see Item 9 for info on using DataType data return values private DataType values Widget w auto vals1 wdata copy wvalues into vals1 Now suppose we have a factory function that creates Widgets Widget makeWidget auto vals2 makeWidgetdata copy values inside the Widget into vals2 However in this case we dont actually need to copy since the original copy in makeWidget is a temporary Move is preferable Compiler may be able to optimize this but dont depend on it We could have this instead move if data is called on an rvalue copy if its called on an lvalue class Widget public using DataType stdvector DataType data for lvalue Widgets return values return lvalue DataType data for rvalue Widgets return stdmovevalues return rvalue private DataType values Back to the matter at hand small mistakes can cause you to think something overrides while in fact it doesnt Eg cpp class Base public virtual void mf1 const virtual void mf2int x virtual void mf3 void mf4 const class Derived public Base public virtual void mf1 virtual void mf2unsigned int x virtual void mf3 virtual void mf4 const Compilers dont have to emit warnings in this case Because declaring override is important to get right and easy to get wrong C11 introduces declaring a function override In which case you are asking the compiler to help check something is indeed overridden cpp class Derived public Base public virtual void mf1 override virtual void mf2unsigned int x override virtual void mf3 override virtual void mf4 const override It also helps you gauge the ramifications if you are contemplating changing the signature of a virtual function in a base class You can see how many derived classes fails to compile override and final are contextual keywords they are reserved only in a context In overrides case at the end of a member function declaration Takeaways Declare overriding functions override Member function reference qualifiers make it possible to treat lvalue and rvalue objects this differently Snippet cpp overridemcpp include include using namespace std class Base public virtual void funcint i cout base i n virtual Base class Derived public Base public virtual void funcunsigned int i override cout derived i n this is not a valid override Without override keyword the compiler wont tell you its not without Wall with Wall it will warn about hiding with override this will be an error virtual void funcconst int i override cout derived i funcx calls base func int y 5 dfuncy calls base func delete d return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it13","content":" Item 13 prefer constiterators to iterators This is in line with use const whenever possible Problem is STL in C98 has constiterators but a lot of functions expect iterators eg vectorinsert as opposed to constiterators making adopting constiterators hard Eg cpp To find a value replace it with another one if found otherwise insert to the end This is possible with constiterators in C11 not before stdvector values as before auto it use cbegin stdfindvaluescbegin valuescend 1983 and cend valuesinsertit 1998 before you may be able to do something like this typedef stdvectoriterator IterT type typedef stdvectorconstiterator ConstIterT defs stdvector values ConstIterT ci stdfindstaticcastvaluesbegin cast staticcastvaluesend cast 1983 valuesinsertstaticcastci 1998 may not compile To write maximally generic library code take into account that some containers and containerlike data structures offer begin and end plus cbegin cend rbegin etc as nonmember functions rather than members This is the case for builtin arrays for example and its also the case for some thirdparty libraries with interfaces consisting only of free functions Maximally generic code thus uses nonmember functions rather than assuming the existence of member versions C11 had nonmember versions of begin end but forgot to add nonmember versions of cbegin cend rbegin rend crbegin This is corrected in C14 Takeaways Prefer constiterators to iterators In maximally generic code prefer nonmember versions of begin end rbegin etc over their member function counterparts Snippet cpp constiteratormcpp include include include include demonstrates vectorinsert when given constiterator and a usage of nonmember begin int main stdvector v stdvectorconstiterator ci vcbegin on OSX this compiles even with stdgnu98 vinsertci 3 auto ci2 stdcbeginv vinsertci2 4 for auto ci v stdcout ci n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it14","content":" Item 14 declare functions noexcept if they wont emit exceptions C98 compiler offers no help in checking exception specification programmers summarize the possible exception specifications and update them as code changes C11 allows indication of if a function may emit any exceptions Why noexcept Failure to declare a function noexcept when you know that it wont throw an exception is poor interface specification whether a function is noexcept is as important a piece of information as whether a member function is const Compiler may generate more efficient code for noexcept the difference between unwinding the stack and possibly unwinding it has a surprisingly large impact on code generation cpp RetType functionparams noexcept most optimizable RetType functionparams throw less optimizable RetType functionparams less optimizable More motivations in practice Think of vectors insert implementation In C98 its allocate copy then remove This has strong exception safe guaranteeecppit2631implementationsmdstriveforexceptionsafecode if an exception is thrown during copying the state of the vector will be unchanged pushback does not guarantee noexcept though In C11 we can leverage move If move doenst have noexcept pushbacks exception safety guarantee is violated a move throwing an exception may cause the vector to be in a different state What about move back on exception Still no guarantee since move back may throw an exception itself Thus the pushback implementation in C11 leverages move when it can ie move has been declared noexcept Other STL functions do that too move if you can copy if you must swap functions comprise another case where noexcept is desirable Heavily used in STL and assignment operator Often times a whether swap is defined as noexcept depends on userdefined types are noexcept conditionally noexcept eg cpp template void swapT aN see T bN noexceptnoexceptswapa b below template struct pair void swappair p noexceptnoexceptswapfirst pfirst noexceptswapsecond psecond Optimization is important but correctness is more important Declare something noexcept only if you are willing to commit to this function being noexcept in the future This is part of the interface in this case the agreement between you and your client code and you risk breaking client code if you change your mind The fact is most functions are exception neutral they dont throw but what they call might If so these exception neutral functions are rightfully defined as not noexcept If some function eg swap has natural noexcept impls its worth implementing them that way and declaring noexcept But if not and you tweak the impl eg an underlying call might throw and you catch all possible ones and return different values such that its noexcept itd be putting the cart before the horse By default some functions are implicitly noexcept eg dtors Some impls differentiate functions by wide contract and narrow contract Wide contract means this function has no precondition unaware of program states and it imposes no constraints on the arguments its given They never exhibit UB Narrow contract means otherwise if a precondition is violated results are undefined Typically we declare noexcept on wide contract functions and situation is trickier with narrow contract functions Compilers offer no help in detecting noexcept The following compiles without warning Reasoning being the called function might be in C might be in C98 style where noexcept doesnt exist cpp void a void b noexcept a Takeaways noexcept is part of a functions interface and that means that callers may depend on it noexcept functions are more optimizable than nonnoexcept functions noexcept is particularly valuable for the move operations swap memory deallocation functions and dtors Most functions are exceptionneutral rather than noexcept Snippet cpp noexceptmcpp include include include demonstrates the contigency of stdswaps noexcept on user types noexcept and that you can declare an obviously throwing function noexcept which would violate correctness class Point public Pointdouble xVal 0 double yVal 0 noexcept xxVal yyVal double xValue const noexcept return x double yValue const noexcept return y void setXdouble newX noexcept x newX void setYdouble newY noexcept y newY private double x y int func noexcept throw stdruntimeerrordeclared noexcept but actually throws this would generate a compiler warning return 0 int main int x23 int y23 Point p1 p2 using stdswap swaps noexcept is contingent upon the noexceptness of the given parameters noexcept tests for the noexceptness of a call stdcout noexceptswapx y n stdcout noexceptswapp1 p2 n can I make swap on two points not noexcept try func note how this is not caught correctness is violated due to the presence of a wrongful noexcept catch const stdruntimeerror ex stdcout caught exwhat n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it15","content":" Item 15 use constexpr whenever possible Conceptually constexpr indicates a values not only constant but also its known during compilation constexpr objects and functions have different meanings constexpr objects are const and known at compile time Values that are known during compile time may be placed in readonly memory And of broader applicability is that integral values that are constant and known during compilation can be used in contexts where C requires an integral constexpr eg array sizes integral template arguments enumerator values alignment specifiers etc Eg cpp int sz nonconstexpr variable constexpr auto arraySize1 sz error szs value not known at compilation stdarray data1 error same problem constexpr auto arraySize2 10 fine 10 is a compiletime constant stdarray data2 fine arraySize2 is constexpr Note const doesnt offer the same guarantee const needs not be initialized with values known during compilation All constexpr objects are const not all consts are constexpr const auto arraySize sz fine arraySize is const copy of sz stdarray data error arraySizes value not known at compilation constexpr functions produce compiletime constants computed during compilation when they are called with compiletime constants If they are called with values not known until runtime they produce runtime values In C11 constexpr may contain no more than a single executable statement a return But you can do recursions to get loops and ternary expr to get ifs In C14 constexpr are limited to taking and returning literal types meaning types that can have values determined during compilation In C11 all builtin types except void are literal types and user defined types may be too if they have their ctors and some other member functions constexpr Eg cpp class Point public constexpr Pointdouble xVal 0 double yVal 0 noexcept xxVal yyVal constexpr double xValue const noexcept return x constexpr double yValue const noexcept return y these arent constexpr since in C11 constexpr are implicitly const and they return void which isnt a literal type in C11 void setXdouble newX noexcept x newX void setYdouble newY noexcept y newY in C14 both these constraints are lifted You can do constexpr void setXdouble newX noexcept C14 x newX constexpr void setYdouble newY noexcept C14 y newY private double x y And its fine to do the following evaluated at compile time constexpr Point p194 277 fine runs constexpr ctor during compilation constexpr Point p2288 53 also fine constexpr Point midpointconst Point p1 const Point p2 noexcept return p1xValue p2xValue 2 call constexpr p1yValue p2yValue 2 member funcs constexpr auto mid midpointp1 p2 init constexpr object wresult of constexpr function and with C14 setter constexpr you can do the following return reflection of p with respect to the origin C14 constexpr Point reflectionconst Point p noexcept Point result create nonconst Point resultsetXpxValue set its x and y values resultsetYpyValue return result return copy of it This blurs the line of computation at runtime with at compile time The more code is moved to compile time the faster your program at runtime Conversely the slower to compile Use constexpr whenever possible both constexpr functions and objects can be employed in a wider range of contexts than nonconstexpr objects and functions Keep in mind that constexpr is part of the interface use it if only you are able to commit to it If you later decide to remove it like adding debug IO since they are generally not permitted you may break an arbitrary amount of client code Takeaways constexpr objects are const and are initialized with values known during compilation constexpr functions can produce compiletime results when called with arguments whose values are known during compilation constexpr objects and functions may be used in a wider range of contexts than nonconstexpr objects and functions constexpr is part of an objects or functions interface Snippet cpp constexprmcpp include include demonstrates a userdefined literal type whose ctor and other member functions are constexpr and user constexpr functions that work with such types This shifts all the work to compile time class Point public constexpr Pointdouble xVal 0 double yVal 0 noexcept xxVal yyVal stdcout ctorn compile error nonconstexpr function operator cannot be used in a constexpr constexpr double xValue const noexcept return x constexpr double yValue const noexcept return y C14 constexpr void setXdouble newX noexcept C14 x newX constexpr void setYdouble newY noexcept C14 y newY private double x y constexpr Point midpointconst Point p1 const Point p2 noexcept return p1xValue p2xValue 2 call constexpr p1yValue p2yValue 2 member funcs and with C14 setter constexpr you can do the following return reflection of p with respect to the origin C14 constexpr Point reflectionconst Point p noexcept Point result create nonconst Point resultsetXpxValue set its x and y values resultsetYpyValue return result return copy of it int main constexpr Point p194 277 fine runs constexpr ctor during compilation constexpr Point p2288 53 also fine constexpr auto mid reflectionmidpointp1 p2 init constexpr stdcout midxValue midyValue n done at compile time Point p349 52 p3setX104 stdcout p3xValue n auto p4 reflectionmidpointp1 p3 stdcout p4xValue p4yValue n done at runtime return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it16","content":" Item 16 make const member functions thread safe If a member function is made const conceptually it should be safe for multiple threads to call the same method at the same time on the same object However consider the case where a const member function modifies a mutable member variable say getRoot of a Polynomial class modifies the rootCache and isRootCacheValid which are declared mutable a const member function is no longer threadsafe One could add a mutex to the getRoot operation Worth noting that stdmutex cannot be copied or moved by doing so Polynomial class loses the ability to be copied or moved stdatomic might be a cheaper solution if all you want is a counter though know that stdatomic are also uncopiable and unmovable If you require two or more memory locations to be synchronized eg a bool isValid and an int value then stdatomic is typically not enough If you only require one they typically are If your code is designed for a single threaded environment then this is not a concern However such environments are becoming rarer The safe bet is that const member functions will be subject to concurrent execution and thats why you should ensure your const member functions are threadsafe Takeaways Make const member functions threadsafe unless youre certain theyll never be used in a concurrent context Use of stdatomic variables may offer better performance than a mutex but theyre suited for manipulation of only a single variable or memory location Snippet cpp constthreadsafemcpp include include include include demonstrates a case where a const member function is only made threadsafe by introducing a mutex The particular example does not involve a multithreaded environment Does const member function mean its threadsafe No they could be operating on mutable states Should you make them threadsafe Yes Unless you are sure this code will run in a singlethreaded environment alternatively you could document the threadsafety behavior like BDE components are required to do The mutex in this example guarantees threadsafety class Polynomial public using RootsType stdvector RootsType roots const stdlockguard gm lock mutex if rootsAreValid if cache not valid computeRoots compute roots store them in rootVals rootsAreValid true return rootVals private void computeRoots const mutable stdmutex m mutable bool rootsAreValid false see Item 7 for info mutable RootsType rootVals on initializers int main Polynomial p return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it17","content":" Item 17 understand special member function generation Special member functions are those that the compiler will generate on its own In C98 we have four Default ctor dtor copycon copy assignment opr They are generated only if they are needed if you declared ctor with param the default one wont be generated They are implicitly public and inline They are not virtual by default except in a derived class whose parent classs dtor is virtual In C11 two more special member functions are generated move ctor and move assignment opr cpp class Widget public WidgetWidget rhs move constructor Widget operatorWidget rhs move assignment operator Both of them perform memberwise move on the nonstatic members of the class The move ctor assignment opr also moves its base class parts if any Use move on data member base class that supports it copy otherwise Copycon and copy assignment opr are independent declare only one and compiler will generate the other one for you Movecon and move assignment opr are not declaring a movecon will cause move assignment opr to not be generated as well vice versa Compilers rationale is that if you need customized movecon you will want custom move assignment opr as well Move operations wont be defined for a class with custom copycon custom copy assignment opr or custom dtor if you need special copy or special dtor youll need special move too This goes in the other direction too Declaring a movecon or move assignment opr causes compilers to disable copy operations To summarize two moves are generated when needed for classes that dont have custom copycon or copy assignment dtor movecon or move assignment The rule of three if you have one of custom dtor copycon or copy assignment opr you should have the other two too All standard library classes that manage memory has big three defined Default copycon copy assignment generation is deprecated in C11 if custom copy assignment copycon dtor is present C11 adds default to let you specify the default memberwise approach is desired This is often useful in a base class where you need to declare the dtor virtual In which case if the default dtor is desirable and you still want the compiler generated moves you could do the following cpp class Base public virtual Base default make dtor virtual BaseBase default support moving Base operatorBase default Baseconst Base default support copying Base operatorconst Base default In fact it might be a good idea to specify default anyway to clearly state your intention Without default consider this case where you decided to add a dtor the impact is profound as moves are silently deleted Say you have a stdmap in this class previously it can be moved now it has to be copied this is orders of magnitude slower Default ctor is the same in C11 as C98 Generated dtor is roughly the same except its noexcept by default If you have template copycon copy assignment opr like cpp class Widget template construct Widget Widgetconst T rhs from anything template assign Widget Widget operatorconst T rhs from anything Compiler still generates the defaults copy move etc for you Takeaway The special member functions are those compilers may generate on their own default constructor destructor copy operations and move operations Move operations are generated only for classes lacking explicitly declared move operations copy operations and a destructor The copy constructor is generated only for classes lacking an explicitly declared copy constructor and its deleted if a move operation is declared The copy assignment operator is generated only for classes lacking an explicitly declared copy assignment operator and its deleted if a move operation is declared Generation of the copy operations in classes with an explicitly declared copy operation or destructor is deprecated Member function templates never suppress generation of special member functions Snippet cpp moveandspecialmemberfunctionmcpp include include demonstrates in a composition default move con of the whole uses the default ctor of the part if the part has its move con disabled Shouldnt the behavior of part be use copy con if no move con in this case though class NoMove public NoMove stdcout NoMove default ctorn NoMoveint y dyy stdcout NoMove ctor taking in yn NoMove default NoMoveNoMove delete stdcout NoMove move ctorn NoMove operatorNoMove delete NoMoveconst NoMove rhs dyrhsdy stdcout NoMove copyconn NoMove operatorconst NoMove rhs stdcout NoMove copy assignmentn dy rhsdy return this void setYint y dy y int y const return dy private int dy Recommended practice is to declare default if the default supplied behaviors are desired Clearly state your intent class Base public virtual Base default make dtor virtual Base stdcout Base default ctorn Baseconst NoMove nm dnmnm stdcout Base ctor taking in NoMoven To test out the move if possible behavior on moving an object containing another whose move is deleted Turns out with default NoMove default ctor is called again when move happens on Base With the commented out NoMove copycon is called as expected BaseBase rhs default xstdmoverhsx dnmrhsdnm stdcout Base moveconn Base operatorBase rhs stdcout Base move assignment oprn x stdmoverhsx return this Baseconst Base stdcout Base copy conn Base operatorconst Base default const NoMove nm const return dnm private int x NoMove dnm int main Q1 the stdmove shouldnt matter here in the first place the param is a temporary and the cast should be noop But with without it the printed result is 0 10 Q2 in the version with stdmove and Bases default move con is called shouldnt the default move con use NoMoves copy con since NoMoves move con is disabled In fact it uses NoMoves default ctor Q3 in the version without stdmove why is Bases movecon not called A3 presumably optimized away by compiler this behavior is present with O0 Base bstdmoveBaseNoMove10 Base bBaseNoMove10 stdcout bnmy n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it18","content":" Item 18 use stduniqueptr for exclusiveownership resource management Its reasonable to assume that by default stduniqueptr is the same size as raw pointers If a raw pointer is small enough and fast enough for you a stduniqueptr almost certainly is too stduniqueptr embodies exclusive ownership semantics A nonnull stduniqueptr always owns what it points to stdunqiueptr is moveonly Copy is not allowed Moving stduniqueptr transfers ownership from source pointer to destination pointer Upon destruction a nonnull stduniqueptr destroys its resource By default resource destruction is accomplished by applying delete to the raw pointer inside the stduniqueptr but during construction stduniqueptr objects can be configured to take custom deleters arbitrary function to invoke when deletion happens A common use case for stduniqueptr is as a factory function return type for objects in a hierarchy Factory constructs an object on the heap and hands ownership to the user A factory method example with custom deleter cpp auto delInvmt Investment pInvestment custom deleter makeLogEntrypInvestment a lambda delete pInvestment expression delete the child object via pointer to parent Parent must have virtual dtor template revised stduniqueptr return type can return auto in C14 makeInvestmentTs params stduniqueptr ptr to be pInvnullptr delInvmt returned if a Stock object should be created pInvresetnew Stockstdforwardparams else if a Bond object should be created pInvresetnew Bondstdforwardparams else if a RealEstate object should be created pInvresetnew RealEstatestdforwardparams return pInv Custom deleters generally cause the size the a stduniqueptr to grow from one word to two Stateless function objects eg lambda expressions with no captures typically incur no size penalty when used as deleters and this means that when a custom deleter can be implemented as either a function or a captureless lambda expression the lambda is preferable Function object deleters with extensive state can yield stduniqueptr objects of significant size because the state would then need to be associated with each instance of the pointer object stduniqueptr is often used to implement pimpl idiom The existence of stduniqueptr for arrays should be of only intellectual interest to you because stdarray stdvector and stdstring are virtually always better data structure choices than raw arrays About the only situation I can conceive of when a stduniqueptr would make sense would be when youre using a Clike API that returns a raw pointer to a heap array that you assume ownership of stduniqueptr is the C11 way to express exclusive ownership but one of its most attractive features is that it easily and efficiently converts to a stdsharedptr This is a key part of why stduniqueptr is so well suited as a factory function return type Factory functions cant know whether callers will want to use exclusiveownership semantics for the object they return or whether shared ownership ie stdsharedptr would be more appropriate By returning a stduniqueptr factories provide callers with the most efficient smart pointer but they dont hinder callers from replacing it with its more flexible sibling How is bslmaManagedPtr in C03 without move semantics From a rough look bslmaManagedPtr does support the unnatural autoptr copycon which takes in modifiable reference and transfers ownership in a copy constructor However it also has a ctor taking in bslmfMovableRef which seems to BDEs backport of move semantics and requires more research Takeaways stduniqueptr is a small fast moveonly smart pointer for managing resources with exclusiveownership semantics By default resource destruction takes place via delete but custom deleters can be specified Stateful deleters and function pointers as deleters increase the size of stduniqueptr objects Converting a stduniqueptr to a stdsharedptr is easy Snippet cpp uniqueptrmcpp include include demonstrates using uniqueptr with custom deleter as a factory method return type const auto CREATETYPE 1 class Investment public virtual Investment default class Stock public Investment public Stock stdcout default ctor Stockn Stock stdcout dtor Stockn class Bond public Investment public Bond stdcout default ctor Bondn Bond stdcout dtor Bondn class RealEstate public Investment public RealEstate stdcout default ctor RealEstaten RealEstate stdcout dtor RealEstaten Example with variadic template and custom uniqueptr deleter auto delInvmt Investment pInvestment custom deleter stdcout custom dtorn a lambda delete pInvestment expression template revised stduniqueptr return type can return auto in C14 makeInvestmentTs params stduniqueptr ptr to be pInvnullptr delInvmt returned if CREATETYPE 1 pInvresetnew Stockstdforwardparams else if CREATETYPE 2 pInvresetnew Bondstdforwardparams else pInvresetnew RealEstatestdforwardparams return pInv int main auto investment makeInvestment return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it19","content":" Item 19 use stdsharedptr for sharedownership resource management An argument for manually manage memory and dtors may be the deterministic nature and predictability of when resource reclamation is going to happen Why cant we have the best of both worlds a system that works automatically like garbage collection yet applies to all resources and has predictable timing like destructors C11 stdsharedptr does this An object accessed via stdsharedptr has its lifetime managed by these shared pointers No single one assumes ownership they collaborate to make sure the object is destroyed when its no longer needed last sharedptr stops pointing at the object A shared pointer knows how many others are pointing at the object by consulting the resources reference count Usually ctor increments it move ctor doesnt dtor decrements it copy assignment opr does both When after a decrement the count drops to 0 the sharedptr destroys the object As a result stdsharedptr is twice the size of unique pointers it holds a reference to the object and another reference to the reference count of the object Memory for the reference count needs to be dynamically allocated Pointed to object has no idea its managed by a pointer Increments and decrements to reference count needs to be atomic to guarantee threadsafety Atomic operations are typically slower than the nonatomic counterparts Move assignment ctor is faster than copy assignment ctor for stdsharedptr since move doesnt involve atomic increments decrements but copy does Note that if a custom deleter is given its not part of sharedptrs type This is not the case with uniqueptr cpp auto loggingDel Widget pw custom deleter as in Item 18 makeLogEntrypw delete pw stduniqueptr upwnew Widget loggingDel stdsharedptr deleter type is not spwnew Widget loggingDel part of ptr type The sharedptr deleter design is more flexible Having a custom deleter changes the size of the uniqueptr and that deleter is part of uniqueptrs type while having a custom deleter does not change the size of a sharedptr Why the difference sharedptr stores the deleter not inside each shared pointer object but together with the reference count as part of a control block The control block contains reference count a custom deleter if specified a custom allocator if specified and a secondary reference count the weak count The following rules exist for creating control blocks stdmakeshared always creates a control block when a sharedptr is created from uniqueptr or autoptr and as part of the construction the uniqueptr is set to null when a sharedptr is called with a raw pointer sharedptr created from sharedptr or weakptr dont allocate new control blocks They expect the control block to be passed in As a consequence more than one sharedptrs created from a raw pointer means more than one control blocks thus double free and UB Avoid doing this cpp auto pw new Widget pw is raw ptr stdsharedptr spw1pw loggingDel create control block for pw stdsharedptr spw2pw loggingDel create 2nd control block for pw Two lessons avoid passing raw pointers to stdsharedptr Use makeshared instead if you have to pass a raw pointer to a sharedptr pass the result directly from new instead A particular case to be careful about is this pointer Say we have the following vector to keep track of processed widgets cpp stdvectorstdsharedptr processedWidgets class Widget public void process processedWidgetsemplacebackthis add it to list of processed Widgets this is wrong if there can be other shared pointers to this object the code is going to UB You could do instead cpp class Widget public stdenablesharedfromthis public void process processedWidgetsemplacebacksharedfromthis fine Widget derives from stdenablesharedfromthis with Widget itself as a template argument This is completely legal and has a name Curiously Recurring Template Pattern stdenablesharedfromthis defines a function sharedfromthis that allocates control block to the current object without duplicating Use sharedfromthis when you want a sharedptr that points to the same object as this Underlying stdenablesharedfromthis it relies on the current object having a control block and there must be an existing sharedptr outside the member function calling sharedfromthis pointing to this If not sharedfromthis typically throws To make sure such a sharedptr exists classes deriving from stdenablesharedfromthis typically hides ctor and provides a factory method returning sharedptr Eg cpp class Widget public stdenablesharedfromthis public factory function that perfectforwards args to a private ctor template static stdsharedptr createTs params void process as before private ctors Control blocks come at a cost they may have arbitrarily large deleters and the underlying impl uses inheritance so theres also vptr how But for the functionality they provide sharedptrs cost is very reasonable With default deleter and allocator and created with makeshared the control block is 3 words in size and its allocation is essentially free Dereferencing is cheap atomic operations should map to machine instructions If you want to model shared ownership sharedptr is still the right way to go uniqueptr cannot be created from sharedptr Another thing sharedptr cant do is working with arrays no array template parameters unlike uniqueptr But given different alternatives to builtin array eg array vector string using a smart pointer to manage a dumb array is probably a bad idea in the first place Takeaway stdsharedptrs offer convenience approaching that of garbage collection for the shared lifetime management of arbitrary resources Compared to stduniqueptr stdsharedptr objects are typically twice as big incur overhead for control blocks and require atomic reference count manipulations Default resource destruction is via delete but custom deleters are supported The type of the deleter has no effect on the type of the stdsharedptr Avoid creating stdsharedptrs from variables of raw pointer type Snippet cpp sharedptrmcpp include include include demonstrates undefined behavior when creating two shared pointers from the same raw pointer double free caused by having two control blocks with count 1 but only one instance of data first case by a pointer to this second case by pointer to data class Widget public stdenablesharedfromthis public void processstdvectorstdsharedptr processedWidgets processedWidgetsemplacebackthis add it to list of processed Widgets this is wrong void processCorrectstdvectorstdsharedptr processedWidgets processedWidgetsemplacebacksharedfromthis private int dx int main auto wp stdmakeshared stdvectorstdsharedptr processedWidgets wpprocessprocessedWidgets wpprocessprocessedWidgets double free without sharedfromthis auto wp stdmakeshared if Widget is not held by a sharedptr itll just throw type std1badweakptr badweakptr hence why we typically have a creator function for classes that inherit from sharedfromthis and disable their ctors stdvectorstdsharedptr processedWidgets wpprocessCorrectprocessedWidgets wpprocessCorrectprocessedWidgets all good auto loggingDel int pw custom deleter stdcout dtorn as in Item 18 delete pw auto pw new int5 stdsharedptr spw1pw loggingDel create control block for pw stdsharedptr spw2pw loggingDel create 2nd control block for pw double free when freeing spw1 and spw2 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it2","content":" Understand auto type deduction Auto type deduction actually largely follows the same rules as template type deduction cpp auto x 27 case 3 x is neither ptr nor reference const auto cx x case 3 cx isnt either const auto rx x case 1 rx is a nonuniversal ref Case 2s auto uref1 x x is int and lvalue so uref1s type is int auto uref2 cx cx is const int and lvalue so uref2s type is const int auto uref3 27 27 is int and rvalue so uref3s type is int Same rule goes for arrays and functions const char name names type is const char13 R N Briggs auto arr1 name arr1s type is const char auto arr2 name arr2s type is const char 13 void someFuncint double someFunc is a function type is voidint double auto func1 someFunc func1s type is void int double auto func2 someFunc func2s type is void int double The only difference is in auto always treats as stdinitializerlist while template deduction does not cpp int x1 27 int x227 int x3 27 int x4 27 these four do the same thing auto x1 27 type is int value is 27 auto x227 ditto auto x3 27 type is stdinitializerlist value is 27 auto x4 27 ditto template type deduction does not work with asis auto x 11 23 9 xs type is stdinitializerlist template template with parameter void fT param declaration equivalent to xs declaration f 11 23 9 error cant deduce type for T instead template void fstdinitializerlist initList f 11 23 9 T deduced as int and initLists type is stdinitializerlist Takeaway auto type deduction is usually the same as template type deduction but auto type deduction assumes that a braced initializer represents a stdinitializerlist and template type deduction doesnt auto in a function return type or a lambda parameter implies template type deduction not auto type deduction Snippet cpp autotypedeductionmcpp include include demonstrates the similarity between template type deduction and auto type deduction int main const int from 15 this is like rule 2 in template type deduction When the given has const but its passbyvalue constness is dropped auto to from to 27 stdcout to n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it20","content":" Item 20 use stdweakptr for stdsharedptr like pointers that can dangle A weakptr is like a sharedptr that does not affect an objects reference count Thus they face the possibility of the object being destroyed when they try to access it A weakptr cannot be dereferenced directly nor can they be tested for nullness Its because it isnt standalone pointer but rather an augmentation of sharedptr The relationship begins at birth stdweakptr is typically created from stdsharedptr They point to the same place as stdsharedptr but they dont affect the reference count in the control block Weak pointers that dangle are said to have expired cpp auto spw after spw is constructed stdmakeshared the pointedto Widgets ref count RC is 1 See Item 21 for info on stdmakeshared stdweakptr wpwspw wpw points to same Widget as spw RC remains 1 if wpwexpired if wpw doesnt point to an object Often you want to do check if expired if not dereference But if you do it in two steps a race condition would be introduced Thus you need one atomic operation of check if expired if not create a sharedptr from it This is called lock Shared pointer ctor taking in a weak pointer is the same operation as lock just that it throws if the weakptr has expired cpp Form 1 of lock stdsharedptr spw1 wpwlock if wpws expired spw1 is null auto spw2 wpwlock same as above but uses auto Form 2 of lock stdsharedptr spw3wpw if wpws expired throw stdbadweakptr How are weak pointers useful One case is the following imagine you have a loadWidgetid call which by itself is expensive and you want to cache things by id inside You cant have an unlimited cache One way to implement it is use a weakptr inside cache the Widgets inside with weak pointers give loaded objects back to the client let client manage their shared ownership When another load is called cache tries locking and if it hasnt expired cache can just serve the content cpp stdsharedptr fastLoadWidgetWidgetID id static stdunorderedmapWidgetID stdweakptr cache auto objPtr cacheidlock objPtr is stdsharedptr to cached object or null if objects not in cache if objPtr if not in cache objPtr loadWidgetid load it cacheid objPtr cache it return objPtr Another use case is the observer pattern in which there is subjects objects whose state may change and observers objects to be notified when a state change happens Subjects typically hold a pointer to observers so that they can be notified when a state change happens Subjects have no interest in the lifetime of observers but they care if an observer is destroyed they dont make subsequent access to it A reasonable design is to let subjects hold weak pointers to observers A third use case is to break cycles in cycling reference by sharedptr Instead of A and B holding sharedptrs to each other A to B could be sharedptr and B to A could be weakptr Its worth noting that this should be a rare case in a typical parents lifetime outlives that of its childrens use case parent could hold uniqueptr to children and children could hold a raw pointer back to parent if needed From an efficiency perspective weakptr makes the same case as sharedptr same size control block and operations such as construction destruction and assignment involves atomic reference count manipulations of weak count in control block Why do we need weak count Takeaways Use stdweakptr for stdsharedptrlike pointers that can dangle Potential use cases for stdweakptr include caching observer lists and the prevention of stdsharedptr cycles Snippet cpp weakptrmcpp include include include demonstrates one use case of weakptr in building a cached load operation in which cache uses a weakptr that can dangle to avoid hogging the space if we use sharedptr and let the client decide the lifetime of the objects returned by cache class Widget public Widget default Widgetconst Widget default WidgetWidget default Widgetint id dxid Widget operatorconst Widget default Widget operatorWidget default private int dx stduniqueptr buildWidgetint id stdcout build widget id n return stdmakeuniqueid stdsharedptr fastLoadWidgetint id stdcout fast load widget id n static stdunorderedmapint stdweakptr cache auto objPtr cacheidlock objPtr is stdsharedptr to cached object or null if objects not in cache if objPtr if not in cache objPtr buildWidgetid build it cacheid objPtr cache it return objPtr int main stdsharedptr wp fastLoadWidget3 auto wp1 fastLoadWidget3 wp nullptr wp1 nullptr auto wp2 fastLoadWidget3 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it21","content":" Item 21 prefer stdmakeunique and stdmakeshared to direct use of new stdmakeunique is part of C14 stdmakeshared is part of C11 If you need to implement it yourself it looks like this without supporting arrays cpp template stduniqueptr makeuniqueTs params return stduniqueptrnew Tstdforwardparams stdallocateshared acts like makeshared except its first argument is an allocator to be used for the underlying dynamic memory allocation Compare the version using makexxx and using raw pointer ctor cpp auto upw1stdmakeunique with make func stduniqueptr upw2new Widget without make func auto spw1stdmakeshared with make func stdsharedptr spw2new Widget without make func makexxx version does not repeat the type Widget whose duplication should be avoided Another concern is exception safety Consider this code where we process widget with a priority ignore the suspicious looking pass sharedptr by value for now cpp void processWidgetstdsharedptr spw int priority int computePriority call site processWidgetstdsharedptrnew Widget potential computePriority resource leak This has potential resource leak Why Three things need to happen here new Widget sharedptr ctor computePriority call Compiler is allowed to generate code that put computePriority call in between Thus if computePriority call throws heap allocation of Widget is done but the memory wont be managed by a smart pointer stdmakeshared avoids such a problem cpp processWidgetstdmakeshared no potential computePriority resource leak even though either one of makeshared and computePriority can be called first in the compiler generated code stdmakeshared also improves efficiency cpp stdsharedptr spwnew Widget two memory allocations of Widget object and its control block auto spw stdmakeshared one memory allocation to hold both the object and the control block There are circumstances where makeshared and makeunique cannot be used First is when deleter is passed in as argument Second is this makeunique and makeshared perfectly forwards to the objects ctor but do they forwward using parentheses or brackets cpp auto upv stdmakeuniquestdvector10 20 auto spv stdmakesharedstdvector10 20 Forwards using or makes a difference for vectors They use parentheses which means if you want to initialize the object using brackets you can either use new or pass an stdinitializerlist cpp create stdinitializerlist auto initList 10 20 create stdvector using stdinitializerlist ctor auto spv stdmakesharedstdvectorinitList For makeshared there are two more caveats Classes with custom operator new and delete who typically allocates the exact size of the object in their new stdallocateshared need to request size of object size of control block it usually doesnt work well with overloaded new Big objects that has sharedptrs and weakptrs pointing to it and wants the object to be destroyed when all sharedptr references are gone Since makeshared allocates the control block and the object together and the control block is only freed after all sharedptr as well as weakptr references are gone makeshared created objects will not have the freedom as newed objects do to deallocate the object and the control block separately If you have to use new watch out for the exceptionsafety issue mentioned earlier You could do cpp stdsharedptr spwnew Widget customDel processWidgetspw computePriority correct but not optimal pass sharedptr by lvalue incurs a copy thus additional atomic opr Or instead processWidgetstdmovespw both efficient and computePriority exception safe Takeaways Compared to direct use of new make functions eliminate source code duplication improve exception safety and for stdmakeshared and stdallocateshared generate code thats smaller and faster Situations where use of make functions is inappropriate include the need to specify custom deleters and a desire to pass braced initializers For stdsharedptrs additional situations where make functions may be illadvised include 1 classes with custom memory management and 2 systems with memory concerns very large objects and stdweakptrs that outlive the corresponding stdsharedptrs Snippet cpp makesharedmcpp include include include demonstrates a case where creating smart ptr from raw ptr may lead to exception unsafe code while make functions wont have such an issue class Widget public Widget default Widgetconst Widget default WidgetWidget default Widgetint id dxid Widget operatorconst Widget default Widget operatorWidget default private int dx void processWidgetstdsharedptr int priority int getPriority throw stdruntimeerrorexpected return 0 int main exception unsafe in compiler generated code getPriority can get in between Widget ctor and sharedptr ctor which will cause memory leak try processWidgetstdsharedptrnew Widget getPriority catch const stdruntimeerror swallow exception safe try processWidgetstdmakeshared getPriority catch const stdruntimeerror swallow return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it22","content":" Item 22 when using the pimpl idiom define special member functions in the implementation file Pimpl idiom is often used to combat excessive build times Why does it help Consider the following cpp class Widget in header widgeth public Widget private stdstring name stdvector data Gadget g1 g2 g3 Gadget is some user defined type This means the header has to include vector string and gadget These headers in turn increase the build time of Widgets clients and if header contents eg gadget and widget change the client has to recompile With pimpl you have cpp class Widget still in header widgeth public Widget Widget dtor is neededsee below private struct Impl declare implementation struct Impl pImpl and pointer to it Where WidgetImpl is an incomplete type There is little you can do an incomplete type but you can make a pointer to it given the size is known The impl in C98 then looks something like this cpp include widgeth in impl file widgetcpp include gadgeth include include struct WidgetImpl definition of WidgetImpl stdstring name with data members formerly stdvector data in Widget Gadget g1 g2 g3 WidgetWidget allocate data members for pImplnew Impl this Widget object WidgetWidget destroy data members for delete pImpl this object With C11 unqiueptr is exactly the tool we need and the code in turn looks something like this cpp include widgeth in widgetcpp include gadgeth include include struct WidgetImpl as before stdstring name stdvector data Gadget g1 g2 g3 WidgetWidget per Item 21 create pImplstdmakeunique stduniqueptr via stdmakeunique However when we use it in a different translation unit cpp Widget w Compiler generates invalid application of sizeof to an incomplete type WidgetImpl Problem is that as compiler generates code for ws deletion delete on the raw pointer inside uniqueptr delete needs to be called on a complete type In this translation unit with pimplh included struct Impl is not a complete type So we declare Widgets dtor in the header but not define it cpp class Widget as before in widgeth public Widget Widget declaration only private as before struct Impl stduniqueptr pImpl And include widgeth as before in widgetcpp include gadgeth include include struct WidgetImpl as before definition of stdstring name WidgetImpl stdvector data Gadget g1 g2 g3 WidgetWidget as before pImplstdmakeunique WidgetWidget Widget definition alternative dtor WidgetWidget default same effect as above Pimpls are often times great candidates for move So we add in the move operations cpp header WidgetWidget rhs noexcept declarations Widget operatorWidget rhs noexcept only impl WidgetWidgetWidget rhs noexcept default Widget WidgetoperatorWidget rhs noexcept default you cant do default in the header since that would make it a definition and to be able to generate code for the default move move assignment needs to destroy the previously managed item move ctor needs to be able to delete Impl in case of an exception even though you declare it noexcept Impl needs to be a complete type And well need to write the copy operations ourselves since compiler wont be generate copy operations for classes with moveonly types like uniqueptr Even if they do itd be a shallow copy of the pointer not a deep copy of the underlying object cpp deep copy of an object using pimpl WidgetWidgetconst Widget rhs copy ctor pImplnullptr if rhspImpl pImpl stdmakeuniquerhspImpl Widget Widgetoperatorconst Widget rhs copy operator if rhspImpl pImplreset else if pImpl pImpl stdmakeuniquerhspImpl else pImpl rhspImpl return this Yet if we use a sharedptr for pimpl the rules of this chapter dont apply This would work just fine in client code and the compiler will supply the big five cpp class Widget in widgeth public Widget no declarations for dtor or move operations private struct Impl stdsharedptr pImpl stdsharedptr instead of stduniqueptr The difference between sharedptr and uniqueptr stems from custom deleter support in uniqueptr due to deleter being part of the type allowing smaller runtime structures and faster runtime code the type must be complete when using compiler generated dtor or moves Such restriction is lifted in sharedptrs case with deleter not being part of the type To use uniqueptr or sharedptr depends on the use case Its possible pimpls could desire shared ownership of the underlying Takeaways The pimpl idiom decreases build times by reducing compilation dependencies between class clients and class implementations For stduniqueptr pImpl pointers declare special member functions in the class header but implement them in the implementation file Do this even if the default function implementations are acceptable The above advice applies to stduniqueptr but not to stdsharedptr think about it even though my custom deleter and compiler supplied deleter does the same thing we still cant use compilers in uniqueptrs case Is it because the inlinebydefault nature of compiler generated ones Snippet cpp pimplh ifndef INCLUDEDPIMPLH define INCLUDEDPIMPLH include class Widget public Widget Widget this dtor declaration cannot be omitted we declare it here so that we dont run into issues deleting an incomplete type uniqueptr whose deleter is part of the type when using Widget in a different translation unit WidgetWidget rhs noexcept declarations Widget operatorWidget rhs noexcept only Widgetconst Widget rhs declarations Widget operatorconst Widget rhs only void doStuff const private struct Impl stduniqueptr pimpl endif pimplcpp include include include include struct WidgetImpl stdvector dx int dy void WidgetdoStuff const stdcout dy n WidgetWidget pimplstdmakeunique WidgetWidget default we still need to define this even if its using default here we define it so that we dont run into linker errors WidgetWidgetWidget rhs noexcept default Widget WidgetoperatorWidget rhs noexcept default you cant do default in the header since that would make it a definition and to be able to generate code for the default move move assignment needs to destroy the previously managed item move ctor needs to be able to delete Impl in case of an exception even though you declare it noexcept Impl needs to be a complete type WidgetWidgetconst Widget rhs pimplnullptr if rhspimpl pimpl stdmakeuniquerhspimpl Widget Widgetoperatorconst Widget rhs if rhspimpl pimplreset else if pimpl pimpl stdmakeuniquerhspimpl else pimpl rhspimpl return this testdrivermcpp include demonstrates a case when using unqiueptr for pimpl we need to define dtor in the implementation file shows copy move impl for pimpl classes using uniqueptr int main Widget w The dtor of stduniqueptr will complain about sizeof or delete to an incomplete type Problem is that as compiler generates code for ws deletion delete on the raw pointer inside uniqueptr delete needs to be called on a complete type In this translation unit with pimplh included struct Impl is not a complete type wdoStuff return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it23","content":" Item 23 understand stdmove and stdforward Neither stdmove nor stdforward generates code at runtime They are function templates that perform casts stdmove unconditionally casts its argument to an rvalue while stdforward performs this cast only if a particular condition is fulfilled stdmove looks something like this cpp C11 template in namespace std typename removereferencetype moveT param using ReturnType alias declaration typename removereferencetype see Item 9 return staticcastparam C14 template still in decltypeauto moveT param namespace std using ReturnType removereferencet return staticcastparam The removereference in return type is important since type is a universal reference in which case if T is an lvalue reference the universal reference would become an lvalue reference To make move truly return a rvalue reference we have this removereference Point is stdmove does rvalue cast not move rvalues are candidates for move by casting to rvalue reference it tells compiler the object is eligible to be moved from Consider this code cpp class Annotation public explicit Annotationconst stdstring text valuestdmovetext move text into value this code doesnt do what it seems to private stdstring value Is text copied or moved to value It is copied due to the fact that the rvalue of const string cant be passed to stdstrings move ctor as move ctors need nonconst stdstring However the param of string copy ctor lvaluereferencetoconst can bind to a const rvalue Thus the string is copied from text to value Moral of the story move doesnt move and if you want to be able to move from something dont declare it const stdforward is a conditional cast Consider this code cpp void processconst Widget lvalArg process lvalues void processWidget rvalArg process rvalues template template that passes void logAndProcessT param param to process auto now get current time stdchronosystemclocknow makeLogEntryCalling process now processstdforwardparam Widget w logAndProcessw call with lvalue logAndProcessstdmovew call with rvalue Without the stdforward since param is an lvalue the overload expecting lvalue will always be called To forward the rvaluelvalueness of a parameter we use stdforward stdforward casts param into an rvalue only if param is instantiated with an rvalue Can you replace stdmove with stdforward everywhere Technically yes And neither is really necessary as you can write casts everywhere just not desirable But remember they are different in a mandatory rvalue cast and conditional rvalue cast thats exactly a forward of the rvaluelvalueness of the object the function param is instantiated with Takeaways stdmove performs an unconditional cast to an rvalue In and of itself it doesnt move anything stdforward casts its argument to an rvalue only if that argument is bound to an rvalue Neither stdmove nor stdforward do anything at runtime Move requests on const objects are treated as copy requests Snippet cpp moveforwardmcpp include include demonstrates perfect forwarding with universal references and the effect of stdmove stdforward class Widget void processconst Widget lvalArg stdcout called with lvaluen process lvalues void processWidget rvalArg stdcout called with rvaluen process rvalues template template that passes void logAndProcessT param param to process processstdforwardparam with forward the expected version dependent upon whats given to param is called processstdmoveparam with move the rvalue version will always be called even if an lvalue is given if an rvalue is given a compiler error is thrown processparam without forward the lvalue version will always be called int main Widget w logAndProcessw logAndProcessWidget return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it24","content":" Itemm 24 distinguish universal references from rvalue references If you see T in source code its not always rvalue reference cpp void fWidget param rvalue reference Widget var1 Widget rvalue reference auto var2 var1 not rvalue reference template void fstdvector param rvalue reference template void fT param not rvalue reference T has two meanings One meaning is rvalue reference to identify objects that may be moved from Another meaning is universal reference either rvalue or lvalue reference they are permitted to bind to rvalues or lvalues const or non const volatile or non volatile Universal references arise in template parameters and auto declarations cpp template void fT param param is a universal reference auto var2 var1 var2 is a universal reference Both of these have template type deduction When there isnt type deduction happening is an rvalue reference References have to be initialized The initializer for universal references decide if it represents an rvalue reference or an lvalue reference if initializer is rvalue its rvalue reference if its lvalue its lvalue references Eg cpp template void fT param param is a universal reference Widget w fw lvalue passed to f params type is Widget ie an lvalue reference fstdmovew rvalue passed to f params type is Widget ie an rvalue reference Universal reference must precisely be T cant even be the likes of const T or vector and type deduction has to happen Consider this cpp templateclass T class Allocator allocator from C class vector Standards public void pushbackT x template void emplacebackArgs args The argument x is not a universal reference since pushback cant exist without a particular vector instantiation for it to be part of and the type of that instantiation fully determines the declaration for pushback Consider this cpp stdvector v class vectorWidget allocator public void pushbackWidget x rvalue reference The arguments args are universal references since Args are independent of T Variables declared with the type auto are universal references because type deduction takes place and they have the correct form T One use case is such in C14 where auto is allowed as lambda parameters cpp auto timeFuncInvocation auto func auto params C14 start timer stdforwardfunc invoke func stdforwardparams on params stop timer and record elapsed time The underlying is actually reference collapsing which well get to later Takeaways If a function template parameter has type T for a deduced type T or if an object is declared using auto the parameter or object is a universal reference If the form of the type declaration isnt precisely type or if type deduction does not occur type denotes an rvalue reference Universal references correspond to rvalue references if theyre initialized with rvalues They correspond to lvalue references if theyre initialized with lvalues Snippet cpp rvaluereferenceuniversalreferencemcpp include include include demonstrates the two preconditions for a reference to be considered universal type deduction and the form T template T or auto t template class Widget public void rvalueReferenceBindingT x stdcout rvalue reference binding boosttypeindextypeidwithcvrprettyname n template void universalReferenceBindingU y stdcout universal reference binding boosttypeindextypeidwithcvrprettyname n y 3 int main Widget w int b 4 wrvalueReferenceBinding3 cannot bind lvalue to something expecting rvalue reference wrvalueReferenceBindingb wuniversalReferenceBindingb wuniversalReferenceBinding2 stdcout b n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it25","content":" Itemm 25 use stdmove on rvalue references stdforward on universal references If a function has an rvalue reference parameter you know the objects bound to may be moved cpp class Widget WidgetWidget rhs rhs definitely refers to an object eligible for moving That being the case youll want to pass such objects to other functions in a way that permits those functions to take advantage of the objects rvalueness The way to do it is to cast parameters to rvalues using stdmove Eg cpp class Widget public WidgetWidget rhs rhs is rvalue reference namestdmoverhsname pstdmoverhsp private stdstring name stdsharedptr p A universal reference might be bound to an object thats eligible for moving they can be casted to rvalues only if they were intialized with rvalues This is precisely what stdforward does cpp class Widget public template void setNameT newName newName is name stdforwardnewName universal reference Dont use stdforward on rvalue references and more importantly dont use stdmove on universal references you may unexpectedly modify lvalues cpp class Widget public template void setNameT newName universal reference name stdmovenewName compiles but is bad bad bad private stdstring name stdsharedptr p stdstring getWidgetName factory function Widget w auto n getWidgetName n is local variable wsetNamen moves n into w ns value now unknown Consider the following two cpp Widget class taking universal reference template void setNameT newName universal reference name stdmovenewName compiles but is bad bad bad You can have these as an alternative void setNameconst stdstring newName set from name newName const lvalue void setNamestdstring newName set from name stdmovenewName rvalue Problem with the alternative is that despite very minor efficiency concerns when passing a string literal when the method takes N or even unlimited parameters youd have 2N overloads Like these guys template from C11 sharedptr makesharedArgs args Standard template from C14 uniqueptr makeuniqueArgs args Standard And inside such functions I assure you stdforward is applied to the universal reference parameters when theyre passed to other functions Which is exactly what you should do eventually after you are done with it in the function body like the following example template text is void setSignTextT text univ reference signsetTexttext use text but dont modify it auto now get current time stdchronosystemclocknow signHistoryaddnow stdforwardtext conditionally cast text to rvalue In rare cases youll want to call stdmoveifnoexcept instead of stdmove If you are in a function returning by value and you are returning an object bound to an rvalue reference or a universal reference youll want to apply stdmove or stdforward when you return the reference Consider the following cpp Matrix byvalue return operatorMatrix lhs const Matrix rhs lhs rhs return stdmovelhs move lhs into return value Matrix as above operatorMatrix lhs const Matrix rhs lhs rhs return lhs copy lhs into return value the first approach works asis if Matrix supports move construction If not casting it to rvalue wont hurt as the rvalue will be copied by Matrixs copycon Similar case goes for using stdforward on universal references But should you do this with the case of returning local variables by value cpp Widget makeWidget Copying version of makeWidget Widget w local variable configure w return w copy w into return value Widget makeWidget Moving version of makeWidget Widget w return stdmovew move w into return value dont do this Return value optimization does this for you and is in the standards With the stdmove RVO cannot be applied as it requires type of the local object is the same as that returned by the function the local object is whats being returned When you do stdmove in this case you are returning a reference instead of the object itself which breaks the condition for RVO Since RVO is not required what if you suspect the compiler doesnt do it or you know the internals of this function is probably too hard for the compiler to apply RVO Still dont do this as the standards say if the conditions of RVO are met but the compiler does not do it the compiler is still required to treat the returned local variable as an rvalue Using stdmove on a local variable could be useful if you know later on you arent going to use it but not in this return local object by value case Takeaways Apply stdmove to rvalue references and stdforward to universal references the last time each is used Do the same thing for rvalue references and universal references being returned from functions that return by value Never apply stdmove or stdforward to local objects if they would otherwise be eligible for the return value optimization Snippet cpp moveonrvalueforwardonuniversalmcpp include include demonstrates using stdmove on rvalue references and stdforward on universal references this particular example shows that when returning a reference variable by value youll want to use stdmove if that reference is rvalue reference or stdforward if its universal reference move ctor of the returned value will be called in the first csae copy will be called in the second case dont do this for returning local variables by value though let rvo take care of it for you class Widget public Widget default Widgetconst Widget noexcept stdcout copy ctorn Widget operatorconst Widget noexcept stdcout copy assignmentn return this WidgetWidget noexcept stdcout move ctorn Widget operatorWidget noexcept stdcout move assignmentn return this Widget stdcout dtorn Widget byvalue return funcWidget w return stdmovew move lhs into return value Widget byvalue return func1Widget w return w copy lhs into return value Widget byvalue return rvo return Widget dont do stdmove or stdforward let rvo take care of this int main Widget x funcWidget Widget x func1Widget return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it26","content":" Item 26 avoid overloading on universal references Consider this code cpp stdmultiset names global data structure void logAndAddconst stdstring name auto now get current time stdchronosystemclocknow lognow logAndAdd make log entry namesemplacename add name to global data structure see Item 42 for info on emplace stdstring petNameDarla logAndAddpetName pass lvalue stdstring a copy is made in the vector since lvalue is passed in logAndAddstdstringPersephone pass rvalue stdstring a copy is made in the vector since name is an lvalue just a move in this case is possible logAndAddPatty Dog pass string literal a string object is first created from the string literal then a copy is made in the vector since name is an lvalue not even a move is needed in this case if the string literal is given to emplace directly emplace would have used it directly to create the string object inside the vector Instead for optimal efficiency we could have this cpp template void logAndAddT name auto now stdchronosystemclocknow lognow logAndAdd namesemplacestdforwardname stdstring petNameDarla as before logAndAddpetName as before copy lvalue into multiset logAndAddstdstringPersephone move rvalue instead of copying it logAndAddPatty Dog create stdstring in multiset instead of copying a temporary stdstring Then consider the case where clients want an overload of logAndAdd taking in int cpp stdstring nameFromIdxint idx return name corresponding to idx void logAndAddint idx new overload auto now stdchronosystemclocknow lognow logAndAdd namesemplacenameFromIdxidx stdstring petNameDarla as before logAndAddpetName as before these logAndAddstdstringPersephone calls all invoke logAndAddPatty Dog the T overload logAndAdd22 calls int overload but short nameIdx give nameIdx a value logAndAddnameIdx error the given short matches the universal reference version as short better match than the int overload thus an emplace to vector is called given a short and its an error since there isnt ctor of string that takes a short Functions taking universal references are the greediest functions in C They instantiate to create exact matches for almost any type of argument This is why combining overloading and universal references is almost always a bad idea the universal reference overload vacuums up far more argument types than the developer doing the overloading generally expects In a similar example with class ctors cpp class Person public template perfect forwarding ctor explicit PersonT n namestdforwardn explicit Personint idx int ctor despite having the perfect forwarding ctor compiler supplies copy and move ctor following the rules of it17 Personconst Person rhs copy ctor compilergenerated PersonPerson rhs move ctor compilergenerated and this would cause Person pNancy auto cloneOfPp create new Person from p this wont compile the perfect forwarding ctor will be called and string does not exist a ctor taking in a Person why the perfect forwarding ctor not the copy ctor Because the perfect forwarding ctor taking in Person is a perfect match while the copy ctor requires adding const change it up a bit const Person cpNancy object is now const auto cloneOfPcp calls copy constructor the perfect forwarding will be instantiated with explicit Personconst Person n instantiated from template but this doesnt matter as one of the overloadresolution rules in C is that in situations where a template instantiation and a nontemplate function ie a normal function are equally good matches for a function call the normal function is preferred And with inheritance cpp class SpecialPerson public Person public SpecialPersonconst SpecialPerson rhs copy ctor calls Personrhs base class forwarding ctor SpecialPersonSpecialPerson rhs move ctor calls Personstdmoverhs base class forwarding ctor note that the derived classs copy and move ctors dont call their base classs copy and ctor they call the base classs perfectforwarding ctor To understand why note that the derived class functions are using arguments of type SpecialPerson to pass to their base class Takeaways Overloading on universal references almost always leads to the universal reference overload being called more frequently than expected Perfectforwarding constructors are especially problematic because theyre typically better matches than copy constructors for nonconst lvalues and they can hijack derived class calls to base class copy and move constructors Snippet cpp avoidoverloaduniversalreferencemcpp include include demonstrates overloading a function taking in universal references will almost always end up with the overload taking in universal references called more often than expected in this case ctor universal reference overloading is particularly problematic class Widget public Widget default Widgetconst Widget noexcept stdcout base copy ctorn Widget operatorconst Widget noexcept stdcout base copy assignmentn return this WidgetWidget noexcept stdcout base move ctorn Widget operatorWidget noexcept stdcout base move assignmentn return this virtual Widget default template WidgetT rhs noexcept stdcout base universal reference ctorn class ChildWidget public Widget public ChildWidget default ChildWidgetconst ChildWidget rhs Widgetrhs stdcout child copy ctorn ChildWidgetChildWidget rhs Widgetstdmoverhs stdcout child move ctorn int main note how they call different overloads Widget w auto w1w calls perfect forwarding ctor which does not require adding const as copy ctor does const Widget cw auto cw1cw calls copy ctor since when a template and a regular function are equally good matches regular function wins note how child copy ctor calls bases universal ref ctor as opposed to copy ctor reasoning being that when calling the parent classs ctor an object of type Child is passed in ChildWidget child auto child1child return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it27","content":" Item 27 familiarize yourself with alternatives to overloading on universal references Avoid overloading is one option Pass by const T could sacrifice a little efficiency but avoids a universal reference overload Pass by value is counter intuitive but if you know youll copy them this dials up performance without any increase in complexity Item 41 Eg cpp class Person public explicit Personstdstring n replaces T ctor see namestdmoven Item 41 for use of stdmove explicit Personint idx as before namenameFromIdxidx private stdstring name None of the above has support for perfect forwarding If we want perfect forwarding we have to use universal references and to use that with overload we can do tag dispatch We start with this universal reference version which is problematic in the face of overload cpp template make log entry and add void logAndAddT name name to data structure auto now stdchronosystemclocknow lognow logAndAdd namesemplacestdforwardname We split the underlying work into two functions cpp template void logAndAddT name logAndAddImplstdforwardname stdisintegral not quite correct Why not quite correct If lvalue reference is passed in T is int and that is not an integral type Instead we have template void logAndAddT name logAndAddImpl stdforwardname stdisintegraltypename stdremovereferencetype In C14 do stdremovereferencet And the impl looks like template nonintegral void logAndAddImplT name stdfalsetype argument add it to auto now stdchronosystemclocknow global data lognow logAndAdd structure namesemplacestdforwardname stdfalsetype is a compiletime type that corresponds to false and the second overload stdstring nameFromIdxint idx as in Item 26 void logAndAddImplint idx stdtruetype integral argument look logAndAddnameFromIdxidx up name and call logAndAdd with it this technique is called tag dispatch We hope that compilers will recognize the 2nd parameter serves no purpose at runtime and will optimize them out A keystone of tag dispatch is the existence of a single unoverloaded function as the client API But in the case of ctors compilers supply their own even if you only write one ctor taking universal references For situations like these where an overloaded function taking a universal reference is greedier than you want yet not greedy enough to act as a single dispatch function tag dispatch is not the droid youre looking for In this case you need stdenableif enableif gives you a way to force compilers to behave as if a particular template didnt exist Eg cpp class Person public templatetypename T typename typename stdenableiftype explicit PersonT n SFINAE is the technology that makes stdenableif work In our case we want to disable the universal reference ctor only if T with cv qualifier and referenceness dropped is the same as Person To drop cv qualifier and referenceness we use stddecay Thus we have class Person public template typename T typename typename stdenableif stdissamePerson typename stddecaytype value type explicit PersonT n That addresses the problem of Person class but in Item 26 there is the issue of derived classes calling bases universal reference ctor instead of bases corresponding copy or move ctor To address that we have cpp class Person public template typename T typename typename stdenableif stdisbaseofPerson typename stddecaytype value type explicit PersonT n note that for user defined type T stdisbaseof is true for builtin types its false and the same in C14 class Person C14 public template typename T typename stdenableift less code here stdisbaseofPerson stddecayt and here value and here explicit PersonT n combining this with the integral type exclusion we solved with tag dispatch earlier class Person public template typename T typename stdenableift stdisbaseofPerson stddecaytvalue stdisintegralstdremovereferencetvalue explicit PersonT n ctor for stdstrings and namestdforwardn args convertible to stdstrings explicit Personint idx ctor for integral args namenameFromIdxidx copy and move ctors etc private stdstring name this uses perfect forwarding and should offer maximal efficiency and with universal references controlled this technique can be used in circumstances where overloading is unavoidable As a rule perfect forwarding is more efficient because it avoids the creation of temporary objects solely for the purpose of conforming to the type of a parameter declaration In the case of the Person constructor perfect forwarding permits a string literal such as Nancy to be forwarded to the constructor for the stdstring inside Person whereas techniques not using perfect forwarding must create a temporary stdstring object from the string literal to satisfy the parameter specification for the Person constructor But perfect forwarding has drawbacks one is some kinds of arguments cant be perfectforwarded Another is the comprehensibility of compiler error messages To combat the comprehensibility issue we can do staticassertion cpp class Person public template as before typename T typename stdenableift stdisbaseofPerson stddecaytvalue stdisintegralstdremovereferencetvalue explicit PersonT n namestdforwardn assert that a stdstring can be created from a T object staticassert stdisconstructiblevalue Parameter n cant be used to construct a stdstring the usual ctor work goes here remainder of Person class as before unfortunately in this case the staticassert being in function body after member initialization list would cause the long message to be printed first Takeaways Alternatives to the combination of universal references and overloading include the use of distinct function names passing parameters by lvaluereferencetoconst passing parameters by value and using tag dispatch Constraining templates via stdenableif permits the use of universal references and overloading together but it controls the conditions under which compilers may use the universal reference overloads Universal reference parameters often have efficiency advantages but they typically have usability disadvantages Snippet cpp overloadwithuniversalreferencemcpp include include include demonstrates alternatives to having a universal reference function override one is enableif where you can specify the condition where this universal reference template can be applied another is tagged dispatch where you can have one universal reference version call different underlying impls based on conditions tested on the type with which the template is instantiated enableif class Widget public Widget default Widgetconst Widget noexcept stdcout base copy ctorn Widget operatorconst Widget noexcept stdcout base copy assignmentn return this WidgetWidget noexcept stdcout base move ctorn Widget operatorWidget noexcept stdcout base move assignmentn return this Widget default C14 template typename T typename stdenableift stdisbaseofWidget stddecaytvalue stdisintegralstdremovereferencetvalue WidgetT rhs noexcept stdcout base universal reference ctorn class ChildWidget public Widget public ChildWidget default ChildWidgetconst ChildWidget rhs Widgetrhs stdcout child copy ctorn ChildWidgetChildWidget rhs Widgetstdmoverhs stdcout child move ctorn tag dispatch we want different behaviors for integral and nonintegral types template nonintegral void logAndAddImplT name stdfalsetype argument treat as string stdvector names namesemplacenamesbegin stdforwardname stdcout universal reference overloadn stdstring nameFromIdxint idx return from an idx void logAndAddImplint idx stdtruetype integral argument look stdvector names up name and namesemplacenamesbegin nameFromIdxidx call logAndAdd stdcout int overloadn with it template void logAndAddT name logAndAddImpl stdforwardname stdisintegraltypename stdremovereferencetype int main Tests enableif Note now they call the copy ctor Widget w auto w1w const Widget cw auto cw1cw Note now child copy ctor calls bases copy ctor ChildWidget child auto child1child And note the enableif defined integral types out meant to have a different overload short i 3 Widget w2i Tests tag dispatch logAndAddgood logAndAdd1 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it28","content":" Item 28 understand reference collapsing Consider template parameter deduction for universal references again cpp template void funcT param Widget widgetFactory function returning rvalue Widget w a variable an lvalue funcw call func with lvalue T deduced to be Widget funcwidgetFactory call func with rvalue T deduced to be Widget Note that reference to reference is illegal in C cpp int x auto rx x error cant declare reference to reference but with lvalue being deduced to match universal references template void funcT param as before funcw invoke func with lvalue T deduced as Widget we would have this void funcWidget param yet the type of param is Widget How compiler handles this is called reference collapsing Compilers allow reference to reference not in user code but may produce them in some contexts eg template instantiation There are four possible combinations lvalue to lvalue lvalue to rvalue rvalue to lvalue rvalue to rvalue If reference to reference occurs in a permitted context compiler follows the following rule to collapse reference If either reference is an lvalue reference the result is an lvalue reference Otherwise ie if both are rvalue references the result is an rvalue reference Reference collapsing is a key part of what makes stdforward work Whose impl can be cpp template in T forwardtypename namespace removereferencetype param std return staticcastparam Reference collapsing can be mimicked in auto form cpp Widget w auto w1 w w1 is an lvalue reference auto w2 widgetFactory w2 is an rvalue reference A universal reference isnt a new kind of reference its actually an rvalue reference in a context where two conditions are satisfied type deduction distinguishes lvalues from rvalues Lvalues of type T are deduced to have type T while rvalues of type T yield T as their deducted type reference collapsing occurs The concept of universal reference is helpful to free you from recognizing the existence of reference collapsing contexts Typedef and using alias declarations also employ reference collapsing cpp template class Widget public typedef T RvalueRefToT suppose we have Widget w reference collapse makes it typedef int RvalueRefToT decltype also employs reference collapsing during its type analysis Takeaways Reference collapsing occurs in four contexts template instantiation auto type generation creation and use of typedefs and alias declarations and decltype When compilers generate a reference to a reference in a reference collapsing context the result becomes a single reference If either of the original references is an lvalue reference the result is an lvalue reference Otherwise its an rvalue reference Universal references are rvalue references in contexts where type deduction distinguishes lvalues from rvalues and where reference collapsing occurs Snippet cpp referencecollapsingmcpp include include include demonstrates an implementation of stdforward and types involved during its instantiation with rvalues and lvalues Test forward impl using template T myforwardstdremovereferencet param stdcout myforward T is deduced to be boosttypeindextypeidwithcvrprettyname n stdcout myforward param is deduced to be boosttypeindextypeidwithcvrprettyname n return staticcastparam template void funcU param stdcout U is deduced to be boosttypeindextypeidwithcvrprettyname n stdcout param deduced to be boosttypeindextypeidwithcvrprettyname n auto forwarded myforwardparam stdcout auto forwarded is deduced to be boosttypeindextypeidwithcvrprettyname n auto autobyvalue myforwardparam stdcout auto by value forwarded is deduced to be boosttypeindextypeidwithcvrprettyname n stdcout n int main int x 3 stdcout calling func with lvaluen funcx stdcout calling func with rvaluen func4 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it29","content":" Assume that move operations are not present not cheap and not used True that move may offer efficiency gains but working with C98 codebase youve no reason to assume move will be present whose generation will be suppressed if any custom copy move or dtor is in place it17 All containers in the standard C11 library support moving but moving in these containers may not be cheap Consider stdarray added in C11 a builtin array with an STL interface stdarray is fundamentally different the other standard containers each of which stores its contents on the heap Objects of those container types hold conceptually pointers to the heap location storing contents of the container making their move constant time Eg cpp stdvector vw1 put data into vw1 move vw1 into vw2 Runs in constant time Only ptrs in vw1 and vw2 are modified auto vw2 stdmovevw1 Such constant time move is not the case for stdarray whose content may not live in heap and have a pointer to the heap location cpp stdarray aw1 put data into aw1 move aw1 into aw2 Runs in linear time All elements in aw1 are moved into aw2 auto aw2 stdmoveaw1 On the other hand stdstring offers constanttime moves and lineartime copies This makes it sound like moving is faster than copying but that may not be the case Many string impl use small string optimization in which small strings 15 characters are stored in a buffer within the stdstring object as opposed to on the heap With SSO moving a string incurs the same cost as copying them cant employ moveonlyapointer trick Even with containers supporting move some move situations may actually incur copying It14 explains due to strong exception guarantee some can only move if they know the underlying move is noexcept Thus with C11 you can still end up with no move operations move not faster than copy move not usable or the source object is lvalue with few exceptions only rvalues are eligible as sources of move it25 Takeaways Assume that move operations are not present not cheap and not used In code with known types or support for move semantics there is no need for assumptions Snippet cpp assumenomovemcpp include include include include int main note the difference in behaviors between move on array and vector stdarray a 1 2 3 4 5 different from containers having pointer on stack pointing to content stored on heap stdarray encapsulates fixed size array and place its content on stack its move is then of the same complexity as its copy auto bstdmovea stdcout a array after move n for const auto s a stdcout s stdcout nb array after move n for const auto s b stdcout s stdvector a1 1 2 3 4 5 auto b1stdmovea1 stdcout na1 vector after move n for const auto s a1 stdcout s stdcout nb1 vector after move n for const auto s b1 stdcout s stdcout n int x 5 auto ystdmovex y 1 stdcout x n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it3","content":" Understand decltype decltype rules differ from template deduction it doesnt potentially drop referenceness have special handling for universal reference or pass by value instead it always spits back the type of the expression given to it In a typical use case where we use decltype to declare the return type of a function which is dependent upon template parameter types we do cpp template C11 works but auto authAndAccessContainer c Index i requires decltypeci refinement authenticateUser return ci the auto and indicates C11s trailing return type to account for the fact that the types of c and i arent known when we see the first auto template C14 auto authAndAccessContainer c Index i not quite correct authenticateUser return ci return type deduced from ci using template type deduction rules in this case byvalue Not quite right in the following sense stddeque d authAndAccessd 0 10 return d0 then assign 10 to it this wont compile as referenceness is dropped template C14 works decltypeauto but still authAndAccessContainer c Index i requires refinement authenticateUser return ci This works in the sense that the compiler is told to use decltype rules in lieu of template deduction rules for return type deduction Needs refinement in the sense that c can only be lvalue ref Similarly Widget w const Widget cw w auto myWidget1 cw auto type deduction myWidget1s type is Widget decltypeauto myWidget2 cw decltype type deduction myWidget2s type is const Widget template final decltypeauto C14 authAndAccessContainer c Index i version authenticateUser return stdforwardci c can now be universal reference template final auto C11 authAndAccessContainer c Index i version decltypestdforwardci authenticateUser return stdforwardci decltype have few surprises including the following cpp int x 0 decltypex yields int decltypex yields int expr x is an lvalue expression not just a name whose type is int Expanding on this In C14 this is the kind of code that puts you on the express train to UB decltypeauto f1 int x 0 return x decltypex is int so f1 returns int decltypeauto f2 int x 0 return x decltypex is int so f2 returns int Takeaways decltype almost always yields the type of a variable or expression without any modifications For lvalue expressions of type T other than names decltype always reports a type of T C14 supports decltypeauto which like auto deduces a type from its initializer but it performs the type deduction using the decltype rules Snippet cpp decltypemcpp include include include demonstrates the behaviors of decltypeauto and using it to deduct function return type test 1 type of expr is a reference decltypeauto f1 int x 0 return x decltypex is int so f1 returns int decltypeauto f2 int x 0 return x decltypex is int so f2 returns int this emits a compiler warning test 2 using perfect forwarding and decltypeauto to achieve function return type deduction template C14 auto authAndAccessContainer c Index i not quite correct return ci return type deduced from ci using template type deduction rules rule 3 by value template auto authAndAccess1Container c Index i return ci return type deduced from ci using template type deduction rules rule 1 nonuniversal reference pointer template decltypeauto authAndAccess2Container c Index i return stdforwardci The correct version int main test 1 f2 4 this would result in undefined behavior test 2 stdvector v vpushback1 authAndAccessv 0 1 would not compile authAndAccess1v 0 1 authAndAccess2v 0 1 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it30","content":" Familiarize yourself with perfect forwarding failure cases What we mean by perfect forwarding one function passes forwards its parameters to another function the second function should receive the first parameters that the first function receives which rules out passbyvalue since they are copies of what the original caller passes in When it comes to general purpose forwarding we are dealing with parameters that are references Forwarding functions are by their nature generic like this one with variadic templates cpp template void fwdTs params accept any arguments fstdforwardparams forward them to f f expression if this does one thing fwd expression but this does something else fwd fails to perfectly forward expression to f Several kinds of arguments can demonstrate the fwd f discrepancy failure Braced initalizers cpp suppose we have f like this void fconst stdvector v f 1 2 3 fine 1 2 3 implicitly converted to stdvector fwd 1 2 3 error doesnt compile The standard dictates that compilers are forbidden from deducing a type for the expression 1 2 3 in the call to fwd because fwds parameter isnt declared to be a stdinitializerlist Interestingly this would work cpp auto il 1 2 3 ils type deduced to be stdinitializerlist fwdil fine perfectforwards il to f As it2 pointed out the difference between auto type deduction and template type deduction 0 or NULL as null pointers would also cause type deduction to think of them as integral as opposed to pointer types Declarationonly integral static const and constexpr data members cpp class Widget public static constexpr stdsizet MinVals 28 MinVals declaration no defn for MinVals stdvector widgetData widgetDatareserveWidgetMinVals use of MinVals Here compiler is Ok with a missing definition of MinVals as it does const propagation to replace occurrences of MinVals with 28 and does not allocate storage for MinVals If MinVals address were to be taken then the above code would compile but would fail at link time And if we have cpp void fstdsizet val fWidgetMinVals fine treated as f28 fwdWidgetMinVals error shouldnt link Nothing in the code takes MinVals address but fwds parameter being universal references and the compiler generated code usually treat them like pointers Not all compilers enforce this behavior To make your code portable define MinVals cpp constexpr stdsizet WidgetMinVals in Widgets cpp file Overloaded function names and template names cpp suppose our f looks like the following void fint pfint pf processing function or void fint pfint declares same f as above and we have int processValint value int processValint value int priority and we have the following fprocessVal fine fwdprocessVal error which processVal same problem if we try to give the fwd function a function template template T workOnValT param template for processing values fwdworkOnVal error which workOnVal instantiation to make this work you could manually specify the overload or instantiation using ProcessFuncType make typedef int int see Item 9 ProcessFuncType processValPtr processVal specify needed signature for processVal fwdprocessValPtr fine fwdstaticcastworkOnVal also fine Bitfields cpp Say we have the following to model IPv4 headers struct IPv4Header stduint32t version4 IHL4 DSCP6 ECN2 totalLength16 void fstdsizet sz function to call IPv4Header h fhtotalLength fine fwdhtotalLength error This is because htotalLength is a nonconst bitfield and the standard says a nonconst reference shall not be bound to a bitfield Reason being bitfields may consist of arbitrary parts of machine words eg bits 35 of a 32bit int but theres no way to directly address such bits as the smallest unit you can point to is a char Workaround this is easy once you find a bitfield make a copy yourself and call fwd on the copy cpp copy bitfield value see Item 6 for info on init form auto length staticcasthtotalLength fwdlength forward the copy Takeaways Perfect forwarding fails when template type deduction fails or when it deduces the wrong type The kinds of arguments that lead to perfect forwarding failure are braced initializers null pointers expressed as 0 or NULL declarationonly integral const static data members template and overloaded function names and bitfields Snippet cpp perfectforwardingfailuremcpp include include include void fconst stdvector v for const auto s v stdcout s n template void fwdTs params fstdforwardparams forward them to f int main f1 2 3 fwd1 2 3 fails to compile auto initializerList 1 2 3 fwdinitializerList all good return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it31","content":" Avoid default capture modes There are two default capture modes in C11 byreference and byvalue Default byreference capture can lead to dangling references Default byvalue capture lures you into thinking youre immune to that problem youre not and it lulls you into thinking your closures are selfcontained they may not be A byreference capture causes a closure to contain a reference to a local variable or to a parameter thats available in the scope where the lambda is defined If the lifetime of a closure created from that lambda exceeds the lifetime of the local variable or parameter the reference in the closure will dangle Eg cpp using FilterContainer see Item 9 for stdvectorstdfunction using Item 5 for stdfunction FilterContainer filters filtering funcs add a filter for multiples of 5 like filtersemplaceback see Item 42 for int value return value 5 0 info on emplaceback if we dont want to hardcode the 5 the code would look like void addDivisorFilter auto calc1 computeSomeValue1 auto calc2 computeSomeValue2 auto divisor computeDivisorcalc1 calc2 filtersemplaceback danger int value return value divisor 0 ref to divisor will dangle UB lambda refers to local var divisor which ceases to exist when addDivisorFilter returns THe function added to the vector is then basically dead on arrival With explicit captures same problem filtersemplaceback divisorint value danger ref to return value divisor 0 divisor will still dangle If you know a closure will be used immediately eg being passed to an STL algorithm and wont be copied there is no risk that references it holds will outlive the local variables and parameters in the environment where its lambda is created But long term its simply better software engineering to explicitly list the local variables and parameters that a lambda depends on One way to solve the problem with divisor would a byvalue capture mode cpp filtersemplaceback now int value return value divisor 0 divisor cant dangle However by value capture might not work for pointer parameters in which case you copy a pointer and outside they might delete the pointer Consider the following code cpp class Widget public ctors etc void addFilter const add an entry to filters private int divisor used in Widgets filter addFilter could be defined like this void WidgetaddFilter const filtersemplaceback int value return value divisor 0 This looks safe but is not captures only apply to nonstatic local variables including parameters visible in the scope where the lambda is created In the body of WidgetaddFilter divisor is not a local variable but a data member of Widget It cannot be captured Yet if you leave out the default capture mode or explicitly put divisor to be captured the capture wont compile cpp filtersemplaceback divisorint value error no local return value divisor 0 divisor to capture filtersemplaceback error int value return value divisor 0 divisor not So what are we capturing and how do we make it safe The explanation hinges on the implicit use of a raw pointer this inside Widget member function compiler replaces divisor with thisdivisor and in the first version of WidgetaddFilter the default byvalue capture captures Widgets this pointer not divisor Like this cpp void WidgetaddFilter const auto currentObjectPtr this filtersemplaceback currentObjectPtrint value return value currentObjectPtrdivisor 0 Thus the viability of the added closure from this lambda is tied to the lifetime of the Widget whose this pointer it obtained a copy of Consider this code cpp using FilterContainer as before stdvectorstdfunction FilterContainer filters as before void doSomeWork auto pw create Widget see stdmakeunique Item 21 for stdmakeunique pwaddFilter add filter that uses Widgetdivisor destroy Widget filters now holds dangling pointer This particular problem can be solved by making a local copy of the data member you want to capture then capturing the copy cpp void WidgetaddFilter const auto divisorCopy divisor copy data member filtersemplaceback divisorCopyint value capture the copy return value divisorCopy 0 use the copy Default capture mode with just would work as well but why tempt fate A default capture mode is what made it possible to accidentally capture this pointer when you thought its capturing divisor In C14 a better way to capture a data member is to use generalizd lambda capture cpp void WidgetaddFilter const filtersemplaceback C14 divisor divisorint value copy divisor to closure return value divisor 0 use the copy An additional drawback to default byvalue captures is that they can suggest that the corresponding closures are selfcontained and insulated from changes to data outside the closures In general thats not true because lambdas may be dependent not just on local variables and parameters which may be captured but also on objects with static storage duration Such objects are defined at global or namespace scope or are declared static inside classes functions or files These objects can be used inside lambdas but they cant be captured Yet specification of a default byvalue capture mode can lend the impression that they are Consider this code cpp void addDivisorFilter static auto calc1 computeSomeValue1 now static static auto calc2 computeSomeValue2 now static static auto divisor now static computeDivisorcalc1 calc2 filtersemplaceback int value captures nothing return value divisor 0 refers to above static divisor modify divisor In here nothing is captured the lambda refers to the static variable divisor Thus the lambdas pushed into filters will exhibit new behaviors as the static divisor changes If you stay away from default byvalue capture clauses you eliminate the risk of your code being misread in this way Takeaways Default byreference capture can lead to dangling references Default byvalue capture is susceptible to dangling pointers especially this pointer and it misleadingly suggests that lambdas are selfcontained Snippet cpp avoiddefaultcapturemodesmcpp include include include using FilterContainer stdvectorstdfunction class Widget public Widgetint x dxx void setXint x noexcept dx x void appendFilterFilterContainer filters this doesnt capture dx it captures this filterspushbackint value return value dx 0 void appendFilterrefFilterContainer filters this doesnt capture dx it captures this filterspushbackint value return value dx 0 void appendFiltersafeFilterContainer filters this doesnt capture dx it captures this filterspushbackdx dxint value return value dx 0 alternatively captures by value int x dx filterspushbackxint value return value x 0 private int dx void uniqueptrubFilterContainer container auto w1 stdmakeunique3 w1appendFiltercontainer int main Test that changing data member value affects the added closures behavior FilterContainer container Widget w3 wappendFiltercontainer stdcout container04 n wsetX4 this changes the behavior of the pushed closure stdcout container04 n Test that deallocating this results in UB FilterContainer container1 uniqueptrubcontainer1 stdcout container104 n undefined behavior as the Widget allocated by the uniqueptr in the call has its this go out of scope stdcout container103 n we dont directly observe anything uncommon here return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it32","content":" Use init capture to move objects into closures Sometimes neither byvalue capture or byreference capture is what you want You want to move the object into the closure say if its moveonly like stdunqiueptr or if copy is much more expensive than move say for most STL containers C11 offers no way to accomplish that C14 introduced init capture to support moving params into lambda and more you cant express a default capture with it but you shouldnt use a default capture mode anyway as item 31 suggests Init capture makes it possible to specify the name of a data member in the closure class generated from the lambda an expression initializing that data member Eg cpp class Widget some useful type public bool isValidated const bool isProcessed const bool isArchived const private auto pw stdmakeunique create Widget see Item 21 for info on stdmakeunique configure pw auto func pw stdmovepw init data mbr return pwisValidated in closure w pwisArchived stdmovepw left is the name of the data member in the closure and right is the initialization expression scope of the left is that of the closure class and scope on the right is where the lambda is defined So pw stdmovepw means create a data member pw in the closure and initialize that data member with the result of applying stdmove to the local variable pw if the configure pw part is not necessary we could just do auto func pw stdmakeunique init data mbr return pwisValidated in closure w pwisArchived result of call to makeunique The notion of capture in C14 as shown above is generalized from C11 thus earning init capture another name generalized lambda capture How can you accomplish in C11 without compiler support You can do it by hand the above C14 code translates to the following function object cpp class IsValAndArch is validated public and archived using DataType stduniqueptr explicit IsValAndArchDataType ptr Item 25 explains pwstdmoveptr use of stdmove bool operator const return pwisValidated pwisArchived private DataType pw auto func IsValAndArchstdmakeunique If you want to stick to C11 lambdas you could move the object to be captured into a function object produced stdbind and giving the lambda a reference to the captured object Eg cpp C14 move vector into lambda stdvector data object to be moved into closure populate data auto func data stdmovedata C14 init capture uses of data C11 move vector into lambda auto func stdbind C11 emulation const stdvector data of init capture uses of data stdmovedata Question bind an rvalue to a const lvalue reference results in a move from the rvalue being bound Like lambda expressions stdbind produces function objects bind object the first argument is a callable object and subsequent arguments are values to be passed to that object For each lvalue argument in bind the corresponding object in the bind object is copy ctored and for each rvalue its moved constructed By default the operator member function inside the closure class generated from a lambda is const The move ctored copy of data inside the bind object is not const To prevent that copy of data from being modified inside the lambda we declare it const reference If the lambda were declared mutable itd be appropriate to drop the const cpp auto func stdbind C11 emulation stdvector data mutable of init capture uses of data for mutable lambda stdmovedata The bind object stores copies of all arguments passed to stdbind in our case a copy of the closure produced by the lambda that is its first argument The lifetime of the closure is therefore the same as the lifetime of the bind object The fundamentals of stdbind in this case would be Its not possible to moveconstruct an object into a C11 closure but it is possible to moveconstruct an object into a C11 bind object Emulating movecapture in C11 consists of moveconstructing an object into a bind object then passing the moveconstructed object to the lambda by reference Because the lifetime of the bind object is the same as that of the closure its possible to treat objects in the bind object as if they were in the closure Similarly cpp auto func pw stdmakeunique as before return pwisValidated create pw pwisArchived in closure auto func stdbind const stduniqueptr pw return pwisValidated pwisArchived stdmakeunique Takeaways Use C14s init capture to move objects into closures In C11 emulate init capture via handwritten classes or stdbind Snippet cpp initcapturemcpp include include include class Widget public Widgetint x dxx void doStuff const noexcept stdcout dx n private int dx What a closure looks like context imitating C14 init capture with C11 class WidgetDoStuff public using DataType stduniqueptr explicit WidgetDoStuffDataType ptr Item 25 explains pwstdmoveptr use of stdmove void operator const pwdoStuff private DataType pw int main Test of C14 init capture auto func widget stdmakeunique3 widgetdoStuff stdvector v 1 2 3 stdcout C14 vector moved into closuren x stdmovev for const auto s x stdcout s n stdcout C14 vector outside after moven for const auto s v stdcout s n Imitate init capture with bind in C11 stdcout C11 vector moved into closuren stdvector v1 4 5 6 auto func1 stdbind stdvector v mutable vpushback7 for const auto s v stdcout s n stdmovev1 func1 stdcout C11 vector outside after moven for const auto s v1 stdcout s n auto w1 WidgetDoStuffstdmakeunique5 w1 return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it33","content":" Use decltype on auto parameters to stdforward them C14 introduced generic lambdas they use auto in their parameter specification Eg cpp auto f auto x return normalizex The underlying closure class is implemented as class SomeCompilerGeneratedClassName public template see Item 3 for auto operatorT x const auto return type return normalizex other closure class functionality In the example all the lambda does with x is forward it to normalize and if normalize treats lvalues and rvalues differently the lambda will always forward a lvalue The correct way is to have the lambda perfect forward x to normalize and to do that cpp auto f auto x return normalizestdforwardx Note that decltypex will produce a lvalue reference if x is an lvalue and rvalue reference if x is an rvalue due to the type of x being universal reference Item 28 explained that when calling stdforward convention dictates the type to instantiate stdforward with be an lvalue reference to indicate an lvalue and a nonreference to indicate an rvalue If x is bound to lvalue using decltypex to instantiate stdforward conforms to convention but if x is bound to rvalue decltypex would yield an rvalue reference instead of a nonreference However applying reference collapse rules instantiating stdforward with nonreference types and rvalue reference types end up producing the same code cpp template in namespace T forwardremovereferencet param std return staticcastparam The above establishes that using decltypex to instantiate the stdforward inside the lambda produces the expected result And since C14 allows variadic lambdas we forward all the arguments inside the lambda cpp auto f auto xs return normalizestdforwardxs Takeaways Use decltype on auto parameters to stdforward them Snippet cpp decltypeautorefforwardmcpp include include void forwardedfuncint a int b int c stdcout lvalue variation of func a b c n void forwardedfuncint a int b int c stdcout rvalue variation of func a b c n int main Test of C14 generic variadic lambdas with perfect forwarding auto f auto xs forwardedfuncstdforwardxs f1 2 3 int a 4 int b 5 int c 6 fa b c return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it34","content":" Prefer lambdas to stdbind stdbind in C11 succeeds stdbind1st and stdbind2nd in C98 In C11 lambdas are almost always a better choice than stdbind Suppose we have the following cpp typedef for a point in time see Item 9 for syntax using Time stdchronosteadyclocktimepoint see Item 10 for enum class enum class Sound Beep Siren Whistle typedef for a length of time using Duration stdchronosteadyclockduration at time t make sound s for duration d void setAlarmTime t Sound s Duration d Then we decide we want an alarm that goes off an hour after its set and will stay on for 30 seconds but we dont know the sound it should play So we revise the interface with a lambda setSoundL L for lambda is a function object allowing a sound to be specified for a 30sec alarm to go off an hour after its set auto setSoundL Sound s make stdchrono components available wo qualification using namespace stdchrono setAlarmsteadyclocknow hours1 alarm to go off s in an hour for seconds30 30 seconds side note with C14 stdliterals you could do using namespace stdliterals setAlarmsteadyclocknow 1h C14 but s same meaning 30s as above To achieve the same thing with stdbind cpp using namespace stdchrono as above using namespace stdliterals using namespace stdplaceholders needed for use of 1 auto setSoundB B for bind stdbindsetAlarm steadyclocknow 1h incorrect 1 30s The first thing we dont like is the placeholder 1 it means the first argument in a call to setSoundB is passed as the second argument to setAlarm Its type is not identified in the call to stdbind so readers have to consult setAlarm declaration to determine what kind of argument to pass to setSoundB In addition how its stored inside the bind object is not clear by reference or by value In fact its stored by value but such facts require muscle memory of bind as opposed to clarity offered by lambdas And also when calling setSoundBsound how is sound passed to setSoundB The answer is that its passed by reference because function call operator for such objects uses perfect forwarding In lambda this is clear from the code as well The bigger problems that the now will be evaluated when bind is called not when setSoundB is called thus not our desired behavior To defer the evaluation of now we do the following cpp C14 where you can do stdplus auto setSoundB stdbindsetAlarm stdbindstdplus stdbindsteadyclocknow 1h 1 30s C11 struct genericAdder template auto operatorT1 param1 T2 param2 decltypestdforwardparam1 stdforwardparam2 return stdforwardparam1 stdforwardparam2 auto setSoundB stdbindsetAlarm stdbindgenericAdder stdbindsteadyclocknow hours1 1 seconds30 When setAlarm is overloaded a new issue arises suppose we have cpp enum class Volume Normal Loud LoudPlusPlus void setAlarmTime t Sound s Duration d Volume v The lambda version would work as it still uses the 3parameter overload but the bind call now has no way to determine which overload should be called To make it work you have cpp using SetAlarm3ParamType voidTime t Sound s Duration d auto setSoundB now stdbindstaticcastsetAlarm okay stdbindstdplus stdbindsteadyclocknow 1h 1 30s Another implication is that compiler can likely inline setSoundL but less likely to be able to inline a bind call made through a function pointer cpp setSoundLSoundSiren body of setAlarm may well be inlined here setSoundBSoundSiren body of setAlarm is less likely to be inlined here If you do something more complicated the scales tip even further in favor of lambdas Eg cpp Lambda version C14 auto betweenL lowVal highVal const auto val C14 return lowVal val val highVal Lambda version C11 auto betweenL C11 version lowVal highVal int val return lowVal val val highVal bind version C14 using namespace stdplaceholders as above auto betweenB stdbindstdlogicaland C14 stdbindstdlessequal lowVal 1 stdbindstdlessequal 1 highVal bind version C11 auto betweenB C11 version stdbindstdlogicaland stdbindstdlessequal lowVal 1 stdbindstdlessequal 1 highVal Before C14 bind can be justified in move capture item 32 and polymorphic function objects like the following cpp Given class PolyWidget public template void operatorconst T param const stdbind can do PolyWidget pw auto boundPW stdbindpw 1 boundPW can then be called with different types of arguments boundPW1930 pass int to PolyWidgetoperator boundPWnullptr pass nullptr to PolyWidgetoperator boundPWRosebud pass string literal to PolyWidgetoperator C11 lambda has no way to express this but not the case in C14 in C14 you can do auto boundPW pwconst auto param C14 pwparam Takeaways Lambdas are more readable more expressive and may be more efficient than using stdbind In C11 only stdbind may be useful for implementing move capture or for binding objects with templatized function call operators Snippet cpp preferlambdaoverbindmcpp include include include using Time stdchronosteadyclocktimepoint using namespace stdchrono using namespace stdliterals using namespace stdplaceholders using Duration stdchronosteadyclockduration enum class Sound Beep Siren Whistle at time t make sound s for duration d void setAlarmTime t Sound s Duration d stdcout make sound staticcasts n int main Comparison of currying using lambdas and binds auto duration 30s auto setSoundB stdbindsetAlarm note how a 2nd bind is needed here to get the desired behavior of evaluting now at setSoundB call instead of at bind call stdbindstdplus stdbindsteadyclocknow 1h note how the placeholder 1 does not convey afterwards when calling setSoundB is this argument passed by value or reference into setAlarm Reference 1 note also its not clear how the captured values are stored inside this bind object by reference or value Value duration auto setSoundL durationSound s setAlarmsteadyclocknow 1h alarm to go off s in an hour for duration 30 seconds setSoundBSoundBeep setSoundLSoundSiren return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it35","content":" Prefer taskbased programming to threadbased If you want to run a function doAsyncWork asynchronously you can do cpp int doAsyncWork stdthread tdoAsyncWork threadbased or auto fut stdasyncdoAsyncWork taskbased fut for future function object passed to stdasync is considered a task The taskbased approach is superior the caller of doAsyncWork likely is interested in its return value with the taskbased approach the future object offers a get call to retrieve the return value and if doAsyncWork throws the future object can get access to that too If the threadbased approach throws the program dies via a call to stdterminate A more fundamental difference lies in the abstraction the taskbased approach embodies it frees you from thread management Three meanings of thread Hardware thread the one actually performing computation Contemporary machine architectures offer one or more hardware threads per CPU core Software OS system threads The threads OS manages across all processes and schedules for execution on hardware threads You can have more OS threads than hardware threads When one is blocked OS can switch to the execution of other unblocked ones stdthreads are objects in a C process that act as handles to underlying software thread Some stdthread objects represent null handles ie no corresponding software threads because they are default ctored have been moved from have been joined the function they were to run has finished or have been detached Software threads are a limited resource If you try to create more than the system can provide a stdsystemerror is thrown cpp int doAsyncWork noexcept see Item 14 for noexcept could throw stdthread tdoAsyncWork throws if no more threads are available Wellwritten software must deal with the possibility of stdthread throw but there is no obvious way run on current thread potential unbalanced load after some thread finishes they could be waiting on something to be produced by doAsyncWork Even if you dont run out of threads you could run into oversubscription having more unblocked software threads than hardware threads in which case CPU time slices software threads on the hardware and performs context switches in between Crosscore switches are particularly expensive CPU caches are cold for the new thread whose execution will pollute the cache for the threads previously there Avoid oversubscription is difficult since the dynamics of the number of unblocked software threads is hard to predict Your life will be easier if you dump these problems on somebody else and in this case stdasync Using cpp auto fut stdasyncdoAsyncWork onus of thread mgmt is on implementer of the Standard Library Your likelihood of outofthreads exception is greatly reduced since stdasync when called in this form doesnt guarantee a new thread will be created rather it permits the scheduler to arrange the function doAsyncWork to be run on the thread requesting doAsyncWorks result the thread calling get wait on the future object Reasonable schedulers take advantage of this freedom if the system is oversubscribed or is out of threads You could roll your own run it on the thread needing the result but stdasync and the runtime scheduler across the OS is likely to have a better picture of the machine than you do However with just stdasync the scheduler doesnt know which of your threads has tight responsiveness requirements eg GUI in which case youll want to pass stdlaunchasync launch policy to the stdasync item 36 Stateoftheart thread schedulers employ systemwide thread pools to avoid oversubscription and they improve load balancing across hardware cores through workstealing algorithms C Standards does not require the use of thread pools or work stealing but some vendors implement this in their standard library and if you use a task based approach you reap the benefits automatically That said there are scenarios where using threads directly may be appropriate Eg You need access to the API of the underlying threading implementation Eg the underlying pthread or Windows thread may have richer set of features than C stdthread in which case you can use nativehandle member function of stdthread which you cant do with stdfuture You need to and are able to optimize thread usage for your application You know the execution profile and are deploying it as the only significant process on a machine with fixed characteristics You need to implement threading technology beyond the C concurrency API eg thread pools on platforms where your C implementations dont offer them These cases are uncommon Most of the time prefer taskbased designs over threadbased Takeaways The stdthread API offers no direct way to get return values from asynchronously run functions and if those functions throw the program is terminated Threadbased programming calls for manual management of thread exhaustion oversubscription load balancing and adaptation to new platforms Taskbased programming via stdasync with the default launch policy handles most of these issues for you Snippet cpp prefertaskbasedoverthreadbasedmcpp include include include include int doAsyncWorkint a int b noexcept stdcout doAsyncWork a b n return a b int main A quick look at stdthread and stdasync future stdthread tdoAsyncWork 4 5 auto future stdasyncdoAsyncWork 3 4 auto res futureget stdcout res n crashes without joining t to main tjoin return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it36","content":" Specify stdlaunchasync if asynchronicity is essential When you stdasync something you are asking it to launch with a launch policy There are two defined stdlaunchasync means the function has to be run asynchronously on a different thread stdlaunchdeferred means the function may run when get or wait is called on the future returned by stdasync When get or wait is invoked the function will execute synchronously If neither is invoked the function will never run The default launch policy is neither of the two but the or of them cpp these two are the same thing auto fut1 stdasyncf run f using default launch policy auto fut2 stdasyncstdlaunchasync run f either stdlaunchdeferred async or f deferred Using the default launch policy means if we do this on a thread t cpp auto fut stdasyncf run f using default launch policy Its not possible to predict whether f will run concurrently with t its not possible to predict whether f runs on a thread different from the thread invoking get or wait on fut It may not be possible to predict whether f runs at all The default launch policy mixes poorly with the use of threadlocal variables TLS thread local storage because if f reads or writes such its not possible to predict which threads variables will be acccessed cpp auto fut stdasyncf TLS for f possibly for independent thread but possibly for thread invoking get or wait on fut It also means loops like this one may run forever cpp using namespace stdliterals for C14 duration suffixes see Item 34 void f f sleeps for 1 second then returns stdthisthreadsleepfor1s auto fut stdasyncf run f asynchronously conceptually while futwaitfor100ms loop until f has stdfuturestatusready finished running which may never happen In this case if f runs concurrently with the thread calling stdasync launch policy stdlaunchasync there is no problem But if f is deferred the status will never be ready waitfor and waituntil on a task thats deferred yields the value stdfuturestatusdeferred This kind of bug can be hard to catch in unit testing the scheduler may defer f only if the system is under heavy loads or threatened by oversubscription To fix this we shouldnt enter the loop if the execution of f is deferred Like this cpp auto fut stdasyncf as above if futwaitfor0s if task is stdfuturestatusdeferred deferred use wait or get on fut to call f synchronously else task isnt deferred while futwaitfor100ms infinite loop not stdfuturestatusready possible assuming f finishes task is neither deferred nor ready so do concurrent work until its ready fut is ready Unfortunately there is no API on future object to tell if its deferred so we do a wait for 0s instead The upshot of these various considerations is that using stdasync with the default policy for a task is fine as long as the task need not run concurrently with thread calling get or wait it doesnt matter which threads local storage are read or written either theres guarantee that get or wait will be called on the future returned by stdasync or its acceptable that the task may never execute code using waitfor and waituntil takes the possibility of deferred status into account If any of the conditions do not hold you may want to guarantee a truly asynchronous execution cpp auto fut stdasyncstdlaunchasync f launch f asynchronously or have this generic tool C14 template inline auto reallyAsyncF f Ts params return future for asynchronous return stdasyncstdlaunchasync call to fparams stdforwardf stdforwardparams use reallyAsync just like async auto fut reallyAsyncf run f asynchronously throw if stdasync would throw C11 template inline stdfuturetypename stdresultoftype reallyAsyncF f Ts params return future for asynchronous return stdasyncstdlaunchasync call to fparams stdforwardf stdforwardparams Takeaways The default launch policy for stdasync permits both asynchronous and synchronous task execution This flexibility leads to uncertainty when accessing threadlocals implies that the task may never execute and affects program logic for timeoutbased wait calls Specify stdlaunchasync if asynchronous task execution is essential Snippet cpp specifylaunchasyncifasynchronicityessentialmcpp include include include include using namespace stdliterals void doAsyncWork noexcept stdthisthreadsleepfor1s int main A quick look at stdthread and stdasync future auto fut stdasyncdoAsyncWork default launch policy can choose deferred auto fut stdasyncstdlaunchdeferred doAsyncWork deferred the loop never stops auto fut stdasyncstdlaunchasync doAsyncWork async works just fine while futwaitfor100ms stdfuturestatusready stdcout waitingn stdcout donen return 0 to fix this we could add if futwaitfor0s stdfuturestatusdeferred futget else while futwaitfor100ms stdfuturestatusready stdcout waitingn stdcout donen "},{"title":"unprocessed","href":"/effectives/emcpp/it37","content":" Make stdthreads unjoinable on all paths Every stdthread is in one of two states joinable or unjoinable A thread corresponding to an underlying thread thats blocked or waiting to be scheduled is joinable stdthread objects corresponding to underlying threads that have completed are also considered joinable Unjoinable thread objects include default ctored stdthread no function to execute stdthreads that have been moved from whose underlying execution now corresponds to a different thread stdthreads that have been joined After the join the thread object no longer corresponds to the underlying thread of execution that has finished running stdthreads that have been detached detach severs the connection between a stdthread object and the underlying thread of execution it corresponds to We care about the joinability of a thread because calling dtor on a joinable thread will cause the entire process to be terminated Say we have the following we use the threadbased approach as opposed to taskbased since we want to configure the priority of this thread the native handle part cpp constexpr auto tenMillion 10000000 see Item 15 for constexpr bool doWorkstdfunction filter returns whether int maxVal tenMillion computation was performed see Item 5 for stdfunction stdvector goodVals values that satisfy filter stdthread tfilter maxVal goodVals populate goodVals for auto i 0 i maxVal i if filteri goodValspushbacki auto nh tnativehandle use ts native handle to set ts priority if conditionsAreSatisfied tjoin let t finish performComputationgoodVals return true computation was performed return false computation was not performed If conditionsAreSatisfied returns true this is all good if it returns false or throws an exception as the stack unwinds dtor will be called on a joinable t and the process would halt Why does a stdthread dtor behave this way Because the other options are worse An implicit join means dtor would wait for the asynchronous execution to finish This is counterintuitive to debug An implicit detach the underlying thread would continue to run but its connection to the thread object is severed In the above code example where goodVals local variable is passed by reference to the thread function when detach happens doWork finishes and goodVals unwinds the running thread would be looking at a stack frame thats popped or worse occupied by a later function call This puts the onus on you to ensure if you use a stdthread object its made joinable on every path out of the scope in which it is defined Any time you want to such things as making sure to perform some action along every path out of a block RAII naturally comes in mind RAII is predominant in the Standard Library like stduniqueptr stdweakptr stdsharedptr and stdfstream dtors etc The Standard Library does not have an RAII class for stdthread you could do one such yourself that takes in a DtorAction cpp class ThreadRAII public enum class DtorAction join detach see Item 10 for enum class info ThreadRAIIstdthread t DtorAction a in dtor take actiona tstdmovet action a on t in ctor note that we only move thread to be managed by this RAII object note that we cant copy stdthread objects ThreadRAII if tjoinable joinability test is necessary since join or detach on unjoinable threads yields UB if action DtorActionjoin tjoin If you are worried about the potential race condition between joinable check and join detach actions such worries are unfounded a stdthread object can change state from joinable to unjoinable only through a member function call or a move operation At the time a ThreadRAII objects dtored invoked no other thread should be making member function calls on that object The client code still could invoke dtor and something else on the object at the same time but should be made aware such calls could result in race conditions else tdetach stdthread get return t see below private DtorAction action stdthread t note the order of data members In this case it doesnt matter but you usually want to put them to last in a classs members since once initialized they may start running immediately and running them may require other member variables to be already initialized And our client code now looks like cpp bool doWorkstdfunction filter as before int maxVal tenMillion stdvector goodVals as before ThreadRAII t use RAII object stdthreadfilter maxVal goodVals for auto i 0 i maxVal i if filteri goodValspushbacki ThreadRAIIDtorActionjoin RAII action auto nh tgetnativehandle if conditionsAreSatisfied tgetjoin performComputationgoodVals return true return false Note that join is still not a desired behavior in that it could lead to performance anomaly or even hung a program item 39 The proper solution would be to communicate to the asynchronously running lambda that we no longer need its work and it should return early But there is no such support for interruptible threads in C They can be implemented by hand but is beyond the scope of now Since weve custom dtors compiler will suppress move operations generation theres no reason ThreadRAIIs not movable so we could add cpp ThreadRAIIThreadRAII default support ThreadRAII operatorThreadRAII default moving Takeaways Make stdthreads unjoinable on all paths eg through RAII joinondestruction can lead to difficulttodebug performance anomalies detachondestruction can lead to difficulttodebug undefined behavior Declare stdthread objects last in lists of data members Snippet cpp threadjoinableraiimcpp include include include include class ThreadRAII public enum class DtorAction join detach see Item 10 for enum class info ThreadRAIIstdthread t DtorAction a in dtor take actiona tstdmovet action a on t in ctor note that we only move thread to be managed by this RAII object note that we cant copy stdthread objects ThreadRAII if tjoinable joinability test is necessary since join or detach on unjoinable threads yields UB if action DtorActionjoin tjoin If you are worried about the potential race condition between joinable check and join detach actions such worries are unfounded a stdthread object can change state from joinable to unjoinable only through a member function call or a move operation At the time a ThreadRAII objects dtored invoked no other thread should be making member function calls on that object The client code still could invoke dtor and something else on the object at the same time but should be made aware such calls could result in race conditions else tdetach stdthread get return t see below ThreadRAIIThreadRAII default support ThreadRAII operatorThreadRAII default moving private DtorAction action stdthread t note the order of data members In this case it doesnt matter but you usually want to put them to last in a classs members since once initialized they may start running immediately and running them may require other member variables to be already initialized bool conditionsAreSatisfied return false bool doWorkstdfunction filter as before int maxVal 10000000 stdvector goodVals as before ThreadRAII t use RAII object stdthreadfilter maxVal goodVals for auto i 0 i maxVal i if filteri goodValspushbacki ThreadRAIIDtorActionjoin RAII action stdthread tfilter maxVal goodVals for auto i 0 i maxVal i if filteri goodValspushbacki auto nh tgetnativehandle if conditionsAreSatisfied tgetjoin tjoin stdcout perform computationn return true return false int main A quick look at stdthread and stdasync future doWorkint x return x 10 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it38","content":" Be aware of varying thread handle destructor behavior A joinable stdthread corresponds to an underlying system thread of execution a future for a nondeferred task has a similar relationship to a system thread As such both stdthread objects and future objects can be thought of as handles to system threads Yet dtor on a joinable thread results in program termination but dtor on a future sometimes behaves as if it did an implicit join sometimes detach and sometimes neither It never causes program termination Think about the execution of a future object the asynchronously executed callee needs to transmit result back to the caller typically via a stdpromise object but where does the result live The callee could finish before caller invokes get It cannot live in the stdpromise as that object being local to the callee would be destroyed when the callee finishes The result cannot live in the callers future either because a stdfuture may be used to create a stdsharedfuture thus transferring ownership of the callers result from the stdfuture to the stdsharedfuture which may then be copied many times after the original future is destroyed Given that not all result types can be copied moveonly for example and the result must live as long as the last future referencing to it which of the potentially many futures corresponding to the callee should be the one to contain its result Since neither caller or callee are suitable for storing the callees result its stored in a location outside both known as the shared state Its implementation is typically heap based but not specified by the standards The dataflow is illustrated as such Caller future Shared State Callees Result stdpromise typically Callee The behavior of the dtor of a future is determined by the shared state associated with it The dtor for the last future referring to a shared state for a nondeferred task launched via stdasync blocks until the task completes implicit join The dtor for all other futures simply destroys the future object for asynchronously running tasks this is an implicit detach for deferred tasks for which this is the final future it means the deferred task will never run In other words the dtor of a future is a normal behavior and one exception The normal behavior is that a futures dtor destroys the future object It doesnt join with anything it doesnt detach from anything it doesnt run anything It just destroys the futures data members and decrements the reference count inside the shared state thats manipulated by both the futures referring to it and the callees stdpromise The exception happens when the future 1 refers to a shared state that was created due to a call to stdasync 2 the tasks launch policy is stdlaunchasync can be explicitly specified or decided by the runtime system and 3 the future is the last future referring to the shared state this matters for sharedfutures This exceptional behavior is to implicitly join This decision of implicit join special case is controversial but present in both C11 and C14 Futures API offers no way to determine whether a future refers to a shared state arising from a call to stdasync so given an arbitrary future object its not possible to know whether itll block on dtor or not Eg cpp this container might block in its dtor because one or more contained futures could refer to a shared state for a non deferred task launched via stdasync stdvectorstdfuture futs see Item 39 for info on stdfuture class Widget Widget objects might public block in their dtors private stdsharedfuture fut Unless you know in your program logic for all these futures one of the three conditions wont be met Like this cpp int calcValue func to run stdpackagedtask wrap calcValue so it ptcalcValue can run asynchronously auto fut ptgetfuture get future for pt this future does not refer to a shared state created by stdasync and its dtor should not block To illustrate why have the special case for reference to shared states arose due to a stdasync say instead of stdasync you create a thread on the future stdthread tstdmovept packagedtask cannot be copied end block if nothing happens to t before end block t will be joinable and program will terminate if a join is done on t there is no need for fut to block its dtor because join is already present if a detach is done on t there is no need for fut to detach in its dtor because the calling code already does that Takeaways Future dtors normally just destroy the futures data members The final future referring to a shared state for a nondeferred task launched via stdasync blocks until the task completes Snippet cpp threadhandledtormcpp include include include include using namespace stdliterals bool doWork stdcout job launchn stdthisthreadsleepfor1s stdcout job donen return true int main A quick look at different behaviors in stdfuture dtor stdcout fut1 asyncn auto fut stdasyncstdlaunchasync doWork implicit join on fut dtor stdcout fut2 packagedtaskn stdpackagedtask ptdoWork can run asynchronously auto fut1 ptgetfuture get future for pt stdthread tstdmovept just destroy members on fut1 dtor do nothing program halts on t dtor do detach tdetach do join tjoin in any case dtor of fut1 just destroys data members no implicit join or detach stdcout program done return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it39","content":" Consider void futures for oneshot event communication Sometimes its useful for a task to tell a second asynchronously running task that a particular event has occurred eg a synchronization between the two threads One obvious choice for this is a condition variable the reacting task waits on the detecting task Eg cpp detecting task the one sending out the signal to tell the reacting task stdconditionvariable cv condvar for event stdmutex m mutex for use with cv detect event cvnotifyone tell reacting task if there are multiple to be notified use notifyall reacting task prepare to react open critical section stduniquelock lkm lock mutex cvwaitlk wait for notify this isnt correct react to event m is locked close crit section unlock m via lks dtor continue reacting m now unlocked before callling wait on the condition variable it must lock a mutex through stduniquelock part of C11 API This code has a code smell works but doesnt seem quite right it uses a mutex who are typically present for controlling access to shared data In this case there may be no shared data Two other problems If detecting task notifies the condvar before the reacting task waits the reacting task will hang The wait statement fails to account for spurious wakeups meaning the code waiting on a condition variable may be awakened even if the condvar wasnt notified Proper code deals with this by confirming that the condition being waited for has truly occurred Like this cpp cvwaitlk return whether the event has occurred this would require that the reacting task be able to determine whether the condition its waiting for is true The reacting task might not be able to tell its what its waiting for in the first place An alternative is a shared boolean flag and busy wait cpp detecting task stdatomic flagfalse shared flag see Item 40 for stdatomic detect event flag true tell reacting task reacting task prepare to react while flag wait for event react to event This approach does not have the drawbacks of the condition variable approach but it incurs the cost of polling the task is not truly blocked an otherwise idle CPU needs to be occupied Its then common to combine the condvar and flagbased design the bool no longer has to be atomic since its now protected by the mutex cpp detecting task stdconditionvariable cv as before stdmutex m bool flagfalse not stdatomic detect event stdlockguard gm lock m via gs ctor flag true tell reacting task part 1 unlock m via gs dtor cvnotifyone tell reacting task part 2 reacting task prepare to react as before stduniquelock lkm as before cvwaitlk return flag use lambda to avoid spurious wakeups react to event m is locked continue reacting m now unlocked This approach avoids busy wait and deals with spurious wakeups Yet the code smell remains because the detecting task communicates with the reacting task in a curious fashion the flag mutex and cv all work participate in achieving this one thing of signalling and its not terribly clean An alternative is leveraging the communication channel of futures The detecting task has a stdpromise object and the reacting task has a corresponding stdfuture object the reacting task calls wait on the future waiting for the promise to be set and the detecting task sets the promise when a signal is detected Both promise and future templates expect the type of the data to be transmitted the stdfuture and stdpromise thus both use a void type Eg cpp given stdpromise p promise for communications channel the detection code detect event psetvalue tell reacting task the reaction code prepare to react pgetfuturewait wait on future corresponding to p react to event Like the approach using a flag the design requires no mutex would work regardless of whether the detecting task sets its stdpromise before or after the reacting task waits and is immune to spurious wakeups Like the condition var approach the wait is truly blocked instead of busy wait However things to keep in mind item 38 explains the stdfuture and stdpromise share a state allocated on the heap thus we should expect heap allocation and deallocation with this mechanism More importantly a promise can be set only once the communication between detection task and reacting task is oneoff One use case is to create a thread in suspended state Say you want to configure its affinity and priority using nativehandle of a stdthread before running it you could do cpp stdpromise p void react func for reacting task void detect func for detecting task ThreadRAII tr use RAII object stdthread pgetfuturewait react ThreadRAIIDtorActionjoin risky see below thread inside tr is suspended here psetvalue unsuspend thread inside tr There is an issue that if before psetvalue is executed an exception is thrown setvalue is never executed the thread will never be fired as its stuck on wait when dtor is called a join is made on the thread making the dtor hang forever There are many ways to address this issue Instead we give an example of using a sharedfuture to have one detecting task fire off multiple reacting tasks cpp stdpromise p as before void detect now for multiple reacting tasks auto sf pgetfutureshare sfs type is stdsharedfuture stdvector vt container for reacting threads for int i 0 i threadsToRun i vtemplacebacksf sfwait wait on local react copy of sf see Item 42 for info on emplaceback ThreadRAII not used so program is terminated if this code throws psetvalue unsuspend all threads for auto t vt make all threads tjoin unjoinable see Item 2 for info on auto Takeaways For simple event communication condvarbased designs require a superfluous mutex impose constraints on the relative progress of detecting and reacting tasks and require reacting tasks to verify that the event has taken place Designs employing a flag avoid those problems but are based on polling not blocking A condvar and flag can be used together but the resulting communications mechanism is somewhat stilted Using stdpromises and futures dodges these issues but the approach uses heap memory for shared states and its limited to oneshot communication Snippet cpp voidfutureoneshotcommunicationmcpp include include include include include using namespace stdliterals void detect stdcout detect startn stdthisthreadsleepfor1s stdcout detect stopn return void react stdcout react firedn return Condition variable to achieve synchronization void condVar stdconditionvariable cv stdmutex m stdthread tm cv stduniquelock lkm if wait is called after notifyone not possible in this case this will hang forever cvwaitlk react not quite right this misses the check for spurious wakeups detect cvnotifyone better yet use ThreadRAII tjoin Busy wait flag approach to achieve synchronization void busyWaitFlag stdatomic flagfalse stdthread tflag while flag stdthisthreadsleepfor100ms react detect flag true better yet use ThreadRAII tjoin condition variable flag to achieve synchronization void condVarPlusFlag stdconditionvariable cv stdmutex m note that the bool does not need to be atomic bool flagfalse stdthread tm cv flag stduniquelock lkm cvwaitlk flag return flag spurious wakeup detection react detect stdlockguard guardm flag true cvnotifyone better yet use ThreadRAII tjoin void future stdpromise p stdthread tp pgetfuturewait react detect if detect throws the program will crash as t is joinable when this stack unwinds if we switch to using ThreadRAII with DtorActionjoin the thread will hang on wait call forever psetvalue this signaling is oneoff involves heap allocation but does not have the code smell from earlier tjoin a common use case for this could be to instantiate the thread in a blocked state using its nativehandle to configure priority affinity etc then execute a setvalue and start the thread int main condVar busyWaitFlag condVarPlusFlag future return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it4","content":" Know how to view deduced types Some IDE tells type Compiler diagnostics Could rely on dummy code like this trigger a compiler error to make it tell the types cpp To view the types of x and y template declaration only for TD class TD TD Type Displayer TD xType elicit errors containing TD yType xs and ys types At runtime cpp stdcout typeidxname n display types for x This approach relies on the fact that invoking typeid on an object such as x yields a stdtypeinfo object and stdtypeinfo has a member function name that produces a Cstyle string ie a const char representation of the name of the type It may show something like a PKi pointer to const integer this display can be demangled stdtypeinfoname could be incorrect because the specification for stdtypeinfoname mandates that the type be treated as if it had been passed to a template function as a byvalue parameter Where IDE and stdtypeinfoname could be wrong BoostTypeIndex is designed to be correct Takeaways Deduced types can often be seen using IDE editors compiler error messages and the Boost TypeIndex library The results of some tools may be neither helpful nor accurate so an understanding of Cs type deduction rules remains essential Snippet cpp viewdeducedtypesmcpp include include include include demonstrates different ways to check deduced types at compiler run time test 1 class Widget public int x stdvector createVec factory method return stdvector to trigger a compiler error to display type template declaration only for TD class TD TD Type Displayer template void fconst T param stdcout T typeidTname n show T stdcout param typeidparamname n show params type at runtime both are reported as PK6Widget pointer to const Widget characterlength6 const Widget which could be incorrect as it should be const const Widget TD p1 TD p1 TD spits the right type const Widget const using boosttypeindextypeidwithcvr show T stdcout T typeidwithcvrprettyname n show params type stdcout param typeidwithcvrprettyname n at runtime boosttypeindex spits the right types int main const auto vw createVec init vw wfactory return fvw0 call f return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it40","content":" Use stdatomic for concurrency volatile for special memory Volatile is often thought to have something to do with multithreading The C feature people often confuse volatile with is stdatomic which guarantees the the operation on the object being seen as atomic by other threads Once a stdatomic object has been constructed operations on it behave more or less as if they were inside a mutexprotected critical section but the operations are generally implemented using special machine instructions that are more efficient than would be the case if a mutex were employed Consider this code cpp stdatomic ai0 initialize ai to 0 ai 10 atomically set ai to 10 stdcout ai atomically read ais value ai atomically increment ai to 11 ai atomically decrement ai to 10 Assuming this is the only thread modifying ai other threads reading ai can only see values of 0 10 11 Aspects worth noting 1 stdcout ai only guarantees the read of ai is atomic Between the time of read and the time of stdcout copies the read value to be printed ais value may have changed 2 the readmodifywrite operations are also atomic In contrast this code with volatile cpp volatile int vi0 initialize vi to 0 vi 10 set vi to 10 stdcout vi read vis value vi increment vi to 11 vi decrement vi to 10 The volatile guarantees nothing This code has UB because these statements modify ai so if other threads are reading ai at the same time there are simultaneous readers and writers of a memory block and thats the definition of a data race what about one writer and multiple readers at the some time In addition consider this code cpp stdatomic ac0 atomic counter volatile int vc0 volatile counter we then increment both values in two threads Thread 1 Thread 2 ac ac vc vc When both have finished ac is guaranteed to be 2 vc is not it can be 1 but its final value is in general unpredictable because vc is involved in a data race and the Standards decree that data races cause undefined behavior means that the code generated by compilers may end up doing literally anything Compiler optimizations can often cause the result to be unpredictable As another example consider this code cpp stdatomic valAvailablefalse auto imptValue computeImportantValue compute value valAvailable true tell other task its available As humans we know setting imptValue before valAvailable is crucial however all compiler sees is a pair of irrelevant variables and as a general rule a compiler is allowed to reorder the such unrelated assignments Even if the compiler does not reorder the underlying hardware might However the use of stdatomic imposes restrictions on how code can be reordered and one such restriction is no code preceding the write of a stdatomic object can be ordered to after the write As a result only by declaring valAvailable atomic can we guarantee that the desired order of execution is guaranteed volatile does not make such order guarantees cpp volatile bool valAvailablefalse auto imptValue computeImportantValue valAvailable true other threads might see this assignment before the one to imptValue Volatile only means dealing with special blocks of memory where an ordinary one would have its value remain until something overwrites it Say you have a normal int cpp int x auto y x read x y x read x again compiler can optimize out the assignment to y because its redundant with ys initialization redundant loads similarly with normal memory if you write and never read again before another write the first write can be optimized out dead stores x 10 write x x 20 write x again After compiler optimizations such code does show up and compiler carries out redundant loads and dead stores optimization only for normal memory but not volatile memory whose one useful case being memory mapped IO where the value of the particular memory location does not matter for your program it may matter for other IO control programs volatile tells compiler to not perform any optimizations on this block of memory So cpp volatile int x auto y x read x y x read x again cant be optimized away x 10 write x cant be optimized away x 20 write x again and you can see stdatomic does not do the same job stdatomic x auto y x conceptually read x see below y x conceptually read x again see below x 10 write x x 20 write x again the atomic version can be optimized to auto y x conceptually read x see below x 20 write x in fact neither of the y assignment will compile because stdatomic is not copiable why not imagine this auto y x y needs to be an atomic as well and thus compiler has to generate code that reads x and writes y in one single atomic operation and hardware typically does not support such stdatomic offers neither copy support nor move support so what can you do if you want to copy the value of one atomic to another try this stdatomic yxload read x ystorexload read x again both store and load are atomic but them being different function calls suggests the two combined doesnt have to be atomic neither in the ctor nor the 2nd call compiler seeing the above code could optimize it to register xload read x into register stdatomic yregister init y with register value ystoreregister store register value into y this kind of optimization causes x to be read only once and this is the kind of optimization that must be avoided when dealing with special memory to avoid this you could see volatile stdatomic vai operations on vai are atomic and cant be optimized away This could be useful if vai corresponded to a memorymapped IO location that was concurrently accessed by multiple threads As a side note some folks prefer load and store calls on atomics to make it clear the vars being read written are special or meant to be special Takeaways stdatomic is for data accessed from multiple threads without using mutexes Its a tool for writing concurrent software volatile is for memory where reads and writes should not be optimized away Its a tool for working with special memory Snippet cpp atomicvsvolatilemcpp include include include include using namespace stdliterals int main stdatomic ai0 volatile int vi 0 stdthread t1ai vi ai vi stdthread t2ai vi ai vi volatile has nothing to do with concurrency t1join t2join stdcout atomic aiload nvolatile vi n volatile does prevent optimization of the following states volatile int vi1 vi vi1 vi will not be optimized out vi1 10 will not be optimized out vi1 20 why does this matter Not in this process but if another process shares this memory or has memory mapped IO then this kind of optimization leads to wrong behaviors return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it41","content":" Consider passbyvalue for copyable parameters that are cheap to move and always copied Some function parameters are intended to be copied Eg cpp class Widget public void addNameconst stdstring newName take lvalue namespushbacknewName copy it void addNamestdstring newName take rvalue namespushbackstdmovenewName move it see Item 25 for use of stdmove private stdvector names This works but it requires two functions to declare and implement two copies to maintain and two copies of binary code if not inlined by the compiler To get rid of two identical copies an alternative is to use a universal reference cpp class Widget public template take lvalues void addNameT newName and rvalues copy lvalues namespushbackstdforwardnewName move rvalues see Item 25 for use of stdforward As a template addNames impl typically needs to be in header file and it yields several functions in the object code since its not just rvalue and lvalue versions but also instantiates differently for types that are convertible to stdstring Also this cant handle clients passing argument types that cant be passed by universal references item 30 and if client passes improper types the error message can be daunting item 27 Itd be nice to write one copy of addName have one copy of addName in generated code without using universal references and does copy for lvalue and move for rvalues The solution could be cpp class Widget public void addNamestdstring newName take lvalue or namespushbackstdmovenewName rvalue move it Typically move is used with rvalue references but in this case we know newName is independent of whats passed in so changing it doesnt matter and this is the final use of newName What about efficiency This goes against the fundamental recommendation of passing user defined types by const reference In C98 this will likely be slow no matter what the user passes in a copy ctor is invoked In C11 newName is copy ctored only for lvalues and move ctored for rvalues cpp Widget w stdstring nameBart waddNamename call addName with lvalue copy ctored waddNamename Jenne call addName with rvalue move ctored This achieves just the desired effect of copy if lvalue move if rvalue We then analyze the three approaches in the rvalue and lvalue references approach its one copy for lvalues and one move for rvalues in the universal reference approach its one copy for lvalues and one move for rvalues if users pass in types that can be converted to stdstring it can be as few as 0 copies moves due to code generated for that particular type item 25 in the pass by value approach the parameter is copied anyway so its one copy one move for lvalues and two moves for rvalues Its one more move than the first two approaches Thus the title of this item you should consider passing by value for writing only one copy of the function generating one copy of object code but incurs additional cost of one move consider pass by value only for copiable parameters In the moveonly case the first approach would only contain one signature taking in rvalue references in the first place Eg a stduniqueptr cpp class Widget public void setPtrstduniqueptr ptr p stdmoveptr private stduniqueptr p a caller might use it this way Widget w wsetPtrstdmakeuniqueModern C here the total cost is one move if Widget setPtr was defined instead as class Widget public void setPtrstduniqueptr ptr p stdmoveptr the cost would be move ctoring ptr then moving it to p thus the total cost of two moves Pass by value in this case is only worth considering for arguments that are cheap to move according to the above analysis an extra move is incurred in the pass by value approach Consider pass by value for parameters that are always copied Consider this cpp class Widget public void addNamestdstring newName if newNamelength minLen newNamelength maxLen namespushbackstdmovenewName private stdvector names If an lvalue is given the function copies moves anyway even if sometimes names do not have this copy moved in Even when youre dealing with a function performing an unconditional copy on a copyable type thats cheap to move there are times when pass by value may not be appropriate Thats because a function can copy a parameter in two ways via construction ie copy construction or move construction and via assignment ie copy assignment or move assignment addName uses copy move ctor thus the analysis we saw above If it uses assignment instead consider this code cpp class Password public explicit Passwordstdstring pwd pass by value textstdmovepwd construct text void changeTostdstring newPwd pass by value text stdmovenewPwd assign text private stdstring text text of password this is all good as analyzed above one extra move ctor is all stdstring initPwdSupercalifragilisticexpialidocious Password pinitPwd this may cause the function to explode in cost stdstring newPassword Beware the Jabberwock pchangeTonewPassword reasoning being one string copy ctor and dynamic allocation is invoked and one dtor is invoked to deallocate the current string in text two dynamic allocations if instead we have the by reference approach void changeToconst stdstring newPwd the overload for lvalues text newPwd can reuse texts memory if textcapacity newPwdsize no dynamic allocation is involved if textcapacity newPwdsize if not its typically not possible to work around the two dynamic allocations this kind of analysis applies to types that involves dynamic allocation like stdstring and stdvector whether the impl uses small string optimization also matters for the analysis And there are other issues with passing by reference what if you have a chain of calls and many functions like addName and each of them incurs an extra move What if you have a function that is designed to accept a parameter of a base class type or any type derived from it passing by value would cause slicing off Usually the most practical approach is to adopt a guilty until proven innocent policy whereby you use overloading or universal references instead of pass by value unless its been demonstrated that pass by value yields acceptably efficient code for the parameter type you need Takeaways For copyable cheaptomove parameters that are always copied pass by value may be nearly as efficient as pass by reference its easier to implement and it can generate less object code For lvalue arguments pass by value ie copy construction followed by move assignment may be significantly more expensive than pass by reference followed by copy assignment Pass by value is subject to the slicing problem so its typically inappropriate for base class parameter types Snippet cpp passbyvaluemcpp include include include using namespace stdliterals class Widget public void addNamebyValuestdstring value mvecpushbackstdmovevalue template void addNameuniversalRefT value would using T here be the same as using decltypevalue what happens with stdforward mvecpushbackstdforwardvalue void addNamerefconst stdstring value mvecpushbackvalue void addNamerefstdstring value mvecpushbackstdmovevalue private stdvector mvec int main Widget w stdstring strbad stuff 2 moves waddNamebyValuegood stuff 1 copy 1 move waddNamebyValuestr 1 move waddNamerefgood stuff 1 copy waddNamerefstr 1 move const char is forwarded the ctor the string to be then moved into the vector if emplace is used this would be 0 copy move waddNameuniversalRefgood stuff 1 copy waddNameuniversalRefstr return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it42","content":" Consider emplacement instead of insertion When you have a container holding stdstring it seems logical when you add an element to it you add via insertion functions insert pushfront pushback insertafter and the type youd pass in is stdstring Consider this code though cpp stdvector vs container of stdstring vspushbackxyzzy add string literal pushback is overloaded as following cpp template class T from the C11 class Allocator allocator Standard class vector public void pushbackconst T x insert lvalue void pushbackT x insert rvalue why is just having an universal reference version not enough In the call of pushback with string literal compiler sees a type mismatch const char6 with what vectors instantiated with stdstring thus what it does becomes the following create a temporary cpp vspushbackstdstringxyzzy create temp stdstring and pass it to pushback here stdstring ctor is called and then the rvalue reference is given to pushback to be move ctored to the memory of the new object inside stdvector immediately after pushback returns the temp object is destroyed by calling stdstring dtor Here the ctor and dtor of the temporary stdstring object is avoidable as we construct the new stdstring object to be held inside the stdvector we could give it the string literal And emplaceback does exactly that cpp vsemplacebackxyzzy construct stdstring inside vs directly from xyzzy no temporary involved emplaceback uses perfect forwarding so it would have the failure unexpected cases with perfect forwarding cpp vsemplaceback50 x insert stdstring consisting of 50 x characters Every std container that supports insert supports a corresponding emplace as well Insertion functions take objects to be inserted while emplacement functions take constructor arguments for objects to be inserted This difference permits emplacement functions to avoid the creation and destruction of temporary objects that insertion functions can necessitate emplacement function can be used even when an insertion function would require no temporary cpp stdstring queenOfDiscoDonna Summer these result in the same vspushbackqueenOfDisco copyconstruct queenOfDisco at end of vs vsemplacebackqueenOfDisco ditto Why not always use emplacement then Because in current std library there are scenarios where insertion is faster than emplacement Such scenarios are hard to categorize but emplacement will almost certainly outperform insertion if all the following are true The value being added is constructed into the container not assigned Consider this cpp stdvector vs as before add elements to vs vsemplacevsbegin xyzzy add xyzzy to beginning of vs The impl likely uses move assignment underneath in which case a temporary object will need to be created to serve as the source of move Emplacements edge would then disappear node based std containers always use ctored elements instead of assigned the rest stdvector stdstring stddeque stdarray which is irrelevant you can rely on emplaceback and emplacefront as well to use ctor The argument types being passed differ from the type held by the container If they are the same insert would not create the temporary either The container is unlikely to reject the new value as a duplicate Reason for this is that in order to detect if a value is already in the container emplacement typically creates the node with the new value first so that it can compare the value with the rest Two other issues worth considering when using emplacement Suppose you have a container of stdsharedptrs cpp stdliststdsharedptr ptrs say you have a custom deleter void killWidgetWidget pWidget and you want to insert shared pointers with a custom deleter ptrspushbackstdsharedptrnew Widget killWidget and it could look like ptrspushback new Widget killWidget note that although recommended you cant use makeshared here since a custom deleter is desired With the pushback approach a temporary will be created Emplacement would avoid creating this temporary but in this case the temporary is desirable say during the allocation of the node in the list container an outofmemory exception is thrown then as the exception propagates out the temp object will be freed and being the sole shared pointer referring to Widget object Widget will be deallocated by calling killWidget Nothing leaks Now consider the emplacement version cpp ptrsemplacebacknew Widget killWidget youd be calling something like this underneath stdlistnodestdsharedptrnew Widget killWidget think of the above as template class Node template NodePs params somethingotherdataxxx Tstdforwardparams what if it throws after new Widget is constructed but before T can be done allocated say some other member ctor throws an outofmemory As explained below the new Widget leaks The raw pointer resulting from new Widget is perfect forwarded to node ctor and if that ctor throws an exception as the exception propagates out there is no handle to the heap allocated Widget any more It is leaked Similarly with stduniqueptr with a custom deleter Fundamentally the effectiveness of memory management classes like stduniqueptr and stdsharedptr is predicated on resources such as raw pointers from new being immediately passed to ctors for resource managing objects The fact that makeshared and makeunique automate this is one of the reasons why they are important In cases like this you need to ensure yourself you are not paying for potentially improved performance with diminished exception safety The emplacement insert versions should then look more like this cpp insert version stdsharedptr spwnew Widget create Widget and killWidget have spw manage it ptrspushbackstdmovespw add spw as rvalue emplacement version stdsharedptr spwnew Widget killWidget ptrsemplacebackstdmovespw in which case emplacement wont outperform insert since spw is essentially the temporary now The other case is with explicit ctors Say you wrote this by mistake cpp using C11s support for regex stdvector regexes you wrote this nullptr by mistake regexesemplacebacknullptr add nullptr to container of regexes once compiled this would be UB compiler does not reject this even though stdregex r nullptr error wont compile or regexespushbacknullptr error wont compile This behavior stems from the fact that stdregex can be ctored from character strings const char like cpp stdregex upperCaseWordAZ And this ctor taking a const char is explicit thus the following cpp stdregex r nullptr error wont compile regexespushbacknullptr error wont compile stdregex rnullptr compiles this is what emplacement would translate to calling this explicit ctor directly In official terminologies cpp stdregex r1 nullptr error wont compile called copy initialization not eligible to use explicit ctor stdregex r2nullptr compiles called direct initialization eligible to use explicit ctor and thus regexesemplacebacknullptr compiles Direct init permits use of explicit stdregex ctor taking a pointer regexespushbacknullptr error copy init forbids use of that ctor Takeaways In principle emplacement functions should sometimes be more efficient than their insertion counterparts and they should never be less efficient In practice theyre most likely to be faster when 1 the value being added is constructed into the container not assigned 2 the argument types passed differ from the type held by the container and 3 the container wont reject the value being added due to it being a duplicate Emplacement functions may perform type conversions that would be rejected by insertion functions Snippet cpp emplacementinsteadofinsertionmcpp include include include include using namespace stdliterals void deleteMyIntint ptr delete ptr int main stdvector vecString less efficient vecStringpushbackbad stuff more efficient vecStringemplacebackgood stuff stdstring strneutral stuff equally efficient vecStringpushbackstr vecStringemplacebackstr stdvector vecRegexes does not compile phew vecRegexespushbacknullptr compiles leads to UB crashes this program vecRegexesemplacebacknullptr stdvectorstdsharedptr vecSharePtrInt more exception safe vecSharePtrIntpushbacknew int3 deleteMyInt less exception safe vecSharePtrIntemplacebacknew int3 deleteMyInt return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it5","content":" Prefer auto to explicit type declarations Upsides Avoids variable uninitalized issue auto variables have their type deduced from the initializer so they must be initialized Avoids verbose variable declarations Can hold a closure in lieu of stdfunction Avoids potential implicit conversion you dont want like sizetype to int and the pairs in unorderedmap who are actually pair instead of pair unnecessary copycon calls Potentially can make refactoring easier stdfunction is a template in C11 that generalizes the idea of a function pointer who can only point to functions a stdfunction can refer to any callable object You specify the type signature of function you refer to when creating a stdfunction object Since lambda expressions yield callable objects closures can be stored in stdfunction objects stdfunction object comes at a fixed size and if not big enough heap allocate additional memory typically uses more memory than the autodeclared object takes as much space as the closure requires invoking a closure via a stdfunction object is almost certain to be slower than calling it via an autodeclared object implementation details that restrict inlining and yield indirect function calls Downsides auto may deduce unexpected types Eg item2it14deducingtypesmdunderstandautotypededuction cpp int x 5 auto y x code readability The counter argument would be that type inference is sufficiently understood and IDE can always help Takeaways auto variables must be initialized are generally immune to type mismatches that can lead to portability or efficiency problems can ease the process of refactoring and typically require less typing than variables with explicitly specified types autotyped variables are subject to the pitfalls described in Items 2 and 6 Snippet cpp preferautomcpp include include include include using namespace std demonstrates small missteps in declaring a type by hand pair of a map could lead to inefficiency in code whereas auto avoids such issues test 1 class KeyClass public explicit KeyClass stdcout ctor1n explicit KeyClassint x dxx stdcout ctor2n KeyClassconst KeyClass rhs stdcout copyconn bool operatorconst KeyClass rhs const return dx rhsx int x const return dx private int dx namespace std template struct hash stdsizet operatorconst KeyClass k const return hashkx int main stdunorderedmap map mapinsertmakepair3 4 mapinsertmakepair4 5 mapinsertmakepair5 6 note how copycon is called 3 times here also explicit on copycon will cause compiler error for const stdpair p map p here is a compiler constructed temporary if you get its address its not gonna pointer to an element in map cout elem psecond n note how copycon is not called here since p is stdpair for auto p map cout type of p boosttypeindextypeidwithcvr n cout elem psecond n cout type of map iterator boosttypeindextypeidwithcvrstdunorderedmapiterator n cout type of map iterator boosttypeindextypeidwithcvr n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it6","content":" Use the explicitly typed initializer idiom when auto deduces undesired types As a general rule invisible proxy classes dont play well with auto Objects of such classes are often not designed to live longer than a single statement so creating variables of those types tends to violate fundamental library design assumptions For example consider this code using vector cpp Widget w bool highPriority featuresw5 is w high priority features call returns a vector processWidgetw highPriority process w in accord with its priority Works just fine Yet if we do cpp Widget w auto highPriority featuresw5 processWidgetw highPriority undefined behavior Reason is in the type of highPriority its now stdvectorreference stdvector operator returns a T except in the case of vector where it returns a vectorreference that can be implicitly casted to bool Reason for having this reference is that the underlying uses a compact bitstorage for vector You cant return a reference to a bit in C so a proxy class reference is used instead So what happened instead is highPriority will be a reference to a temporary and when we use it in processWidget the temporary is already destroyed and it causes undefined behavior Solution is to use explicitly typed initializer Like cpp auto highPriority staticcastfeaturesw5 So when do we use explicitly typed initializer with auto when auto will look at invisible proxy types another common example is a Matrix class where the sum of 3 matrices is of type SumSum Matrix for performance reasons and Sum is a proxy class within Matrix when you intend to do a conversion eg double float Takeaways Invisible proxy types can cause auto to deduce the wrong type for an initializing expression The explicitly typed initializer idiom forces auto to deduce the type you want it to have Snippet cpp explicitlytypedinitializermcpp include include include include using namespace std demonstrate a case working with proxy classes vector where auto might cause undefined behavior while bool is fine vector buildVector vector bs bspushbackfalse bspushbackfalse bspushbackfalse bspushbackfalse bspushbackfalse bspushbacktrue return bs int main auto highPriority buildVector5 cout type boosttypeindextypeidwithcvr n if highPriority undefined behavior reference to deleted temporary cout high priorityn else cout low priorityn instead do explicitly typed initializer idiom auto highPriorityGood staticcastbuildVector5 if highPriorityGood all good cout high priorityn else cout low priorityn return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it7","content":" Item 7 distinguish between and when creating objects Different ways of initializing cpp int x0 initializer is in parentheses int y 0 initializer follows int z 0 initializer is in braces int z 0 initializer uses and braces generally the same as braces First differentiate assignment operator and copy constructor cpp Widget w1 call default constructor Widget w2 w1 not an assignment calls copy ctor w1 w2 an assignment calls copy operator C11 introduces the concept of uniform initialization using braces cpp stdvector v 1 3 5 vs initial content is 1 3 5 not possible in c03 class Widget private int x 0 fine xs default value is 0 int y 0 also fine int z0 error Uncopiable objects can use but not to initialize stdatomic ai1 0 fine stdatomic ai20 fine stdatomic ai3 0 error braced initialization prohibits implicit narrowing conversions among builtin types double x y z int sum1 x y z error sum of doubles may not be expressible as int int sum2x y z okay value of expression truncated to an int int sum3 x y z ditto C parse says anything that can be interpretted as a declaration must be interpretted as one thus Widget w110 call Widget ctor with argument 10 Widget w2 most vexing parse declares a function named w2 that returns a Widget Widget w3 calls Widget ctor with no args Why not always use braced initialization then There can be unexpected behaviors due to the tangled relationship among initializers stdintializerlists and constructor overload resolution Eg in item 2it14deducingtypesmdunderstandautotypededuction deduced type for auto using braced initialization is stdinitializerlist The more you like auto the less you may like braced initialization cpp Braced initialization always prefers a constructor overload that takes in stdinitializationlist and what would normally be copy and move construction can be hijacked by stdinitializationlist ctors class Widget public Widgetint i bool b Widgetint i double d Widgetstdinitializerlist il operator float const convert Widget w110 true uses parens and as before calls first ctor Widget w210 true uses braces but now calls stdinitializerlist ctor 10 and true convert to long double Widget w310 50 uses parens and as before calls second ctor Widget w410 50 uses braces but now calls stdinitializerlist ctor 10 and 50 convert to long double Widget w5w4 uses parens calls copy ctor Widget w6w4 uses braces calls stdinitializerlist ctor w4 converts to float and float converts to long double Widget w7stdmovew4 uses parens calls move ctor Widget w8stdmovew4 uses braces calls stdinitializerlist ctor for same reason as w6 Compiler will even block calling other matched ctors if stdinitializerlist ctor exists braced initialization but the braced initialization requires narrowing conversions no conversions is fine And if you have a default ctor and a initializerlist ctor Widget w1 calls default ctor Widget w2 also calls default ctor Widget w3 most vexing parse declares a function Widget w4 calls stdinitializerlist ctor with empty list Why does this matter Consider stdvector it has a length value ctor and a stdinitializerlist ctor And the difference could be such cpp stdvector v110 20 use nonstdinitializerlist ctor create 10element stdvector all elements have value of 20 stdvector v210 20 use stdinitializerlist ctor create 2element stdvector element values are 10 and 20 That means as a class designer dont do what stdvector does Its best to design your constructors so that the overload called isnt affected by whether clients use parentheses or braces And if you need to add a stdinitializerlist ctor do so with great deliberation The second lesson is that as a class client you must choose carefully between parentheses and braces when creating objects Takeaway Braced initialization is the most widely usable initialization syntax it prevents narrowing conversions and its immune to Cs most vexing parse During constructor overload resolution braced initializers are matched to stdinitializerlist parameters if at all possible even if other constructors offer seemingly better matches An example of where the choice between parentheses and braces can make a significant difference is creating a stdvector with two arguments Choosing between parentheses and braces for object creation inside templates can be challenging One could do braces only when necessary or instead always do but understand when semantically is desirable Snippet cpp bracedinitializationmcpp include include include using namespace std demonstrates initialization always picks the ctor taking stdinitializerlist when multiple matches are present This can be undesirable as a class designer be careful about providing a ctor taking stdinitializerlist Anything using initialization will be matched class Widget public Widgetlong double a bool b cout ctor1n Widgetstdinitializerlist list cout ctor initializerlistn private int main Widget w1120 true Widget w2120 true long double x 0 Widget w3x true compiler would complain about long double narrowing as opposed to trying ctor1 stdinitializerlist ctor in action dont make the same mistake as a class designer And be careful as a client stdvector vec11 2 stdvector vec21 2 cout n for auto p vec1 cout vec1 p n cout n for auto p vec2 cout vec2 p n return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it8","content":" Item 8 prefer nullptr to 0 and NULL Neither 0 or NULL has a pointer type In C98 the primary implication of this was overloads on pointer and integral types could be surprising cpp void fint three overloads of f void fbool void fvoid f0 calls fint not fvoid fNULL might not compile if NULL is 0L since the conversion from long to int to bool is equally good but typically calls fint if NULL is 0 Never calls fvoid fnullptr calls fvoid overload nullptr does not have an integral type It does not suffer from overload resolution surprises that 0 and NULL are susceptible to It doesnt have a pointer type either but you can think of it as a pointer of all types Its actual type is stdnullptrt which implicitly converts to all pointer types nullptr improves code clarity Consider cpp auto result findRecord arguments if result nullptr vs auto result findRecord arguments if result 0 nullptr shines even more in template specialization Takeaways Prefer nullptr to 0 and NULL Avoid overloading on integral and pointer types Snippet cpp nullptrmcpp include include include using namespace std demonstrates how nullptr is a pointer type while neither 0 nor NULL is a pointer type int f1stdsharedptr spw return 0 call these only when double f2stduniqueptr upw return 00 the appropriate bool f3int pw return true mutex is locked template auto lockAndCallFuncType func MuxType mutex PtrType ptr decltypefuncptr C11 in 14 do decltypeauto using MuxGuard stdlockguard MuxGuard gmutex return funcptr int main stdmutex fm auto result1 lockAndCallf1 fm 0 error no known conversion from int to sharedptr auto result2 lockAndCallf2 fm NULL error no known conversion from long to uniqueptr auto result3 lockAndCallf3 fm nullptr fine auto result4 lockAndCallf2 fm nullptr fine auto result5 lockAndCallf1 fm nullptr fine return 0 "},{"title":"unprocessed","href":"/effectives/emcpp/it9","content":" Item 9 prefer alias declarations to typedefs Alias is easier to swallow when dealing with types involving function pointers cpp FP is a synonym for a pointer to a function taking an int and a const stdstring and returning nothing typedef void FPint const stdstring typedef same meaning as above using FP void int const stdstring alias declaration A more compelling reason to use alias is that they work with having templates inside cpp alias C11 template MyAllocList using MyAllocList stdlistT MyAlloc is synonym for stdlistT MyAlloc MyAllocList lw client code And to use MyAllocList within another template template class Widget private MyAllocList list compared with below no typename no type typedef C03 template MyAllocListtype struct MyAllocList is synonym for typedef stdlistT MyAlloc type stdlistT MyAlloc MyAllocListtype lw client code It gets worse If you use typedef inside a template for the purpose of creating a linked list holding objects of a type specified by a template parameter you have to precede the typedef name with typename because MyAllocListtype is now a dependent type nested dependent name Compiler doesnt know for sure MyAllocListtype is a type There might be a specialization of MyAllocList somewhere that has type not as a type but as a data member Compiler doesnt know for sure template class Widget Widget contains private a MyAllocList typename MyAllocListtype list as a data member C11 has type traits to perform type transformation in C11 its implemented with enclosing structs C03 typedef and in C14 its done with alias So we have cpp stdremoveconsttype C11 const T T stdremoveconstt C14 equivalent stdremovereferencetype C11 TT T stdremovereferencet C14 equivalent stdaddlvaluereferencetype C11 T T stdaddlvaluereferencet C14 equivalent Takeaways typedefs dont support templatization but alias declarations do Alias templates avoid the type suffix and in templates the typename prefix often required to refer to typedefs C14 offers alias templates for all the C11 type traits transformations Snippet cpp aliasmcpp include include include demonstrates typedefs cant work with templates while using aliases can template using MyList stdlist template typedef stdlist MyListType compiler error a typedef cannot be a template int main MyList list1 MyList list2 return 0 "},{"title":"unprocessed","href":"/effectives/estl/it1","content":" Know your options STL sequence containers vector string deque list STL associative containers set multiset map multimap non standard sequence containers slist singly linked list rope a heavyduty string Hash based associative containers unorderedset unorderedmap unorderedmultiset unorderedmultimap Standard nonSTL containers array bitset valarray stack queue priorityqueue algorithmic complexity contiguousmemory containers vector string deque rope stores multiple elements per chunk of dynamically allocated memory random insertion and deletion will have to cause shifts vs nodebased containers list slist associative containers and hash based ones to a degree stores a single element per chunk of dynamically allocated memory element values need not be moved around when inserting erasing Things to consider do you need to be able to randomly insert If so you need a sequence container associative containers wont do do you care about element ordering If so hashbased containers wont do must the container be part of standard what category of iterator do you require If random access iterator required vector deque string rope If bidirectional required avoid slist and one common implementation of hash based containers is it important to avoid shifting elements when insertion erasure takes place If so contiguous memory containers wont do does the data layout need to be compatible with C If so you are limited to vector is look up speed critical If so consider hash sorted vectors and standard associative containers probably in that order do you mind if underlying container uses reference counting If so youll want to steer clear of string because many string implementations use reference counting Also steer clear of rope To represent your string consider a vector do you need transactional semantics for insertions and erasures do you require the ability to reliably roll back insertions and erasures If so use a node based container If you need transactional semantics for multipleelement insertions the range form use list as its the only one from standard that offers transactional semantics for multipleelement insertions Transactional semantics are particularly important for exceptionsafe code do you need to minimize iterator pointer and reference invalidation If so use nodebased containers because insertions and erasures on such containers never invalidate iterators pointers or references unless they point to an element you are erasing In general insertions and erasures on contiguousmemory containers may invalidate all iterators pointers and references into that container would it be helpful to have a sequence container with random access iterators where pointers and references to the data are not invalidated as long as nothing is erased and insertions take place only at the ends of the container deque works for this exact case These questions are hardly the end of the matter Eg this doesnt take into account different memory allocation strategies of different container types Takeaways Container choice should not only base on algorithmic complexity Memory access pattern ordering iteration support layout transactional semantic etc are often important as well Know your options Contiguous vs nodebased Sequence vs associative Hashbased associative vs orderbased associative"},{"title":"Gallery","href":"/gallery/gallery","content":""},{"title":"\"Bloomberg by bloomberg - memorable quotes\"","href":"/notes/bloomberg-by-bloomberg","content":"The more you work the better you do Its that simple The more you try to do the more life youll have Life Ive found works the following way daily you are presented with many small and surprising opportunities Sometimes you seize one that takes you to the top Most though if valuable at all take you only a little way To succeed you must string together many small incremental advances rather than count on hitting the lottery jackpot once Constantly enhance your skills put in as many hours as possible and make tactical plans for the next few steps Then based on what actually occurs look one more move ahead and adjust the plan Take lots of chances and make lots of individual spurofthemoment decisions Planning has its place the actual thought process sometimes leads to great new ideas But you can only accomplish whats possible when you get there Stay flexible often what we accomplished isnt what we set out to do Then whatever your idea is youve got to do more of it than anyone else a task that is easier if you structure your things such that you like doing them Start with a small piece fulfill one goal at a time on time Do it with all things in life Sit down and learn onesyllable words You cant jump to the end right away in computers politics love or any other aspects of life Outsiders do only whats asked at best insiders do whats needed While you are writing this they are out to get us In life unlike in childrens games second place is first loser We simply throw everyone interested into the deep end of the pool as it were and stand back It becomes obvious very quickly who the best swimmers are We just watch who people go to for help and advice If you really want to do it there is a way I long ago declared that we would never rehire anyone who quit for other than family reasons Be honest work hard treat each other fairly and openly Add a dash of competence and well be together for a long time Any supplier who offers today what it sold yesterday will be out of business tomorrow Thinking and interpersonal skills have been are and will be keys to survival Lifes a compromise Will the satisfaction derived outweigh the sacrifices required Of course I never look back Do things now while you have the time so you dont have to later when you are rushed Consistency in thought and conduct Think Prepare Plan in advance when theres no time pressure Then in real life do what you said Three things usually separate the winners from the losers over the long term time invested interpersonal skills and plain old fashioned luck The rewards almost always go to those who outwork the others Youve got to come in early stay late lunch at your desk take projects home and weekends If you put in the time you arent guaranteed success But if you dont Im reasonably sure of the results"},{"title":"\"Classical mythology\"","href":"/notes/classical-mythology","content":" Genesis Hesiod Theogony Hesiods other major work Work and Days is much like a farming manual Chaos Gaia Tartarus Eros Erebus Hemera Nyx Uranus Gaia Cronus Rhea Zeus Poseidon Hades Hestia Hera and Demeter Hestia home hearth is rarely mentioned in literature almost negligible in mythology but presumably very important in religion Zeus Hera Ares whom everyone hates Enyo Hebe Eileithyia childbirth Eris discord Hephaestus Hera by herself Zeus Metis wisdom Athena from Zeuss head Zeus swallows Metis who gave birth to Athena inside Zeus Metiss son is prophecied to overthrow Zeus One of the rare cases where gods mortals cheat fate Greeks believed thoughts wisdom comes from the torso and Metis gave counsel to Zeus from inside his belly The Titan Prometheus In mens meeting with gods at Mecone to decide sacrifice Prometheus tried to trick Zeus into picking the pile of bones covered by meat and fat as sacrifice and leaving the pile of meat covered by bones and unedible parts to human Zeus knew and decided to punish Prometheus by punishing men and took fire from them Prometheus later stole fire and gave it back to men for which act he was chained to a pillar and his liver grew everyday to be eaten by an eagle Zeus punished men too for Prometheus stealing fire by giving men the first woman named Pandora in Work and Days Heracles rescued Prometheus from the chain Greek gods have little care for human beings While these are from Theogony many anecdotes stories we hear today are actually from Ovids Metamorphoses a collection of stories about transformation eg Apollo and the nymph Daphne the story of laurel Phaethon son of Apollo asking to ride the chariot of the sun for a day and Narcissus falling in love in his own reflection in water Demeter Persephone the Eleusinian Mysteries seasons and Greek view of life and death Persephones abduction by Hades and splitting her time in the underworld vs Mount Olympus resulted in the seasons where one third of the year crops dont grow As told by the Homeric Hymn on Demeter Homeric in time and style but not by Homer during Demeters search for Persephone she visited Eleusis disguised as a crone and babysitted the queens son whom she intended to bring to immortality by anointing and throwing into the fire The plan was foiled by the queen who happened upon the ritual angering Demeter and causing the death of the prince The Eleusinian Mysteries were a cult ritual dedicated to Demeter in which only the initiated were told of the rituals Our view of what the ritual is about preserved from those of the potentially biased early Christian authors Presumably symbolisms of life and death While the Greek view of afterlife is in general pretty grim most men became shadows images in the grim world of Tartarus except extraordinary souls or terrible sinners eg Tantalus who intended to trick the gods into eating flesh by serving his own son whose shoulder only Demeter ate mistakenly resulting in an ivory shoulder when the gods bring the son back the Eleusinian Mysteries might have offered a brighter view Note that the general grim view differs from traditional belief of Christianity or Islam in that the latter believes the afterlife the eternal is the important part and the deeds of this life would affect that of the afterlife while the former places its emphasis on this life and rarely afterdeath consequences of ones deeds in this life Orphism is another cult dealing with life and death The teaching of the mortal Orpheus who surprisingly some sources claim is the child of Apollo and a muse while in the underworld trying to bring back his dead wife Eurydice Persuaded by his superb music Hades and Persephone gave their consent on the condition that Orpheus does not look back when leading Eurydice out of the underworld which he failed to do Reincarnation might have been a central concept in Orphism which took a view like that of Buddhism that reincarnation cycleaftercycle is something one wants to escape from and one does it by saying certain things to certain gods in the underworld Apollo and Artemis Children of Zeus and Leto who is of little mythological importance otherwise Apollo of medicine and plague youth and sudden death struck by the arrow of Apollo all the creative arts administers over the Muses rationality prophesy delegated by Zeus and later on associated with the Sun Depicted as a male in 20s with bow and quiver showing that of typical youthful manly beauty Pythia prietess of the Oracle of Delphi delivers the words of Apollo to those seeking prophesy which are most of the time ambiguous in nature eg Croesus of Lydia was told if he attacks the Persians he would destroy a great empire that of his own Motto at the Oracle of Delphi know yourself nothing in excess Know yourself in the sense of knowing your limitations and not offend the god Queen of Thebes suggested she should be worshipped for having 14 children while Leto only has two and all her children were struck dead by Apollo and Artemis Artemis the huntress goddess of animals the untamed women before marriage and the young Hestia Artemis and Athena are chaste goddesses of Greek myths In Artemiss case its potentially related with her representation of the untamed sex is viewed as domination and in one Greek dialect the word wife came from the word tamed Her association with hunting is also related with protection of the young which hunters often times do for sustainability Her most famous place of worship in Turkey near Selcuk temple at Ephesus Hermes and Dionysus Hermes of exchange of commerce trading thieves beggars boundaries the messenger and deliverers of souls to the underworld Hermes invented the lyre from tortoise shells and traded it with Apollo for a cattle he stole from Apollo in the first place The pillar herma is associated with Hermes herma represents boundaries those of a market eg and usually depicts a bearded god with an erect phallus The pillar is sacred and their mass destruction before Alcibiadess expedition to Syracuse was a serious matter that caused him being investigated Dionysus of madness irrationality frenzy growth of wild plants and wine Male worshippers Satyrs are human with beast features such as horse tails and later on goat below the waist always in a frenzy Female worshipers maenad are human women who has the strength to tear wild animals apart Nietzsche find greek culture often times representing the contrast of Apollonian reason and Dionysian madness Dionysus is often associated with tragic and comedian traditions of Athens perhaps due to their origins in rituals and actors impersonating someone else bears some similarity to Dionysian irrationality Euripidess Bacchae describes birth of Dionysus Zeus in disguise bedded mortal woman Semele whose nanny Hera disguised as out of jealousy and suggested Semele verify her partner is indeed a god and not pretending to be one by asking him to swear on the river Styx to fulfill one wish of Semeles which Zeus did Semele asks him to reveal his true glory which incinerated her Zeus took her embryo Dionysus and nurtured him in his thigh from where Dionysus is born He is thus sometimes referred to as the twice born The jealous Theban siblings of Semele refused to acknowledge Dionysuss divine status for this slight Dionysus drove the women of Thebes mad and tore apart the siblings Aphrodite Sexual passion Quite a different concept as modern perception of love in its being transient ephemeral She was born from Uranuss genitals in Theogony or child of Zeus and Dione in Iliad Aeneas son of Anchises and Aphrodite Zeus giving Aphrodite a taste of her own medicine Gods men and animals are said to be unable to resist Aphrodites power except the three chaste goddesses Homeric Hymn to Aphrodite tells the story of Anchises and Aphrodite who lied to the former of being a mortal woman Aeneas being the hero of Aeneid and claimed ancestor of Romans No good ever comes of the union between men and goddesses Examples including Adonis killed for his relationship with Aphrodite and Tithonus and Eos the goddess of dawn Zeus agreed in Eoss plead to make Tithonus immortal but she forgot to ask for youth Tithonus ages but never dies and eventually became just a babbling voice that Eos shut away Sexual relationship between an older man and an adolescent young man is often seen as Ok in greek myths but not other forms of homosexuality Maybe it does not make sense to question the ancient Greeks whether they believe in their gods who are personifications of natural forces To ask if an ancient Greek believes in Aphrodite would be like asking if he acknowledges the presence of sexual passion Ancient Greek belief in gods could be very different from monotheistic worship of God or Allah and the perspective is usually quite different if someone outside a cultural group were to inspect that groups myths Minoan and Mycenaean civilzations Minoan civilization prospered on Crete in about 2000BC its literature in two writings linear A and linear B were left the latter was not yet deciphered Mycenaean civilization prospered about 1600 to 1100BC after which a Greek dark age followed Mycenaeans seemed to have lost their ability to create intricate artwork and magnificent cities Trojan war and the Heroic Age was towards the end of the Mycenaean golden age Homers work came after this time but has description of ancient Greek cities verified by modern day archaeologists to match excavated Mycenaean cities His description may have been transcribed from oral traditions that traced back to historic events during that time The Five Ages Works and Days described the human race going through the golden age rein of Cronus no strife or turmoil long lives silver age Zeus Long childhood 100 years lifespan destroyed by Zeus for impiety bronze age Created by Zeus from ash tree common material for spear shaft Warlike and violent ended with Deucalions flood heroic age an improvement compared with bronze age heroes and the Trojan War and iron age Hesiods time theres no help against evil and eventually the grim vision that babies will be born greyhaired due to worries Story of Theseus unifier of Attica The test and quest pattern in Greek myth stories Dual fathership Aegis king of Athens and Poseidon both mated with the same woman on the same night the daughter of a king whose land the formers passed by on his way back from the Oracle at Delphi Aegis told Theseuss mother about a pair of sandals and a sword under a boulder and that if she bore a son when the son is old enough to lift the boulder collect the items and head to Athens On his way Theseus slew Procrustes who insists travelers stay in his bed and that they must fit the bed too tall lop off their legs too short stretch them Aegiss then wife Medea tried to poison Theseus but the plan was foiled by Theseus drawing his sword to cut the meat and Aegis recognizing the sword thus Theseus being his son common theme reunion and the last minute Medea escaped Theseus then volunteered as tribute to Minos to feed the minotaur whom he slew with the help of princess Ariadne common theme help of a young woman The latter provided him with a ball of threads the clue of Ariadne to guide him out of the labyrinth Theseus took her with him back on the ship to Athens but apparently forgot about her on an island on the way back Dionysus later rescued Ariadne married her and made her a goddess This is quite unusual and prompts assumptions that Ariadne was a Cretan goddess Theseus also forgot about raising white sails as promised causing Aegiss suicide After becoming king he tried to marry three times First being abducting the young Helen then journeying with Pirithous the duo conspired to marry daughters of Zeus Theseus picked Helen and Pirithous picked Persephone to the underworld to abduct Persephone but tricked by Hades and got stuck on stone chairs only to be rescued later by Heracles common theme in heros justification running into Heracles Pirithous was left in the underworld and Theseus returned to Athens to find Helen rescued by his brothers Theseus then abducted Hippolyta queen of the Amazons another common theme in heroes encounter who died giving birth to Hippolytus Theseus defeated Amazons trying to rescue their queen In Euripidess play Hippolytus Theseus then marries Phaedra sister of Ariadne and daughter of Minos Phaedra wanted Hippolytus was refused due to the latters oath of chastity and hanged herself leaving a letter accusing Hippolytus of raping her Theseus exiled Hippolytus and the latter died after which Theseus learnt of Phaedras lies The timeline of Theseuss story seems hopelessly confused Minos lived three generations before the Trojan War Heracles one generation before yet Theseus encountered both and abducted Helen Presumably Theseus held such importance in Athens that they needed to connect him with every major plot in classical myths Yet his origins may correspond with historical events archaeologists uncovered a ritual sport handling bulls and Athens very well could be paying tribute to Minoan civilization in Theseuss time Minotaur is the offspring of Minoss wife queen of Crete with a bull Minos promised to offer the bull to Poseidon but instead offered a lesser animal Poseidon enlisted the aid of Aphrodite to make the queen desire the bull and the best artisan in Crete crafted her a disguise as a cow to mate with the bull and conceived Minotaur The labyrinth in which Minotaurs locked is designed by the same artisan Theseuss story represents that of a rite of adulthood of a male Heracles PanHellenic hero and god Son of Zeus and a human mother Alcmene a descendant of Perseus Life of hardship because of jealousy of Hera Birth Descendant of Perseus by Zeuss decree to be king of Mycenae trick of Hera strangling the snakes A man of excessive appetite sexuality impregnated 50 daughters of a king in one night many later Greek kings traces back to this eg king of Sparta and rage Depicted naked with his lion skin and club Killed his wife Megara and his children in his rage Told by the Pythia to accomplish 12 labors in service to his cousin the king of Mycenae to redeem himself and become immortal The first 6 in Peloponnese related with animals Nemean lion nineheaded hydra bringing back Artemiss hind Erymanthian Boar Augean stables Stymphalian Birds The next 3 farther away maneating mares of Thrace bull of Crete Hippolytas girdle belt The last 3 in the far west associated with death and immortality driving back cattle of monster Geryon golden apples of the goddesses daughters of the night guarded by a dragon bringing back Cerberos the guarddog of the underworld Marries Deianira afterwards killed Nessus a centaur who attempted to rape Deianira with arrow poisoned by the blood of the hydra Centaur persuades Deianira to keep his blood as a love potion when Heracles loses interest in her someday in the future Deianira did and soaked a cloak in the centaurs blood Heracles did lose interest in her and Deianira brought out the cloak to Heracles which he wore which tore his skins and exposed his bones In great agony he threw himself upon a funeral pire and died His psyche soul then joined and Olympians and he married Hebe as a god A man of great contrasts in his tragic life and ascension as a god his great courage and rage etc Trojan War The most important episode in classical mythology and marks the end of heroic age We have heroic accounts of sons of those who fought in Trojan War but not grandchildren the connections heroes had with gods had ended and the iron age had begun Greeks believed the fall of Troy happened in 1184BC at the same time the Mycenaean civilization waned Over time the myths developed into a complex and timelineconfused system Including Wedding of Thetis a river goddess whose son is prophesied to be greater than the father for which Zeus married her off to a human Eris of discord not being invited and feeling offended tossed the golden apple for which Hera Athena and Aphrodite vied and Zeus left Paris to judge The family of Agamemnon and Menelaus tragic house of Atreus and the formers sacrifice of his daughter The sack of Troy the fate of Ajax Cassandra Priam Agamemnon Menelaus and Troy after the war Homer did not mention these but mentioned Aeneas and his prophesied escaped from Troy which Virgil took note and claimed him as an ancestor of Romans Tragic house of Atreus one of kinslaying and incest Tantalus father of Pelops who was served by Tantalus to the gods and revived Pelops later tricked fatherinlaw in chariot race to win his daughters hand in marriage causing the death of the fatherinlaw Atreus and Thyestes sons of Pelops wife of the former betrayed him for the latter Latter cheated in deciding kingship of Mycenae between the two former took vengeance by exiling the latter and later inviting him back to a feast during which he was served his children Agamemnon sacrificed his own daughter Menelaus and Aegisthus fathered by Thyestes on his own daughter due to Oracles prophecy his son by his own daughter would exact vengeance on Atreus Killed Agamemnon was killed by Oreste Moral sins are passed on from fathers to sons Story of Oreste son of Agamemnon is best told by Aeschyluss Oresteia the only complete surviving tragic trilogy of ancient Greece The third act is the trial of Oreste for killing his mother who murdered his father with Apollo defending Furies persecuting 12 citizens of Athens in jury and Athena as judge Athena casted the vital vote to acquit Oreste Oresteia in particular is written in 5th Century BC during Athenss transition from being ruled by the court of Areopagus to classical democracy The play might have wanted to remind the people the importance that the court still serves in the judicial system as opposed to city politics Greek Tragedies Format of Greek tragedy is usually a tragic trilogy followed by a comedy where the three episodes of the trilogy are connected via same events characters or the same theme Performed during the festival of Dionysus The creativity freedom an author was given may be very different from those of todays playwrights the theme story is somewhat fixed and well known but an author can add or twist particular details to serve his dramaturgy purposes Eg in Oresteia to justify matricide Clytemnestra as opposed to Aegisthus is described as the killer of Agamemnon Commonly depicted tragedy involves a moral dillema that a character faces Eg Oreste needs to avenge his father and he also must not hurt his mother but his mother murdered his father Or Agamemnon needs to lead his army across the sea to do Menelaus and his house justice but to do that he must sacrifice his daughter to appease Artemis Trying to evade fate but only falling in its traps instead was another common theme Eg Oedipus fated to kill his father and marry his mother Modern interpretation tends to focus on the struggle between fate and free will in Oedipuss story or Freuds farfetched theory of the same name though the concept of free will might have been foreign to ancient Greeks whose language did not have a word for it Amazons are a recurring encounter for heroes and the counter usually ends with heroes taming them eg Heracles Theseus putting women in their proper place as the patriarchal ancient Greek society saw it Medea is not an Amazon but another one of the dangerous immoral yet attractive foreign woman figure described as a witch from Colchis married first to Jason tore her own brother to bits to slow down his fathers pursuit of them for the stolen golden fleece and later poisons Jasons sons by her to hurt him then to Aegis Intergenerational tension and anxiety about women sexuality are recurring themes in classical myths in which males of the heavily patriarchal society is anxious about having heirs that are their own Ancient Greeks might have believed that babies are not blood relatives of mothers father does the work by planting seeds in the mother like how plants were grown Influence on Romans and after Romans largely took Greek myths for their own and rhetoric philosophy or culture in general Julius Caesar traces his lineage to Aeneis thus to Aphrodite Virgil would claim Aeneis is the ancestor of the Roman people without contradicting the myth that Romulus founded Rome the city 400 years after the fall of Troy Livy recounts the story of Romulus Vergil Horace Livy and Ovid are authors during Augustuss reign which saw a golden age for Roman literature Ovid of these did not receive patronage of the emperor due to his books on practical techniques of seduction being in conflict with the emperors agenda to reestablish traditional morals and religion punish extra marrital affairs and encourage fertility Shakespeares work is influenced by Ovids Metamorphosis often times quotes and allusions in the former traces to stories in Metamorphosis Ovid named his book Metamorphosis due to each story being related with turning from one form to another though many stories barely reflect that"},{"title":"\"Clean architecture\"","href":"/notes/clean-architecture","content":" Part 1 Goal of architecture To minimize human resources required to build and maintain the required system A good indication of a badly designed system tremendous cost of each line of code and low productivity of engineer over time And yet despite all their heroics overtime and dedication they simply arent getting much of anything done anymore All their effort has been diverted away from features and is now consumed with managing the mess Their job such as it is has changed into moving the mess from one place to the next and the next and the next so that they can add one more meager little feature And so the developers never switch modes They cant go back and clean things up because theyve got to get the next feature done and the next and the next and the next And so the mess builds and productivity continues its asymptotic approach toward zero The only way to go fast is to go well The misconception of going faster by spending less time in design The bigger lie that developers buy into is the notion that writing messy code makes them go fast in the short term and just slows them down in the long term Developers who accept this lie exhibit the hares overconfidence in their ability to switch modes from making messes to cleaning up messes sometime in the future but they also make a simple error of fact The fact is that making messes is always slower than staying clean no matter which time scale you are using And making matters worse Their overconfidence will drive the redesign into the same mess as the original project Two values of a program behavior and architecture Behavior programmers are hired to make machines behave in a way that makes or saves money for the stakeholders To fulfill its purpose software must be softthat is it must be easy to change When the stakeholders change their minds about a feature that change should be simple and easy to make The difficulty in making such a change should be proportional only to the scope of the change and not to the shape of the change From the stakeholders point of view they are simply providing a stream of changes of roughly similar scope From the developers point of view the stakeholders are giving them a stream of jigsaw puzzle pieces that they must fit into a puzzle of everincreasing complexity Each new request is harder to fit than the last because the shape of the system does not match the shape of the request The problem of course is the architecture of the system The more this architecture prefers one shape over another the more likely new features will be harder and harder to fit into that structure Therefore architectures should be as shape agnostic are practical Argument architecture is the more important value Consider the two extremes something that works but cannot be changed is useless facing changing requirements while something that doesnt work but can be easily changed can be changed You may not find this argument convincing After all theres no such thing as a program that is impossible to change However there are systems that are practically impossible to change because the cost of change exceeds the benefit of change Many systems reach that point in some of their features or configurations I have two kinds of problems the urgent and the important The urgent are not important and the important are never urgent Eisenhower The dilemma for software developers is that business managers are not equipped to evaluate the importance of architecture Thats what software developers were hired to do Therefore it is the responsibility of the software development team to assert the importance of architecture over the urgency of features Fulfilling this responsibility means wading into a fightor perhaps a better word is struggle Frankly thats always the way these things are done The development team has to struggle for what they believe to be best for the company and so do the management team and the marketing team and the sales team and the operations team Its always a struggle Just remember If architecture comes last then the system will become ever more costly to develop and eventually change will become practically impossible for part or all of the system If that is allowed to happen it means the software development team did not fight hard enough for what they knew was necessary Part 2 Paradigms of programming Structured programming impose discipline on direct transfer of control OOP impose discipline on indirect transfer of control Functional impose discipline upon assignment Each of these paradigms takes something away from us Note how well the three aligns with the big concerns of architecture function separation of components data management Structured programming Goto not in the context of if while makes a program hard to decompose into smaller units making proof by divide and conquer hard Structured programming allows modules to be recursively decomposed into provable units Falsifiable testable Tests can prove a program incorrect but not correct Software is like scientific laws we show correctness by failing to prove incorrectness despite our best efforts OOP Whats really the point Encapsulation Cs forward declaration where data members of a struct need not be declared in header can achieve hiding members and implementation from clients C breaks this kind of encapsulation Inheritance In C if one struct A is a pure superset of another B A can masquerade as B Cstyle cast of A to B Such trickery is how C implements single inheritance as well Polymorphism C can achieve polymorphism as well Eg the definition of STDIN how does a console getchar call know which device is STDIN Unix require that every IO device driver provide five standard functions with same function signatures open close read write seek FILE is a struct with members that are pointers to the above function signatures STDIN can then be pointer to FILE who was populated with consoles implementation of the above five functions This approach is not that different from Cs vtable polymorphism While polymorphism is achievable in C C makes it more convenient and safer This plugin architecture of device drivers makes programs device independent Dependency inversion the source code dependency of an inheritance relationship points in the opposite direction compared to the flow of control where a highlevel function needs to know the source of a lowerlevel function in order to call it This means any source code dependency no matter where it is can be inverted And one is not constrained to organizing dependencies aligned to the flow of control With the dependencies organized via an interface and plugged in one also achieves independent deployability and independent developability Having compile dependency being opposite to runtime dependency is dependency inversion What is OO to an architect OO is the ability through the use of polymorphism to gain absolute control over every source code dependency in the system It allows the architect to create a plugin architecture in which modules that contain highlevel policies are independent of modules that contain lowlevel details The lowlevel details are relegated to plugin modules that can be deployed and developed independently from the modules that contain highlevel policies Functional programming Immutability Variables in functional languages do not vary Why would immutability be a concern race conditions deadlocks concurrent update problems are all due to mutable variables Mutability in Clojure functional languages like Clojure can allow mutable variables but only mutated under very strict conditions that are enforced by swap which uses a traditional compare and swap algorithm Thus a wellstructured program can be modeled in a functional language by segregating its variables into mutable and immutable ones And itd be wise to push as much as possible into immutable area Event sourcing The limits of storage and processing power have been rapidly receding from view Nowadays it is common for processors to execute billions of instructions per second and to have billions of bytes of RAM The more memory we have and the faster our machines are the less we need mutable state As a simple example imagine a banking application that maintains the account balances of its customers It mutates those balances when deposit and withdrawal transactions are executed Now imagine that instead of storing the account balances we store only the transactions Whenever anyone wants to know the balance of an account we simply add up all the transactions for that account from the beginning of time This scheme requires no mutable variables If this still sounds absurd it might help if you remembered that this is precisely the way your source code control system works BigTable those that are based on log structured merge trees Change data capture and building derived views from log Kafka and logbased message brokers are all ideas along this line Part 3 The principle of SOLID for module mid level software design component in BDE terms Goal tolerate change easy to understand create basis of components that can be used in many software systems Not confined in OOP general enough for any paradigm that groups functions and data into general classes Single responsibility principle so that each module has only one reason to change being responsible only to one actor group of stakeholders Cohesion SRP Symptoms eg putting functions that support different parties in one class Resolution eg putting them in different classes and if required have them share the same underlying data Now more classes needs to be instantiated and kept track of Consider Facade pattern if needed Similarly common closure principle and at architecture level axis of change Open closed principle software system should allow change of its behavior by adding new code rather than changing existing code A software artifact should be open for extension but closed for modification Or the behavior of a software artifact ought to be extendible without having to modify that artifact Architecture separates functionality into a hierarchy of components no bidirectional dependencies crossing package group boundaries Higherlevel components are protected from the changes made to lowerlevel components depend on an interface not an implementation dependency inversion Liskov substituion principle build software from interchangeable components and have them conform to a contract so that they are substitutable We want this substitution property if for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T input type contravariant Remind yourself of this especially when you start to see a lot of special case handling in the code It might be that we have the wrong generalization Can Colombian quasiIL Chinese dimsum Italian tax rate be justified Interface segregation principle avoid depending on things you dont use This includes transitive dependencies consider using an interface for information hiding in that case In general it is harmful to depend on modules that contain more than you need Achieving flexibility in dynamically typed languages like Python may be different in this sense from say C Consider the current sfgs architecture Or by DI the same principle should hold highlevel policy should not depend on lowlevel details Dependency inversion principle highlevel policy should not depend on lowlevel details should be the other way round The most flexible systems are those in which source code dependencies refer only to abstractions not to concretions Reduce the volatility of interfaces Dont refer to volatile concrete classes The relationship between this and the usage of abstract factory Dont derive from volatile concrete classes Dont override concrete functions Dont mention the name of anything concrete and volatile Main will often times be a concrete component that relies on concrete components as ultimately one needs to specify which concrete implementation should be used Rethinking our question on TCPClient in client side business objects Say main creates a client side business logic object One should consider using abstract factory to abstract away a transport mechanism from client side business logic object assume this object needs to instantiate thus one of the differences between taking in an interface and an abstract factory other consideration being fully isolated unit tests acquire which might make transport layer client not such a great example a transport mechanism which should be transport mechanism independent So it looks like when you instantiate the client side business object in main give it a could be global concrete factory Think about architectural boundaries and dependency inheritance lines that cross them more in dependency rule Part 4 package group rules package a unit of release that retains the ability to be independently deployable and developable Early days of programming and nonrelocatable code linking loader external reference U symbols and external definitions definitions exposed to external callers T symbols Then linking and loading were split linking is a one time job after compilation and loading happens at runtime claim shared object is reexploring the idea of a linker loader Package cohesion which components go into which packages REP reuse release equivalence principle The granule of reuse is the granule of release the package must have some overarching theme or purpose that components in it all share and components grouped into a package should be releasable together weak rule CCP Gather into packages those components that change for the same reason and at the same times Separate into different packages those components that change at different times and for different reasons Its similar with Single Responsibility Principle in that the latter discusses what to put in one component and this discusses what to put in a package CRP common reuse principle Dont force users of a package to depend on things they dont need Classes and modules that tend to be reused together belong in the same package We want to make sure that the classes we put into a package are inseparable it is not possible for a client to depend on some and not on others Its similar with Interface Segregation Principle package level vs component level The first two rules are inclusive and third is exclusive and an architect strikes the right balance and expect the balance to change over time Package coupling the dependency between packages ADP acyclic dependencies principle Break down dependency cycles so that when one package changes the affected parties are only its clients and their change should not propagate back to this changed package and the release of each packge can go from bottom to up Cycles essentially make the packages one large piece and difficult to isolate packages and unit test Common ways to break down cycles include separating a smaller piece out that both packages depend on and applying DIP The logical design of the system can go topdown with whom the packages dependency design grows and evolves The weekly build similar as ADP also introduced to combat multiple contributors to the same project changing things others depend on and project being unable to build four days independent dev and one day integration not exactly robo cycle SDP stable dependency principle Depend in the direction of stability There are packages that are developed knowing theyd be volatile and subject to change Any volatile package should not be depended by a package that is difficult to change Stability does not necessarily mean frequent changes but rather the effort required to change the package size clarity complexity amount of clients eg as denoted by NumberOfDependencies NumberOfDependencies NumberOfClients and the lower the more stable the package is Typically abstract interfaces in statically typed languages should be very stable SAP stable abstraction principle A package should be as abstract as it is stable Stable abstract or consists of interfaces unstable concrete implementation This echos with OCP where stable interfaces are closed for modification but open to extention SAP SDP DIP for packages If something is widely depended on it better be abstract and do not change often With packages there can be a shade a grey partially abstract and stable a measure of abstractness can be number of interfaces divided by total number of classes in package Consider a plane of Stableness Abstractness per calculations above Packages grouping around 0 0 likely introduce pain since these are concrete and stable Some packages do fall into this zone eg a DB schema or a concrete utility such as String class which is concrete and highly depended on Packages grouping around 1 1 are likely useless abstractions that few uses The most desirable position for components is at the two ends of the diagonal aka the main sequence Thus a design can be analyzed for the overall distance to the main sequence The quantization of this metric though is arbitrary and imperfect Part 5 architecture Architecture What is a software architect He is a programmer first and foremost The purpose of an architecture is to facilitate development deployment operation and maintenance of the software system contained within The strategy behind this facilitation is to leave as many options open as possible for as long as possible Note that it is not architectures primary focus to make sure the system works correctly sure it helps but many existing systems work fine but have poor architecture for these systems the pain is in development maintenance and deployment Different team structures imply different architectural decisions On the one hand a small team of five developers can quite effectively work together to develop a monolithic system without welldefined components or interfaces In fact such a team would likely find the strictures of an architecture something of an impediment during the early days of development This is likely the reason why so many systems lack good architecture They were begun with none because the team was small and did not want the impediment of a superstructure On the other hand a system being developed by five different teams each of which includes seven developers cannot make progress unless the system is divided into welldefined components with reliably stable interfaces If no other factors are considered the architecture of that system will likely evolve into five componentsone for each team Such a componentperteam architecture is not likely to be the best architecture for deployment operation and maintenance of the system Nevertheless it is the architecture that a group of teams will gravitate toward if they are driven solely by development schedule Deployable to be effective a software system must be deployable ideally with one single action One potential downside of micro services For example in the early development of a system the developers may decide to use a microservice architecture They may find that this approach makes the system very easy to develop since the component boundaries are very firm and the interfaces relatively stable However when it comes time to deploy the system they may discover that the number of microservices has become daunting configuring the connections between them and the timing of their initiation may also turn out to be a huge source of errors Had the architects considered deployment issues early on they might have decided on fewer services a hybrid of services and inprocess components and a more integrated means of managing the interconnections Operation architecture tends to be a less dramatic effect as almost any operational difficulties can be resolved by throwing more hardware at the system Yet architecture should reveal operation elevate use cases features and the required behaviors of the system to first class entities that are visible landmarks for the developers Maintenance of all the aspects of a software system maintenance is the most costly whose primary costs come from spelunking cost of digging through existing software to decide the best place and strategy to add a new feature or repair a defect and risk changing existing behavior and introducing bugs Keep options open recall the two values of a software with the greater value being structure as thats what makes software soft it does so by leaving choices open Choices that are left open are details that dont matter All software systems can be decomposed into two major elements policy and details Policy embodies business rules and procedures which is where the true value of the system lives Details are things that are necessary to enable humans other systems and programmeres to communicate with the policy but that do not impacting the policy at all IO devices DBs web systems communication protocols etc The goal of the architect is to create a shape for the system that recognizes policy as the most essential element of the system while making the details irrelevant to that policy This allows decisions about those details to be delayed and deferred And the longer you wait to make those decisions the more information you have to make them properly and this leaves you open to trying different experiments flavors of details What if the decisions have already been made by someone else What if your company has made a commitment to a certain database or a certain web server or a certain framework A good architect pretends that the decision has not been made and shapes the system such that those decisions can still be deferred or changed for as long as possible A good architect maximizes the number of decisions not made Device independence detail vs policy we learnt from the early days Good architects carefully separate details from policy and then decouple the policy from the details so thoroughly that the policy has no knowledge of the details and does not depend on the details in any way Good architects design the policy so that decisions about the details can be delayed and deferred for as long as possible Independence A good architecture must support use cases in making them obvious and operation maintenance development deployment A good architecture makes the system easy to change in all the ways that it must change by leaving options open Conways law Any organization that designs a system will produce a design whose structure is a copy of the organizations communication structure A good architecture does not rely on dozens of little configuration scripts and property file tweaks It does not require manual creation of directories or files that must be arranged just so A good architecture helps the system to be immediately deployable after build In the real world the difficulty is that most of the time we dont know what all the use cases are nor do we know the operational constraints the team structure or the deployment requirements Worse even if we did know them they will inevitably change as the system moves through its life cycle Meanwhile some principles of architecture are relatively inexpensive to implement and can help balance those concerns even when you dont have a clear picture of the targets you have to hit Those principles help us partition our systems into wellisolated components that allow us to leave as many options open as possible for as long as possible Decoupling layers Consider the use cases an architect may not know all of them but knowing the intent of the system an architect can use Single Responsiblity Principle and Open Closed Principle to gather things that change for the same reason Some obviousness exists in identifying these things UI business rules and DB query language schema accessor etc usually change for different reasons thus keeping them independently evolvable is a good practice Business rules may serve different goals eg input validation and calculation of interest on an account These rules will change at different times for different reasons so they should be separated and kept independently changeable Decoupling use cases Use cases can be grouped as well based on when and why they change thus making use cases a natural way to divide the system While the components UI business rules DB are horizontal layers use cases are vertical slices through the horizontal layers Thus we divide the system both horizontally and vertically Advantage of decoupling use cases dividing vertically being each use case uses a different aspect of UI business rule DB and adding new cases wont affect existing ones The division by functions YASN SWPM OVME CDSW etc is an example of this vertical decoupling We do find awkwardness in this decoupling This could be a case where the general guideline applies but making the right decision is case by case and requires knowledge and observation of the system Decoupling mode eg into services The decoupling we did for layers use cases also helps with operations different parts of the system can run on different machines cater to different performance throughput requirement scale differently To take advantage of the operational benefit the decoupling must have the appropriate mode eg not depending on addressing in the same machine Many architects call such components services and architecture based on services is a serviceoriented architecture Modes can be decoupled at Source level changing one module does not force the recompilation of one that is independent from it Binary deployment level changing one module like changing a shared object do not force others to rebuilt and redeployed Execution unit service level reduce dependency between components down to the level of data structures And each execution unit is independent from others SoA may not be the best architecture for everything The best changes as the project evolves Although the general solution is SoA it may be expensive development time and system resources and encourage coarsegrained decoupling Rather a good architect leaves the options open for as long as possible with decoupling mode being one of those options and protects majority of the code from potential changes foresees these changes and appropriately facilitates them Independent developability The horizontal and vertical decoupling helps with team organization and focus Independent deployability If done right the decoupling should help each component deploy and roll out separately Duplication Architect often fall into a trap that hinges on their fear of duplication Duplication is generally a bad thing in software But there are different kinds of duplication two seemingly duplicated code blocks may evolve along different paths and for different reasons When you separate vertically you will be tempted to think two use cases are similar because of similar algorithm UI screens or DB accesses Be careful Make sure the duplication is real before committing into coupling them Similarly when separate horizontally you may notice the data structure of a particular database record is very similar to the data structure of a screen view You may be tempted to simply pass the DB record to the UI rather than to create a view model that looks the same and copies the elements across Be careful this duplication is most likely fake better keep them decoupled The discussion with Fei about DPQA using an individual DBRecord class client structure or generated message type may reflect consideration for this coupling Boundaries drawing lines Separate software elements from one another with those side A knowing little about side B side B may know about side A Some are drawn early on some later Those early on are drawn for the purpose of deferring decisions avoid having decisions about details pollute business logic Coupling because of premature decisions saps a systems maintainability A good system avoids premature decisions made on details such as frameworks databases web servers util libraries dependency injection and defer them to the latest possible without significant impact what might go wrong in the design of an SoA they are not panacea Which lines to draw between things that matter and things that dont Contrary to common misconception usually business rule doesnt need to know database schema thus the layer of abstract database accessor which fortunately is in dpqa design Eg business rule and accessor interface belong to one element accessor impl belongs to another Direction of lines crossing the line we draw business rules element doesnt know about DB impl DB impl knows about business rules as db cant exist without business rules query languages in accessor impl reflects business logic not vice versa business rule only needs to know about an accessor interface what underlying storage is does not matter Similarly IO GUI is irrelevant to business rules With this we form a plugin architecture DB GUI are plugins to business rules This deeply asymmetric relationship is what we want in our systems for certain modules business rules to be immune to others DB GUI Touching back on earlier content elements on different sides of the line change at different times for different reasons Single Responsibility Principle tells us where to draw the boundaries Boundary anatomy Architecture of a system is defined by a set of components with boundaries separating them At runtime a boundary crossing is a function on one side calling one on the other side Trick to creating appropriate boundary crossing is to manage source code dependencies Without OO or an equivalent form of polymorphism architects must fall back on the dangerous practice of using pointers to functions to achieve the appropriate decoupling Reminder for the argument that high level business logic should not depend on low level mechanism should be the other way round Threads are not architectural boundaries but rather a way to organize the schedule and order of execution They may be contained in one component or span across multiple A local process is a much stronger physical boundary and a service even stronger For local processes and services the same argument of high level services should not contain specific knowledge of low level services but rather low level services can serve as plugins to high level services applies Policy and level Software systems are statements of policy that which transforms inputs to outputs Policies further break down into smaller ones eg business rules for calcs input validation output formatting etc Part of the art of developing an architecture is carefully separating policies from one another grouping them based on the ways they change and into separate components which are formed into an acyclic graph Meaning policies too should be grouped by Single Responsibility Principle and Common Closure Principle In a good architecture the direction of those dependencies is based on the level of the components that they connect In every case lowlevel components are designed so that they depend on highlevel components Level a strict definition of level is the distance from the inputs and outputs The farther a policy is from both the inputs and the outputs of the system the higher its level The policies that manage input and output are the lowestlevel policies in the system The data flows and the source code dependencies do not always point in the same direction This again is part of the art of software architecture We want source code dependencies to be decoupled from data flow and coupled to level Higherlevel policiesthose that are farthest from the inputs and outputstend to change less frequently and for more important reasons than lowerlevel policies Lowerlevel policiesthose that are closest to the inputs and outputstend to change frequently and with more urgency but for less important reasons Lower level policies should be plugins to higher level policies Think about the sapiwrapper service we recently looked at retrieving different data from a number of services carry out some business logic and serve the requests to it In this case the services sapiwrapper asks for data should be plugins to sapiwrapper service imagine the data source could be a BAS client or file system etc with sapiwrapper services own business logic being higher level Then if we think about plugging in guts to a business logic module metrics being low level should probably also be not a direct dependency of the higher level business logic module But instead we have a metrics interface Business rules If we are to divide our application into business rules and plugins we need to understand what business rules really are Critical business rules those that will save make business money executed on a computer or not Critical business rules usually require critical business data to work with We model critical business rules and critical business data with an Entity an object a module etc and this entity should exist unsullied with concerns about databases UI or thirdparty software Not all business rules are entities they can be use cases a description of how the automated system is used applicationspecific business rules one that wouldnt exist on paper Use cases contain rules that specify when critical business rules within the entities are invoked they control the dance of entities while entities should have no knowledge of them Entities are high level use cases are low level in the sense that use cases are application specific rules while entities are general enough for any applications Use case does not describe say a concrete user interface or a data ingestion method which are at an even lower level use case takes in input structure and gives output structure From what transport mechanism those come is lowlevel detail You might be tempted to contain references to entity objects inside requestresponse model because they share so much data Avoid this temptation as they change for different purposes at different times Business rules are the family jewels that should remain pristine unsullied by baser concerns such as UI or DB who should be plugged in as lesser concerns Screaming architecture Software architectures are structures that support the use cases of the system Just as the plans for a house or a library scream about the use cases of those buildings so should the architecture of a software application scream about the use cases of the application as opposed to a particular framework When using a framework think about hwo you can preserve the use case emphasis of your architecture Develop a strategy that prevents the framework from taking over the architecture If you have kept your frameworks at arms length and your architecture is all about the use cases then you should be able to unit test each particular use case without having frameworks say BAS ComDB2 in place The clean architecture An architecture should be independent of frameworks UI DB external agencies and testable Figure 221 is a good illustration in a concentric circle entities being the inner most wrapped by use cases then by interface adapters and finally by frameworks Given this circle we have the Dependency Rule source code dependency should only point inwards no changes in the outer circle should cause changes in the highlevel business object Note how the flow of control is the reverse of source code dependency business object calls something from the outer circle via an interface flow of control but in this callingaconcreteimplementationviaaninterface pattern why do we say source code dependency is inverted Maybe although the DB accessor does not include the business object header but the queries it makes reflect the usage pattern mandated by the business object An answer could be that the interface is considered part of the inner circles package and a concrete implementation includes this interface definition such that it can inherit from this interface And in terms of what goes in this interface take DB as an example recall that we dont allow SQL in use case layer So the particular query inputoutput would be hidden behind an interface and implemented by the concrete accessor as opposed to accessor exposing a SQL execution interface Isolated simple data structures are passed across boundaries we dont want data structures to have any kind of dependencies that violate the Dependency Rule eg passing a DB row structure thats particular to a DB Presenters and humble objects The humble object pattern was introduced as a way to help unit testers to separate behaviors that are hard to test from those that are easy to test split the behaviors into two classes the humble one containing all the hard to test behavoirs eg where an element is in GUI the view and the other one containing the easy to test behaviors the presenter that formats the data it gets and makes them screendisplay ready Focus on testing the presenter and keep the view humble does nothing more than load data from View Model into the screen Another example would be a DB accessor the concrete component that wraps around a particular DB query is humble and the interactors using such accessors via an interface are not they encapsulate particular business rules and perform things based on the output Focus the test on the interactor Then speaking of real examples is iribsz sft mapper humble I believe yes one way to think about it is this module packs a plain old data to be carried across boundaries the mapper itself matters less than the interactor that works with the data not unlike given a View Model load data to the screen If this holds does it justify not unit testing iribsz sft mapper Partial boundary A full boundary may be expensive boundary interfaces input output data structures and hard to maintain A purely anticipatory design may violate YAGNI you arnet going to need it in which case architects sometimes propose a partial boundary in which all the components may be in place but sitting in one group or a dependency inversion interface is in place or like a Facade pattern where even a dependency inversion is not enforced Layers and boundaries Hunt the Wumpus game example support multiple languages Different input interfaces Underlying data storage Multiplayer Player status management as a microservice The point is however simple a program architects need to identify boundaries when they are needed and if we dont add them now how much effort would it be to add them later Over engineering is often much worse than under engineering but on the other hand when you discover you do need boundaries whereas they dont already exist its often risky and costly to add them As an architect you make educated guesses and keep a watchful eye and revisit your decisions as the project progresses The main component In every system there is at least one component that creates coordinates and oversees others We call this component Main Main is the ultimate detail the lowestlevel entry Nothing depends on Main Main create higher level objects and hand over control to them Think of Main as a plugin to the application who sets up the initial conditions and configs gathers outside resources then hand over control to higher level policy of the application A different Main configuration could be plugged in for each scenario Services great and small SOAs Are services always architecturally significant Not necessarily the architecture of the system is defined by boundaries that separate highlevel policy from lowlevel detail Services themselves are a form and they are architecturally significant when they represent architectural boundaries Do they offer better decoupling Not necessarily They can still be coupled by shared resources on the network or data they share Eg similar to function signature changes plumbing a new field in a series of microservices needs service schema changes Do they offer independent development and deployment True to some extent but often times we see operations still needing coordination Architecture is not defined by the physical mechanisms by which elements communicate and execute Cross cutting concerns a system built with functional decomposition are very vulnerable to new features that cut across all functional behaviors SOA or not eg Uber to deliver cats Considering which taxi suppliers drivers are in passenger allergies etc The answer is to think in terms of functional components a taxi supplier component service or not could offer an interface where the concrete human cat rides behavior can derive from In which case the boundary may run through services dividing them into components To summarize useful as they are SOAs are not panaceas and to account for cross cutting changes the internals of a service or a functional component may need to be designed with Dependency Rule in mind and allow pluggable concrete implementations Test boundaries Tests of different flavors unit integration etc are part of the system They are very detailed and concrete nothing depends on the them and they follow the Dependency Rule pointing inwards Fragile test problem changing common system components breaks tons of tests Tests that are not well integrated into the design of the system tend to be fragile and they make the system rigid and difficult to change Design for testability tests and others should not depend on volatile things Eg test the business logic without depending on UI Structural coupling questionable if a test suite has a test class for each production class and a test method for each production method tests have to be changed as well for each production interface change The proposed solution is a test API that allows changes in production which dont affect the tests I find this questionable and should be analyzed case by case in unit tests all public interfaces should be tested At unit test level having test API probably means having methods to force the object into a certain state This would be useful but does not allow not having a test method corresponding with each method in the public interface what the authors claim to be structural coupling Clean embedded architecture Software does not wear out unlike hardware However software can be destroyed from within by unmanaged dependencies on firmware and hardware What really is firmware Code that lives on ROM Maybe another way of seeing it is code that is heavily dependent on hardware In a sense we write firmware too if our software is coupled with particular hardware details Dont write firmware if you want your code to have a long useful life First make it work Then make it right Then make it fast The hardware is a detail Split your software from your firmware and make those that dont need to depend on hardware hardwareindependent Software should allow off target testing Introduce a Hardware Abstraction Layer between firmware and software if needed and HAL exposes applicationsemantic interfaces Similarly you should insulate your software from OS dependencies An OSAL should be introduced Similarly for us pricing code should be testable off target as well Dont repeat yourself Part 6 details The database is a detail The particulars of a database is of little architectural importance while the data model is How would one define a data model Business entity should not know the detail of the underlying data stored in a relational way represented as tabular rows of records etc Why are database systems so prevalent They work with disks file system does and offers a document abstraction databases are content based and provide a natural and convenient way to find records based on their content Relational database used to be a buzzword nowadays are SoAs such words as well The web is a detail The endless pendulum clientside execution serverside execution superhosts and dumb terminals or vice versa cloud or edge centralizing distributing the spiraling up These oscillations will continue for some time to come The story of the UI of company Q The upshot is simple the GUI is a detail and the web is a GUI as an architect keep such stuff away from your business logic Alternatively does it make sense to provide abstractions over different technologies like the web and the desktop or over RabbitMQ and Kafka The argument can be made that a GUI like the web is so unique and rich that it is absurd to pursue a deviceindependent architecture When you think about the intricacies of JavaScript validation or draganddrop AJAX calls or any of the plethora of other widgets and gadgets you can put on a web page its easy to argue that device independence is impractical To some extent this is true The interaction between the application and the GUI is chatty in ways that are quite specific to the kind of GUI you have The dance between a browser and a web application is different from the dance between a desktop GUI and its application Trying to abstract out that dance the way devices are abstracted out of UNIX seems unlikely to be possible The argument is that its still possible to provide such abstractions especially isolating business logic from the underlying details This kind of abstraction is not easy and it will likely take several iterations to get just right But it is possible And since the world is full of marketing geniuses its not hard to make the case that its often very necessary Frameworks are details Frameworks are not architectures Framework authors do not know your problems although your problems may overlap The framework author is asking you to couple with their framework your commitment to the framework is huge while the framework makes no commitment to you Risks with a framework the framework may help you with early features but your product may outgrow the framework the framework may evolve in a direction you dont want a better framework may come along the way The solution is that you can use a framework but dont marry it dont let it in the inner circle If the framework wants you to derive your business objects from its base classes say no Derive proxies instead and keep those proxies in components that are plugins to your business rules Use case Video serving website roles and use cases division of components The missing advice Your best design intentions can be destroyed in a flash if you dont consider the intricacies of the implementation strategy Think about how to map your desired design on to code structures how to organize that code and which decoupling modes to apply during runtime and compiletime Leave options open where applicable but be pragmatic and take into consideration the size of your team their skill level and the complexity of the solution in conjunction with your time and budgetary constraints Also think about using your compiler to help you enforce your chosen architectural style and watch out for coupling in other areas such as data models The devil is in the implementation details Afterword The evolution from Big architectures and engineering practices to Agile Now youve seen how its possible to write code that delivers value today without blocking future value tomorrow the onus is on you to put in the practice so you can apply these principles to your own code Like riding a bicycle you cant master software design just by reading about it To get the best from a book like this you need to get practical Analyze your code and look for the kinds of problems Bob highlights then practice refactoring the code to fix these problems If youre new to the refactoring discipline then this will be a doubly valuable experience"},{"title":"\"The creative habit -- twyla tharp\"","href":"/notes/creative-habit","content":"Being creative is a fulltime job with its daily patterns More than anything this book is about preparation in order to be creative you have to know how to prepare to be creative If art is a bridge between what you see in your mind and what the world sees skill is how you build that bridge Establish rituals for preparation automatic but decisive patterns of behavior at the beginning of the creative process Build yourself an environment thats habit forming Everyday you dont practice you are one day further from being good Give me a week without XXX wechat instagram email texts Identify your creative DNA Are you excited by creating something at a macro scale or micro scale The general zoe or the specifics bios Harness your memory Metaphor is the life blood of all art as it transforms something strange into something familiar If all art is metaphor then all art begins with memory Before you think out of the box start with a box an organizational system that helps consolidate your work and ideas Scratching the first steps of a creative act are like groping in the dark random and chaotic feverish and fearful Scratch for ideas through reading everyday conversation peoples handiwork mentors and heroes and nature Ideas can be acted upon in four ways generate the idea from memories or experience or activities retain it in your memory or your box inspect it to study it and make inferences transform it alter it in some way to suit your higher purpose Be in shape Scratching takes longer when you are not Scratch in the best places Dont scratch the same place twice Maintain the white hot pitch Be prepared for accidents They will happen Have a plan in mind when you get down to work but also know what to do if that plan falls apart Meanwhile there is a fine line between planning and overplanning you never want planning to inhibit the natural evolution of your work Identify the spine of your work the first strong idea as you scratch then develop it into the spine of your work Skill gives you the wherewithal to execute whatever occurs to you Without it you are just a font of unfulfilled ideas You are only kidding yourself if you put creativity before craft Take an inventory of your skills try taking one away would you still be able to create Ruts and grooves They happen There will come a time when your creativity fails a rut you stare at the screen but it refuses to meet your eyes You spin your wheels it spits out mud and splatters other people but it doesnt go anywhere You get the feeling the world is moving on but you are standing still It could be the consequence of a bad idea bad timing bad luck etc To get out of a rut identify and admit the rut first then act on it see whats not working write down your assumptions about it challenge your assumptions and act on the challenge A groove is a golden age in your creation Failures They happen No matter who you are at some point you will present your work to the outside world and the world will find it wanting Patrons shrug critics hiss audiences stay away in droves and even closest friends avert their eyes Incredibly there is good news here Sometimes you will fail but the world will give you a second chance to get it right Admit your failure and adapt You do your best work after your biggest disasters The long run It takes time to master an art Think act rinse and repeat Your best work may come from a creativity bubble where you eliminate distractions sacrifice pleasures and place yourself in a singleminded isolated chamber where all your efforts feed to the work Create eat sleep exercise and solitude Being in the bubble doesnt mean exciling yourself from the outside world We have families jobs responsibilities that impinge on our desire to create At its worst the bubble conjures up an image of the artist toiling away in his studio ignoring the cries of his children and sacrifices his wife makes so that he can concentrate on his work I dont find that a heroic image Maintain a balance Sooner or later youll have to come out of the bubble and when you do itd be nice if there were loved ones to greet you"},{"title":"\"Designing data intensive applications\"","href":"/notes/data-intensive-application","content":" Chap 1 Reliability scalability and maintainability Data intensive vs compute intensive Reliability Fault and failure Hardware error usually uncorrelated and handled by redundancy Software error usually happens when certain assumptions about the system dont hold any more Correlated and cascading and harder to debug Human error Scalability Defining load The Twitter example to handle large fanout large read workload problem Relational query approach Fanout queue approach Measuring performance Percentiles Load parameters Head of line blocking Tail latency amplification Conventional wisdom for scaling databases or stateful data systems is to scale up grow tall as opposed to scale out grow wide until you hit a limit Think ComDB2 Elasticity Maintainability Operability simplicity extendibility agility of a system Complexity as native to the use case required functionality or accidental debt and hacks accumulated over time fitting a circle into a square etc Good abstractions albeit hard to come by help bring down accidental complexity Nonfunctional requirements Chap 2 Data model and query languages Relational data model SQL Data is organized by relation into tables of rows unordered collection of tuples Comes from business data processing business transactions and batch processing Back in the days network model and hierarchical model eg xml DB were the main alternatives NoSQL describes a variety of nonrelational approach to data modeling Driven by these concerns Scalability very large datasets very high write throughput Open free software over commercial software Specialized queries not well supported in SQL Restrictiveness of a relational schema need more dynamic and expressiveness In the near future well see a polyglot persistance relational approach coexists with NoSQL Objectrelational mismatch matching SQL tables rows columns and OOP objects takes effort ObjectRelational Mapping tools are introduced to address this If we are to model onetomany with SQL the options are separate table with foreign key latest SQL has structured data type support eg xml json This comes with indexing query support inside This is almost like a blend between documentbased and relational encode as json xml and store encoded Wont be able to index query in this case The resume use case selfcontained document one person to many jobs One json object a row makes sense documentoriented DB like MongoDB comes in handy This features a lack of schema flexibility and better locality normalization in DB refers deduplicating humanmeaningful information using one level of indirection eg the neighborhood a person lives in should not be represented with a raw string but rather an enum with another map from enum to neighborhood name string This brings the problem of modeling manytomany where documentoriented storage can get awkward wed need applevel joins on several documents network model uses a treelike structure with multiple parents and software traverses the structure following along access paths like traversing a linked list This makes it difficult to change data model and the software the relational model addresses this with separate tables and supporting arbitrary select and join conditions Query optimization is abstracted away from applevel code In dealing with manytomany relationship relational and document databases arent fundamentally different a unique id to identify the external record called a foreign key and a document reference respectively Relational vs document databases today Schema flexibility better performance due to locality closer to the data structure of the application for document databases Better support for join manytoone and manytomany for relationship databases Relational can employ the technique of shredding splitting a documentlike structure into multiple tables linked by a foreign key to achieve a documentlike structure which is cumbersome to schema and complicates application code Document cannot refer directly to a nested item within a document instead you say the 2nd item in the list of positions for user 256 like an access path To model manytomany in document you can do denormalization but the application code need to do additional work to keep denormalized data consistent Schemaonread the reading application interprets expects certain schema like runtime type checking and schemaonwrite the database enforcing some schema upon writing data like static type checking in a compiled language In general there is no right answer in terms of which is preferable The difference is particularly noticeable when performing a schema change Changing application code or migrating DB Managing locality When you need large parts of data in the same document Or in relational model Spanners interleaved table rows within a parent table Similarly columnfamily in Bigtable Cassandra HBase data model When the relational model is introduced SQL a declarative way to query the data came with it whereas others of its time queried databases primarily imperatively The declarative approach follows relational algebra notions more closely while the imperative way is more like telling the computer step by step what to do Higherlevel abstractions are often preferable as they are easier more expressive and the application query is decoupled from underlying database performance improvement Higher level query languages like SQL can be implemented as a pipeline of MapReduce operations and there is nothing that constraints a SQL query to run on a single machine Graphlike data models social graph web graph road networks etc where nodes are homogeneous Eg Facebooks social graph can have nodes as people locations events etc Property graphs Each node has a uniqueid incomingvertices outgoingvertices and a collection of keyvalue pairs Each vertice has a uniqueid headvertex tailvertex label and a collection of keyvalue pairs Can store flexible information and good for extension Described in a relational schema it looks like the following CREATE TABLE vertices vertexid integer PRIMARY KEY properties json CREATE TABLE edges edgeid integer PRIMARY KEY tailvertex integer REFERENCES vertices vertexid headvertex integer REFERENCES vertices vertexid label text properties json CREATE INDEX edgetails ON edges tailvertex CREATE INDEX edgeheads ON edges headvertex any can connect with any with the indexes we can efficiently find head and tail edges of a given vertex thus traversing the graph we then insert entries like in Cypher graph query language CREATE USAlocation nameUnited States typecountry Europelocation nameEurope typecontinent Francelocation nameFrance typecountry Lucyperson nameLucy France WITHIN Europe Lucy BORNIN USA Lucy LIVEIN France and the query of all people emigrated to Europe from the US looks like MATCH person BORNIN WITHIN0 uslocation nameUnited States person LIVEIN WITHIN0 eulocation nameEurope RETURN personname find any vertex call it person that meets both conditions has an outgoing BORNIN edge to some vertex from there you can follow any number of WITHIN edges until reaching a node of type location whose name property is United States Similar for the Europe analysis the query optimizer then chooses the optimal way to execute this query from all persons or the two regions eg depending on where you have index Although its possible to put a graph database in a relational database supporting queries in such can be difficult as the number of joins is not known beforehand SQL supports WITH RECURSIVE but is very clumsy Triplestore graph model all information stored in threepart statement subject predicate object where an object can be a value or another node In case of value this means a property in case of another node this means an edge TurtleN3 France a location name France type country Lucy a person name Lucy livein France this can alternatively be expressed in XML Semantic web independent from triplestore proposes a Resource Description Framework under which websites publish data in a consistent format thus allowing different websites to be automatically combined into a web of data like a graph database of the entire web SPARQL query language operates on tripstores using the RDF data model Cyphers pattern matching above is borrowed from SPARQL difference being triplestore does not differentiate properties and edges thus both can be queried using the same syntax Are graph databases the second coming of network model CODASYL No CODASYL schema specifies what record types can be nested within which other record types Graph databases allow any vertex to connect to any other In CODASYL the only way to reach a node is via an access path graph database allows query by index on uniqueid as well CODASYL children of a record are an ordered set graph databases have no such constraint on nodes and edges In CODASYL all queries are imperative and easily broken by change in schema Cypher or SPARQL queries are highlevel Datalog Foundation for later query languages Similar data model as triple store generalized to predicatesubject object We define rules that depend on other rules or themselves Datalog Prolog withinrecursiveLocation Name nameLocation Name withinrecursiveLocation Name withinLocation via withinrecursivevia Name migratedName BornIn LivingIn nameperson Name borninperson bornloc withinrecursivebornloc BornIn liveinperson liveloc withinrecursiveliveloc LivingIn migratedWho United States Europe The Datalog approach is powerful in that rules can be combined and reused in different queries Summary Data models Hierarchical historically relational came along to tackle manytomany relation Recent applications whose data model does not fit either NoSQL mostly diverged in document and graph databases These dont usually enforce a schema on data they write but has assumptions about data when reading Each model comes with their own sets of query languages SQL MapReduce Cypher Datalog MongoDBs pipeline etc Relational document and graph are all widely used today and one model can be emulated using another though the result is often awkward Some ongoing research about data models try to find suitable ones for sequence similarity searches genome data PBs amount of data particle physics and fulltext search indexes Chap 3 Storage and retrieval Database is all about storage and query Logbased storage engines an appendonly sequence of records and pageoriented storage engines An index is an additional structure that is derived from the primary data Adding and removing indexes does not affect the contents of the database only performance of queries Any kind of index usually slows down writes because the index also needs to be updated onwrite Thus databases typically let the application developer choose what to index on since they know the access pattern of the database best Logstructured indexes Inmemory hash index A simple approach could be a logstructured storage where each record is and a hash index is stored in main memory Bitcask is one example does this and is well suited for cases where key cardinality is low fits in memory but updated often hence requiring fast writes To avoid only growing disk usage we can chunk the log into segments of a certain size and when closing off on writing one segment we perform compaction where we only keep the most recent update to each key We can also merge several segments together when performing compaction While compaction is being performed we can continue to serve read and write requests using the old segment files compaction never modifies an existing segment files the result is written to a new segment files After compaction is done we switch read and write requests to using the newly produced segment file and delete the old segments With a hash index each segment now needs its own hashmap index in memory and when looking for a key we first search in the hashmap of the most recent segment and if not found we go to the hashmap of the next most recent segment We usually store the log in binary format append a special tombstone to indicate deletion of a record To recover from a crash one could read all the segments to repopulate the hashmap index which is slow Bitcask stores a snapshot of each segments hashmap on disk which can be loaded into memory for faster recovery To handle partially written records eg crash while writing Bitcask uses a checksum For concurrency control we can allow only one writer thread to append to the current segment Multiple threads can read at the same time Appendonly log may seem wasteful but as opposed to updating the record appendonly uses sequential write operations which are generally much faster than random writes especially on magnetic spinning disk hard drives Concurrency and crash recovery are also made much simpler if segment files are appendonly or immutable Merging old segments avoids the problem of data files getting fragmented over time Hashbased index should fit in memory you could maintain a hashmap on disk but ondisk hashmap query requires a lot of random access IO is expensive to grow when it becomes full and hash collisions require fiddly logic Hashbased index is also efficient when doing range queries SSTables and LSM tree Previously our stored records key value pairs appear in the sequence they are written SSTable Sorted String Table require that the sequence of key value pairs is sorted by key SSTables has several advantages over log segments with hash indexes Merging segments is simple and efficient even for file bigger than the available memory starting at the head of all segments each time copy over the lowest orderedkey over to the new file This produces a merged segment file also sorted by key If the same key appears in multiple input segments we only need to keep the value from the most recent segment To find a particular key we dont need to keep an index of all the keys in memory Instead you can have an inmemory sparse index to tell you the offsets of some keys and you can scan a range of two offsets to find if the key you are looking for is in Blocks of entries between every two sparse indices can be compressed which reduces IO bandwidth Constructing and maintaining SSTables We maintain an inmemory stored structure memtable say a redblack tree and write requests gets added to this tree When tree gets big enough we flush it into disk While this tree is being flushed we can keep writing to a new memtable When querying we first look inside the memtable then the most recent ondisk segment then the next recent etc From time to time run merge and compaction to combine segment files and to discard overwritten or deleted values To recover from memtable crashes as we write to memtable we also write to a log This log doesnt need to be sorted by key as all it serves is crash recovery and can be discarded when a memtable gets flushed to disk LevelDB and RocksDB use the algorithm described above Similar storage engines are used in Cassandra and HBase both were inspired by Bigtable paper which introduced the terms SSTable and memtable Lucene indexing engine for fulltext search used by Elasticsearch and Solr uses a similar method for storing its term dictionary This merging and compacting indexing structure originally builds upon Log Structured Merge trees which was built upon earlier work on logstructured file systems Optimization Query can be slow first memtable then segmentbysegment lookups We can add a bloom filter to definitively tell if a key does not exist Sizetiered and leveltiered compaction RocksDB and LevelDB use leveltier where key range is split into smaller SSTables and older data is moved into separate levels HBase uses sizetier where newer and smaller SSTables get merged into older and larger ones Cassandra supports both The basic idea of LSMtrees keeping a cascade of SSTables that are merged in the background is simple and effective It scales well when data is much bigger than memory supports range query well and because all disk writes are sequential the LSMtree can support remarkably high write throughput Btree indexes The most widely used indexing structure is Btree They remain the standard implementation in almost all relational databases and many nonrelational databases use them too Similar as SSTables Btree also sorts by key but thats where the similarity ends Logstructured indexes break the database down into variablesize segments and always write a segment sequentially while Btrees break the database down to fixedsize blocks or pages traditionally 4KB in size and read write one page at a time This corresponds closely to the underlying hardware One page is designated as root of the tree root points to child pages where each child is responsible for a continuous range of keys A leaf page can either contain the key value inline or contains references to pages where the values can be found Typically branching factor of a Btree is several hundred To update an existing value for an existing key find the leaf page change the page and write the page back to disk Adding a new key may split an existing page into two The split algorithm keeps the Btree balanced Most databases can fit into a Btree that is three or four levels deep 4KB pages 4 levels branching factor of 500 can store up to 256TB The basic write operation of a Btree is to overwrite a page on disk with new data and it is assumed that the overwrite does not change the location of the page ie all references to the page remain intact when the page is overwritten This is in stark contrast with LSMtrees where files are appendonly and deleted but never modified inplace A write causing a split will cause two children pages and parent page to be overwritten This is a dangerous operation as a crash after some pages have been written leaves you with a corrupted index In order to make Btree resilient to crashes it includes a writeahead log an appendonly file to which every Btree modification must be written before it can be applied to the pages of the trees itself This log is used to restore Btree to a consistent state after crash Concurrency control is also more complicated a reader may see a tree in an inconsistent state without concurrency control Btrees typically use latches lightweight locks Logstructured approaches are simpler in this regard as all merging are done in the background without interfering with incoming queries and atomically swap old segments for new segments from time to time Btree optimizations have been introduced over the years eg copyonwrite pages where a modified page is written to a different location and a new version of the parent pages in the tree is created pointing at the new location Abbreviating keys in interior pages Optimized storage layout such that leaf pages appear in sequential order on disk Adding additional pointers such as left and right siblings Fractal trees borrow some logstructured ideas to reduce disk seeks Comparison Btree and LSMtree As a rule of thumb LSMtrees are typically faster in write whereas Btrees are thought to be faster in reads LSMtrees are thought as slow in this regard as potentially several SSTables at different stages of compression has to be checked However benchmarks are generally inconclusive and sensitive to the details of workload Write amplification one write to the database would result in multiple writes to the disk over te course of the datas lifetime Eg compaction in LSM trees Btrees overwriting an entire page even if only a few bytes changed This is a concern for SSD as blocks can only be overwritten a limited number of times before wearing out In highwriteworkload scenarios bottleneck might be the rate at which the database can write to disk in which case write amplification directly affects performance LSMtrees can typically sustain higher write workload than Btrees because of lower write amplification depends on workload and storage engine configuration and also writing compact SSTable structures sequentially as opposed to overwriting several pages in the tree This is particularly important for magnetic hard drives where random writes are far slower than sequential writes LSMtrees typically have lower storage overhead Btree leaves some disk space unused due to fragmentation eg when splitting LSM trees are not page based and periodically compact SSTables to remove fragmentation giving them typically lower storage overhead Some SSDs internally use a logstructured algorithm to turn random writes into sequential writes hence nullifying the downsides of Btrees random writes but LSMs typically lower write amplification and reduced fragmentation still matters for throughput One downside of LSM tree is expensive compactions affecting the performance of readwrites as a user request may need to wait for disk to finish a compaction The impact on throughput and response is usually small but at higher percentiles the response time of LSMtrees can be less predictable than that of Btrees Also as compaction threads and the logging flushing memtable to disk thread shares the write bandwith the larger the database gets the more write bandwidth compaction threads might use If compaction cannot keep up with new writes tbe number of unmerged segments grow until you run out of disk space reads would become slow as well This is a situation you want to monitor Btrees also have a key at one specific location only while LSM trees can have the same key stored in multiple places The former also made offering strong transactional semantics easier as what they can do is to lock the key range and attach those locks directly to the tree Btrees are very ingrained in the database architecture of todays and arent going away any time soon LSMtrees are getting popular but you should assess given your workload to decide which is more suitable Other indexes Seconary indexes The above discusses key value indexes like a primary key in the relational model unique identifier Secondary indexes are very common not unique and both LSMtrees and Btrees can be used as secondary indexes When storing records for each key we can store the value itself or a reference to somewhere else known as a heap file which uses no particular order can be appendonly or keep track of deleted entries and overwrite them The heap file approach is common to deduplicate the actual data When not changing the key overwriting with a smaller value is easy but overwriting with a larger value will cause the value to be relocated thus needing to update all references as well or leave a forwarding pointer in its old heap location Sometimes the extra hop from index to heap file is too much of a performance penalty for reads so it can be desirable to store the indexed row directly within an index This is called a clustered index which the primary key of a table in MySQLs InnoDB storage engine is always a clustered index and secondary indexes refer to the primary key rather than a heap file location A compromise between a clustered index and a nonclustered storing only references to the data within the index is a covering index where some of a tables columns are stored with the index allowing some queries to be answered by using the index alone Covering and clustered index can speed up reads but introduce extra storage and overhead to writes Transactional guarantee also becomes harder because of duplication Multicolumn indexes Multicolumn index can be a concatenated index where its concatenating a few keys and it would allow querying by a number of prefix keys of the concatenated key Eg to support 2D geospatial data search one option is to translate the 2 dimensions into a single number using a spacefilling curve then use a Btree or more commonly specialized indexes such as Rtrees are used Another case is when needing to support filtering by multiple columns at the same time Fulltext searches and fuzzy indexes Fuzzy search within a certain edit distance ignore the grammatical variations of the searched keyword Lucene supports such and internally it uses an SSTablelike structure where the inmemory index is a finite state automaton over the characters in the keys similar to a trie This automaton can then be transformed to a Levenshtein automaton which supports efficient search for words within a given edit distance Keeping everything in memory Compared with memory disks are awkward to deal with you have to lay out data carefully if you want good performance on reads and writes With memory getting cheaper keeping entire databases in memory becomes possible memcached inmemory caching provides a solution when data durability is not required When durability is required an inmemory database can write a log to disk writing peiodic snapshots to disk or replicating the inmemory state to other machines Despite the disk interaction its still considered an inmemory database because disk is only used as an appendonly log for durability and reads are served entirely from memory Writing to disk also has operational advantages where its easier to backed up inspected and analyzed by external utilities Redis provides weak durability by writing to disk asynchronously Counterintuitively the performance advantage of inmemory databases is not due to the fact they dont need to read from disks Even a diskbased storage engine may never need to read from disk if you have enough memory as the OS caches recently used disk blocks anyway Rather they can be faster as they avoid the overheads of encoding inmemory data structures in a form that can be written to disk Inmemory databases also provides data models that are difficult to implement with diskbased indexes eg Redis offers a DBlike interface to various data structures such as priority queues and sets Keeping all data in memory makes its implementation comparatively simple Inmemory databases can store data larger than memory without bringing back the overheads of using a disk the anticaching approach works by evicting the least recently used data to disk when there is not enough memory and loading it back in when accessed This is similar to what OS does with swaps and virtual memory but with more granularity eg individual records as opposed to memory pages hence more efficient than what the OS does This approach still requires the entire index to fit in memory Transaction processing or analytics The word transaction traces back to databases early days for recording money changing hands now it refers to a group of reads and writes that form a logical unit A transaction neednt necessarily have ACID atomicity consistency isolation and durability properties transaction processing just means allowing clients to make lowlatency reads and writes as opposed to batch processing jobs which run only periodically Over time we see two major query patterns for databases look up a small number of records by some key using an index Records are then inserted or updated based on the users input These application are usually interactive and became known as online transaction processing OLTP scan over a huge number of records reading only a few columns per record and calculates aggregate statistics sum avg etc rather than returning the raw data to user These are known as online analytics processing OLAP Relational DBs started out working fine for both OLTP and OLAP over time some companies switched over to data warehouse for their OLAP workload In some setups the OLTP systems being latency sensitive and mission critical does not serve analytics requests a readonly copy of data extracted from the OLTP system transformed into an analysis friendly schema and cleaned up is loaded into the data warehouse This process of loading OLTP data into OLAP warehouse is known as ExtractTransformLoad ETL Having a separate OLAP system allows optimization specific to its query pattern The data model of a data warehouse is most commonly relational as SQL is generally a good fit for analytics queries MS SQL Server supports transaction processing and data warehousing in the same product however they are becoming increasingly two separate storage and query engines served through the same SQL interface Open source SQLonHadoop projects fall into the same data warehouse category Apache Hive Spark SQL Cloudera Impala Facebook Presto Apache Tajo and Apache Drill Some of them are based on Google Dremel Data warehouses dont see a variety of data models as transaction processing DBs do Most follow a starschema where a giant central table facts table records events and foreignkey references to other tables dimension tables to eg normalize dimension tables would record the who what where how and why of an event This is like a star where facts table sits in the center and connects to peripheral dimension tables Snowflake schema is a variation of the star where dimensions are further broken down into subdimensions They are more normalized but harder to work with Columnoriented storage If you have trillion of rows in your facts table storing and querying them efficiently becomes a problem Fact tables are always over 100 columns wide but one query rarely accesses more than 4 or 5 of them at the same time OLTP databases including documentbased ones usually organize one row document as a contiguous sequence of bytes columnoriented storage instead stores all the values from each column together Eg each column of a facts table gets stored in its own file such that when only accessing a few columns in a query we only read those files as opposed to reading all the columns then filter Columnoriented storage layout requires each column file containing rows in the same order Columnoriented storage often offers great opportunities for compression bitmap encoding is often used The cardinality of the set of distinct values in a column is often small compared with the number of rows We then use a bitmap to represent the set of distinct values in a column eg 200 countries in the world 25B to store and each distinct column value X would then correspond with a series of bits where we have 1 bit for each column and wed have 1 if that column is X and 0 if not For each column value X wed then end up with a series of bits that are predominantly 0 and we can then apply runlength encoding 100 zeroes followed by 2 ones to further compress This storage schema also makes filtering by a few column values easier eg we apply a bitwiseor over all the series of bits of those column values and return the selected columns Cassandra and HBase offer columnfamilies which they inherited from BigTable Those would still be rowbased storage as within each column family they store all columns from a row together along with the row key and they dont use column compression Hence the BigTable model is still mostly roworiented Columnoriented layouts also are good for making efficient uses of CPU cycles CPU loads one L1cacheful of compressed column data does some bitwise andor without function calls iterate through the data in a tight loop on the compressed data directly singleinstructionmultidataSIMD instructions on modern CPUs This leverages vectorized processing In a column store it might be easiest to store data in the order they come in as then insertion becomes simple append its also possible to impose an order as in an SSTable note that fields of the same record needs to remain in the same kth record in every column data file Sorting would also help with compression especially for the first sort key Vertica sorts a columnoriented storage in several ways the data needs to have multiple copies anyway so why not sort them in different orders to answer different kinds of queries Writing to columnoriented storage can be tricky as insertion in the middle requires rewriting all column data files LSMtrees dont have this constraint column or row based when enough writes have accumulated they are merged with the column files on disk and written to new files Data cubes and materialized views Not every data warehouse is columnar if queries often involve count sum avg etc we could cache some of the counts or sums that queries use most often A view is often defined as the resulting table of some query A materialized view is an actual copy of the query result written to disk a denormalized copy of the data matching some conditions When the underlying data changes the materialized view needs to be updated as well They make writes more expensive which is why materialized view is not often seen in OLTP databases A materialized data cube is a denormalized and highdimensional materialized view with some aggregation statistics such that particular queries on those statistics are faster Summary How databases handle storage and retrieval internally OLTP transaction processing workload request volume is large each touches few records usually via some key index expect lowlatency in response Disk seek time is often the bottleneck here Storage LSMtrees appendonly immutable data files SSTables merge systematically turn randomaccess writes to sequenetial writes enabling higher write throughput Btrees overwrite fixed sized pages inplace Indexing multiindex inmemory database OLAP analytics workload data warehouse lower volume queries needing to read a large number of records Disk bandwidth is bottleneck Columnoriented storage is increasingly popular for this Indexing are less relevant instead compression becomes important Chap 4 Encoding and Evolution Application change Evolvability is important Server software usually goes through staged rollout client software upgrade are at the mercy of clients Coexistence of old and new code makes backward and forward compatibility important new being able to work with old old being able to work with new Forward compatibility is usually trickier as it requires old code to ignore additions new code did Programs work with inmemory data structures as well as serialized encoded data when needing to transfer over the network maybe with the exception of memory mapped files Languagespecific serialization formats javaioSerializable pickle etc These are usually easy to code but they may be specific to that language in order to restore data in the same object types the decoder needs to able to instantiate arbitrary classes which is a source of security problems versioning is often an after thought for these utilities efficiency is often an after thought These encodings can be very bloated CPUinefficient to use For these reasons its generally a bad idea to use the languages builtin serialization library for anything other than very transient purposes JSON XML and binary variants JSON XML CSV are all textual formats somewhat humanreadable and widely supported encodings Some subtle problems besides superficial syntactic issues ambiguity around encoding of numbers XML and CSV cannot differentiate a string of numbers and a number without referencing external schema JSON can but does not distinguish floats and integers and doesnt specify a floating point precision An integer larger than 253 cannot be exactly represented by IEEE 754 double precision float Consequently JSON returned by Twitters API includes tweet IDs twice once as JSON number and once as decimal string to work around the fact that number this large may not be correctly parsed by JS applications JSON and XML have good support for unicode character strings but not binary strings People get around this limitation by using base64 to encode the binary data and this increases size JSON and XML both have optional schema support XML schema is widely used JSON not as much In cases where a schema is not used the decoding application potentially needs to hardcode the appropriate encoding decoding logic CSV does not have schema each column is up for interpretation Its also vague eg what happens if a value contains a comma Escaping has been formally specified but all parsers support them correctly Despite these flaws these encodings will likely remain popular as data interchange formats Binary encoding has been developed for JSON and XML since they dont prescribe a schema they still need to include all the field names within encoded data Thrift and protobuf Apache Thrift and protobuf are binary encoding libraries that are based on the same principle Both require a schema in an interface definition language IDL and come with a code generation tool that takes the schema and produces code in various languages that implement it One big difference with binary encoding of JSONXML is field names are not present in encoded data instead field tags numbers are used like normalization with a schema definition Protobuf binary encoding uses variable length integers and encodes very similarly to Thrift CompactProtocol Thrift also has a BinaryProtocol which does not use variable length integer Encoding is TLV Handling schema evolution Add new fields to the schema provided that you give each field a new tag number Forward compatibility old code not recognizing the tag number can just ignore it Backward compatibility new code can still read old messages since tag number doesnt change the only detail is that when adding a new field you cannot mark it required they must be optional or have a default value Removing is like adding with backward and forward compatibility concerns reversed you cannot remove a field that is required and you can never use the same tag again Changing data types of fields may be possible data can lose precision or become truncated One peculiarity in Protobuf there is no explicit array but instead a repeated marker a third option along with required and optional meaning something can appear 0 to N times exactly 1 time or 0 or 1 times Avro Avro is another encoding format different from Protobuf and Thrift it is developed since Thrift was deemed not a good fit for Hadoops use cases Avro has an IDL to describe schema The peculiarity is in Avro having no field tags or type indication in the encoded data a string or an integer is a length prefix data UTF8 or variable length integer encoding Being able to decode relies on going through the fields in the order that they appear in the schema meaning the decoder can work only if using the exact same schema as encoder In order to support schema evolution the writers schema and readers schema dont have to be the same When decoding Avro resolves difference by looking at writers schema and readers schema sidebyside and translating the writers schema into readers schema Field reordering can be reconciled fields in writers schema but not readers will be ignored by decoder and field in readers but not writers will be filled with default values in readers schema To maintain and backwards and forwards compatibility you can then only add or remove fields with a default value Avro doesnt have optional required marker as protobuf and thrift do it has default values and union types instead where allowing a field to be null requires including null in a union type In the context of Hadoop Avro is often used for storing a large file of millions of records all encoded with the same schema Hence the overhead of including that schema with the file is not huge In a database where records are written at different times with different schema Avro keeps a versioned schema in a schema database When sending data over the network Avro RPC protocol negotiates a shared schema between two parties Why might this be preferable to Protobuf and Thrifts schema Not having tag numbers makes Avro friendlier to dynamically generated schemas Eg in a case where you want to encode a table in a relational database exporting it to Avro Protobuf Thrift and then the table schema changes when exporting the newer version Protobuf Thrift versions have to be careful about field tag while Avro has no such concern Code generation is often useful for statically typed languages where the generated code allows type checking while in dynamically typed language code generation is often frowned upon Protobuf Thrift and Avro schemas are simpler than XMLJSON schemas as the latter support more detailed validation rule like regexp integer ranger etc Pro of schemas These encodings are based on the idea introduced in ASN1 used to define various network protocols and its binary encoding DER is still used to encode SSL certificates X509 Most relational database vendors also have their own proprietary binary encoding for their query protocol over the network the database vendors usually then provides a driver using the ODBC or JDBC APIs that decodes data over the network Binary encodings based on a schemas are viable compared to textual formats like JSONXML in particular binary encoding is more compact omitting field names schema is a good form of documentation keeping a database of schemas allows checking backward and forward compatible changes and in statically typed languages generating code from schema allows compile time type checking Modes of dataflow Usually data flows via databases service calls asynchronous message passing Via databases Be mindful of forward and backward compatibility forward compatibility and when writer of an old version reads data written by a new version writes on top of that and being able to save that data back Data outlives code upgrading server software is fast but data written long ago will still be there Most databases avoid migration if possible due to its being expensive Archiving snapshotting data usually uses the latest schema and Avro object container files are a good fit or in columnoriented analytical format Via services Server defines an API clients request data from it The API is called a service The approach is usually to decompose a larger application into smaller services by area of functionality called a serviceoriented architecture microservices architecture Key design goal being to make the application easy to change and maintain by making services independently deployable and evolvable A web service is where HTTP is used as the underlying protocol of talking to the service REST and SOAP are two popular approaches to web services They are diametrically opposed in terms of philosophy REST is not a protocol but a design philosophy that builds upon the principles of HTTP where it uses simple data formats URLs for identifying resources HTTP features for cache control authentication and content type negotiation An API designed with such in mind is called RESTful SOAP is an XMLbased protocol for making network API requests Although most commonly used over HTTP it aims to be independent from HTTP and avoids using most HTTP features API of a SOAP web service is described using Web Services Description Language WSDL an XMLbased language WSDL is not designed to be humanreadable users of SOAP rely heavily on tool support and code generation Web services are the latest incarnation of a long line of technologies for making API requests over a network based off the ideas of RPC whuch tries to make a request to a remote service look the same as calling a function or method in your programming language This may be fundamentally flawed in the sense that a local function call is predictable and either succeeds or fails depending only on the parameters that are under your control A network request is unpredictable a local function call either returns a result or throws an exception or never returns a network request can have more outcome like returning without a result due to a timeout retrying a call that didnt respond in time may actually cause the action to happen twice in the remote in which case idempotence needs to be built in Local calls dont have this problem local function call times are more predictable while network delays can vary wildly a local function call can take in references efficiently network call requires all actual data to be sent over the network the client and service may be implemented in different languages so RPC framework has to translate datatypes from one language into another not a problem for local function calls REST does not try to hide the fact that it goes through network RPC libraries can be built using REST With these flaws RPC isnt going away in the short term gRPC is an RPC implementation using protobuf Thrift and Avro come with RPC support The new generation of RPC framework is more explicit about a remote request being different from a local call eg they may use futures promises to encapsulate asynchronous actions that may fail gRPC also supports streams with a call consisting of more than one request and response Dataflow through services can be easier to involve than dataflow via database you can assume servers are always upgraded first and clients after them Hence responses need to be forward compatible and requests need to be backward compatible Forward and backward compatibility properties of an RPC scheme are usually inherited from the encoding format they use Thrift gRPC Avro RPC SOAPs XML schemas RESTful API usually uses JSON without a formally specified schema where adding optional request param and adding to response are usually changes that maintain compatibility Messagepassing dataflow Asynchronous messagepassing systems are somewhere between RPC and databases A clients request message is delivered to another process by going through an intermediary called a message broker message queue or messageoriented middleware Compared with RPC a message broker can act as a buffer if the recipient is unavailable or overloaded thus improving reliability can automatically redeliver messages to a process that has crashed preventing messages being lost avoids the sender needing to know the IP address and port number of the recipient allows one message to be sent to multiple recipients logically decouples the sender from the recipient However a difference compared with RPC is that messagepassing is usually oneway a sender does not expect to receive a reply A reply is possible via a different channel and this communication pattern is asynchronous in that the sender doesnt wait for the message to be delivered but simply sends it and then forgets it Recently RabbitMQ Apache Kafka etc have become popular message brokers In general message brokers are used as follows one process sends a message to a named queue or topic and the broker ensures the message is delivered to one or more consumers of or subscribers to the topic There can be many producers and many consumers on the same topic Message brokers typically dont enforce any particular data model and you can use encoding format If the encoding is backward and forward compatible you have the flexibility to change producers and consumers independently and deploy them in any order The actor model is a programming model for concurrency in a single process Rather than dealing directly with threads race conditions locking etc logic is encapsulated in actors Each actor has some local states and communicates with other actors by sending and receiving asynchronous messages Message delivery is not guaranteed Distributed actor model uses this programming model on different nodes as there is less of a fundamental mismatch between local and remote communication when using the actor model This model essentially integrates a message broker and the actor programming model Some distributed actor model frameworks are Akka Javas builtin serialization Orleans Erlang OTP Summary Encoding their efficiency and evolvability impact Programming specificspecific encodings Textual formats like JSONXMLCSV Optional schema Can be vague about types Binary schemadriven formats like Thrift Protobuf Avro Modes of dataflow Via databases Writer process encodes reader process decodes Via RPC and RESTful APIs SOAP Client encodes request decodes response and the opposite for server Via asynchronous message passing using message brokers or actors Backwardforward compatibility and rolling upgrade are achievable with a bit care Deployment should be incremental and frequent Part II Distributed data Part I deals with a single node Part II deals with multiple where we may gain in scalability fault tolerance high availability and latency Problem with scaling vertically communication via shared memory using a more powerful machine is cost grows nonlinearly Fault tolerance and latency are also issues with a single powerful node Another approach is a shared disk architecture but contention and overhead of locking limit its scalability Share nothing architecture horizontally scaling are not necessarily the best solution for everything while having advantages in cost of scaling high availability and low latency distributed to near where clients are they usually add complexity for applications and sometimes limits the expressiveness of data model Two common ways of distributing data across nodes Replication Same copy of data in different locations Provides redundancy and helps improve performance Partitioning sharding Split a big dataset into smaller ones to be stored separately Chap 5 Replication Reasons for replication keep your data geographically close to your users allow the system to continue working even part of it have failed to scale out the number of machines that can serve read queries thus increase read throughput This chapter assumes your dataset is small enough to live in one machine The complexity in replication lies in handling changes to replicated data Algorithms for replicating changes across nodes singleleader multileader and leaderless Leaders and followers To ensure change gets to all replicas the most common solution is leaderbased replication active passive master slave replication One of the replicas is the leader master primary all write requests must go through the leader which first writes the new data to its local storage Whenever leader writes to its local storage it sends the change to all of its followers read replicas slaves secondaries hot standbys as a part of replication log or change stream Each follower updates accordingly by applying the changes in the same order as they were processed by the leader a client read can be handled by the leader or any of the followers This mode of replication is builtin for many relational DBs MySQL PostgreSQL etc and some nonrelational DBs MongoDB Espresso etc This is not limited to distributed DBs Kafka and RabbitMQs high availability queues also use it Synchronous and asynchronous replication Synchronous the leader wait for a follower to confirm it received the write before reporting success to the user and before making the write visible to other clients Advantage the follower is guaranteed to have an uptodate copy of the data that is consistent with leader If leader suddenly fails data is still available on the follower Disadvantage the write cannot be processed if the follower does not respond and the leader has to block all writes Its impractical for all followers to be synchronous Asynchronous leader does not wait for follower response before telling the user Usually if you enable synchronous replication on database its semisynchronous where one of the followers receive synchronous updates and all others async If the synchronous followers becomes slow one of the async followers is made sync This guarantees uptodate data on at least two nodes In practice leaderbased replication is often configured to be full async where a write even if confirmed by the leader are not guaranteed to be durable Setting up new followers To spin up a new follower full copy usually doesnt work as data is being written as we copy while locking the DB means lowering availability Usually we take a snapshot of the leaders DB at some point copy this snapshot over and the new follower then requests all the changes that have happened since the snapshots taken until it fully catches up This requires the snapshot being assoicated with an exact location in leaders replication log known as log sequence number binlog coordinates Handling node outages Followerfailure catchup each follower keeps a log of data changes it has received from leader When a follower crashes it picks up the last processed transaction from this log and request all the data changes since that Leaderfailure failover one of the followers needs to be promoted leader and clients need to be reconfigured to send their writes to the new leader and other followers need to start consuming data changes from the new leader Determining leader failure usually relies on heartbeat Electing a new leader is a consensus problem Usually the replica with most uptodate data changes from the leader is chosen to minimize data loss Reconfiguring the system to follow the new leader and if old leader comes back ensure it becomes a follower to the new leader Failover is fraught with things that can go wrong With asynchronous replication there can be writes from the leader before it failed that are not in the new leader We could discard those unreplicated writes but this would violate clients durability assumption Discarding writes is especially dangerous if other storage systems outside of the DB need to be coordinated with database content In some fault scenarios it could happen that two nodes believe they are the leader As a safety catch some systems shut one down in this case these need to be carefully designed as well so as to not shut both down deciding the right heartbeat timeout for a leader to be considered dead Implementation of replication logs Statementbased replication In a relational system each INSERT UPDATE or DELETE is forwarded to followers This has problems with calling nondeterministic function like NOW and RAND if statements depend on existing data in the column they must be executed in exactly the same order on each replica statements with side effects triggers stored procedures userdefined functions may result in different side effects on different replica unless side effects are deterministic there are workarounds but other replication methods are usually preferred Writeahead log shipping in LSM trees this log is the main place for storage in Btrees this is the writeahead log for restoring to a consistent state after a crash we can use the same log for replication leader appends to its own log and also sends it across the network to followers main disadvantage is log describes data on a very low level like which bytes were changed in which disk blocks this makes replication closely coupled to the storage engine Its typically not possible to run different versions of the database software on the leader and followers this advantage has a big operational impact upgrade requires downtime If this allows followers to run a newer version than the leader then zero downtime upgrade can be achieved by upgrading some followers then perform a failover Logical rowbased replication uses a different log format for replication than the log of the storage engine The formers a logical log while the latters a physical log logical log for a relational DB is usually a sequence of records describing writes to tables at the granularity of a row insert contains new values of all columns delete cotntains the primary key or if no primary key old values of all columns update contains the primary key or enough info to uniquely identify a row and the new values of all columns a transaction that modifies several rows generates several such records followed by a record to indicate a commit MySQL binlog uses such this higherlevel log allows leader and follower to run different storage engines this log is easier for external applications to parse eg transcribing data to a warehouse or for building caches custom indexes This is called change data capture Triggerbased replication the above are replications implemented by the database system Sometimes you want more flexibility as an application to eg replicate a subset of the data Then replication then may be moved up to application level or use triggers and stored procedures available in many relational DBs which lets you register custom application code executed automatically when data changes This usually has more overhead is more errorprone but more flexible Problems with replication lag Replication helps with availability read throughput and latency In a system with few writes and lots of reads one leader and many followers may seem ideal though this system can only replicate asynchronously meaning a client reading from an asynchronous follower may see outofdate data reading at the same time from leader and a follower may give different results transiently Without writes eventually they should converge and this is known as eventual consistency where eventual is vague and can be arbitrarily long Read your own writes Readyourwriteconsistency readafterwriteconsistency is a guarantee that user will always see any updates they submitted themselves To implement readafterwriteconsistency we could when reading something that the user might have modified read it from leader otherwise read from follower This requires your system to be able to identify what the user can modify eg profile of that user on a social media page or track the time of last update and for a certain time based on replication lag after the last update make all reads from the leader or client can remember the timestamp of its most recent write then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp and switch to a different replica or wait if that replica does not have this timestamp This can be a logical timestamp that indicates the ordering of writes or system clock where clock synchronization becomes critical Another complication arises when the same user is accessing your service from multiple devices Timestamp remembering becomes more difficult this metadata needs to be centralized and known across devices If your replicas are distributed across different data centers the users multiple devices may connect to different data centers If your approach requires reading from the leader you may first need to route requests from all of a users devices to the same data center Monotonic reads Another anomaly with asynchronous replication is user can see things moving back in time Monotonic reads is a guarantee this does not happen a guarantee stronger than eventual and weaker than strong consistency One way to achieve this is make the user read always from the same replica eg using a hash of the UserID rather than randomly However this needs to handle rerouting when that replica fails Consistent prefix reads A third anomaly with asynchronous replication is violation of causality Consistent prefix reads is a guarantee this does not happen which says if a sequence of writes happens in a certain order then anyone reading those writes will see them appear in the same order This is a particular problem in sharded databases as if the database always applies writes in the same order reads always read a consistent prefix and this cannot happen But in many distributed databases different partitions operate independently so there is no global ordering of writes One solution is to make sure any writes causally related to each other are written to the same partition but in some applications that cannot be done efficiently When working with an eventually consistent system its important to consider what if replication lag gets long and what kind of consistency guarantee you need to provide to users Application code can manually perform some operations on the leader to provide a stronger guarantee than the underlying DB but this is error prone and complex and itd be ideal if application doesnt need to worry about consistency and can trust their DB to do the right thing This is why transactions exist they provide stronger guarantees from the database such that applications can be simpler When moving away from a single node to multi node many dropped transactions claiming they are too expensive or hurts availability too much and eventual consistency is the only guarantee This is true to some extent but an oversimplified statement Multileader replication A multileader configuration mastermaster activeactive replication allows more than one node to accept writes Each leader in this setup simultaneously acts as a follower to other leaders It rarely makes sense to use a multileader setup in a single datacenter as the benefits rarely outweighs the added complexity With multiple data centers this can serve as an alternative to having all your writes go through a leader in one data center Perceived performance may be better in this case as user writes can be handled by the data center closest to them and the interdatacenter network delay is hidden from them Having all writes go through one node may defeat the purpose of multidata center This can tolerate data center outage This is more tolerant of network problems as interdata center connections usually go through the Internet and is much more error prone than local network within a data center a temporary network outage does not prevent writes from being made Some databases support multileader configuration by default but it often implemented by external tools over commercials DBs Multileader comes with a big downside of the same data can be concurrently modified in two different data centers and those writes have to have their conflicts resolved Multileader can be quite dangerous due to configuration pitfalls and surprising interaction with other DB features such as autoincrementing keys triggers and integrity constraints Clients with offline operation support is essentially a multileader replication as when offline the clients local DB acts as a leader that can accept writes Realtime collaborative editing such as Google Doc poses a similar problem where the user edits can be written to the local web browser acting as a leader You could make a user wait until another has finished editing holding a lock which would be similar to singleleader replications with transactions on the leader For faster collaboration youll want to make the unit of change very small and avoid locking which brings the challenges of multileader replication including requiring conflict resolution Handling write conflicts Imagine two users modifying the same thing at the same time in a singleleader system the second write ordering is deterministic can be either blocked or rejected while the first is ongoing You could make the replication synchronous to handle this but this breaks the main advantage of multileader replication allowing each replica to accept writes independently Conflict avoidance Best way to deal with conflicts is to avoid them if the application can ensure all writes for a particular record go through the same leader the social media profile example then conflicts cannot occur In some cases you have to change the designated leader for a record due to datacenter failure or user having moved in which case concurrent writes on different leaders needs to be dealt with again Converging towards a consistent state In a multileader setup if each leader were to just apply writes in the order they receive them eventual consistency cannot be guaranteed in that writes can have different orders getting to different leaders Well want a convergent way which can be achieved with give each write a unique ID random number timestamp UUID hash etc and pick the write with the highest ID as the winner If timestamp is used this is known as last write wins a popular approach but prone to data loss give each replica a unique ID and let writes originated at a highernumbered replica always take precedence This also implies data loss somehow merge them eg order them alphabetically then concatenate record the conflict in an explicit data structure that preserves all information and write application code that resolves the conflict at some later time perhaps by prompting the user Custom conflict resolution logic Conflict resolution in multileader systems are often applicationspecific and these systems would let application supply their custom logic for conflict resolution which gets executed either onwrite or onread In latters case all conflicting versions are written and given to the reader the next time they are read the reader resolves automatically manually and the result gets written back Note that conflict resolution usually applies at the level of an individual row or document not for an entire transaction A transaction with several writes usually have each of these writes considered separately for conflict resolution Conflict resolution is errorprone Amazon is often cited as having surprising effects due to this This inspired research in CRDT conflictfree replicated datatypes a family of data structures for sets maps ordered lists counters etc that can be updated concurrently by multiple users and automatically resolve conflicts in reasonable ways Mergeable persistent data structure tracks history explicitly similarly to git and uses a threeway merge function whereas CRDT uses two way merge Operational transformation is the algorithm behind Google docs designed particularly for concurrent editing of an ordered list of items such as the list of characters that constitute a document Multileader replication topologies With more than two leaders various replication topologies are possible circular star alltoall etc In circular and star a replication passes through several nodes before reaching all replicas Nodes forward data and to prevent infinite replication loops each node is given a unique identifier and in the replication log each write is tagged with the identifiers of all nodes it passed through and a node wont apply changes that are already tagged with its own tag Node failure in a circular or star topology interrupting replication flow is also a bigger issue than in a more densely connected topology Alltoall topologies may have issues with some replication messages overtaking others like causality being violated because different paths gets causally related messages over at different times Simply adding a timestamp wont fix it as timestamp cannot be assumed to be in sync all the time To order these correctly versionvector can be used Many multileader replication arent implemented carefully and its worth checking your DBs docs and test to ensure it actually provides the guarantees you believe it to have Leaderless replication Dynamo is an example of this where a client can write to any nodes Riak Cassandra are opensource leaderless replication systems inspired by Dynamo There is no concept of failover in a leaderless replication system The client writes to multiple replicas and as long as enough number of replicas returned success the write is considered successful by the client The client also reads from multiple replicas and in parallel and version numbers are used to decide which value is newer When an unavailable node comes back online to get uptodate data to it we could use read repair where a client makes a read from several nodes in parallel and detect stale response in which case they write the newer value back to that relica This works well for values that are frequently read Many data stores also have an antientropy process that constantly looks for differences in data between replicas and copies over missing data from one replica to another This may have signifcant delays Quorums for reading and writing If there are n replicas every write must be confirmed by w to be considered successful and we must query at least r nodes for each read As long as w r n we expect to get uptodate value when reading because at least one we read from must be up to date Reads and writes that obey this are called quorum reads and writes A typical setup is to let n be an odd number and r w be n2 rounded up When fewer nodes returned success the read or write operation returns error Lowering w r below n will make you likely to read stale values but allows higher availability and lower latency Although quorums appear to guarantee that a read returns the latest written value in practice its not so simple due to edge cases Dynamosstyle databases are generally optimized for use cases that can tolerate eventual consistency and r w shouldnt be taken as guarantees In particular you usually dont get readyourwrites monotonicreads consistentprefixreads as they require transactions or consensus Monitoring staleness For leaderbased replication because leaders and followers apply the write in the same order you can typically monitor the amount of replication lag In leaderless replication there is no fixed order in which writes are applied making monitoring more difficult Eventual consistency is a deliberately vague guarantee but for operability its important to be able to quantify eventual Sloppy quorums and hinted handoff Leaderless databases with appropriately configured quoroms can tolerate failures without failover as well as slowness since as long as w r nodes returned the operation succeeds This makes them appealing for highavailability lowlatency workload and one that can occasionally tolerate stale reads This scheme as described is not tolerant to network partition during which its likely fewer than w or r reachable nodes remain and a quorum cannot be reached Designers face a tradeoff in this case block off all reads writes or let writes proceed anyway to nodes that are reachable but arent among the n nodes on which they value usually lives The latter is sloppy quorum reads and writes still require r and w successful responses but those may include nodes that arent among the designated n home nodes for a value Once the partition is fixed any writes that one node temporarily accepted on behalf of another are sent to the appropriate home nodes this is called hinted handoff Sloppy quorum is particularly useful for increasing write availability the database can accept writes as long as any w nodes are available This also means even when w r n you cannot be sure to read the latest value for a key as the value may have been temporarily written to some nodes outside of n Hence sloppy quorum isnt a quorum but a durability assurance Sloppy quorum are optional in all common Dynamo implementations Riak Cassandra Voldemort Multidata center operation Leaderless replication is also useful for multidatacenter operation since it is designed to handle conflicting concurrent writes network interruptions and latency spikes Cassandra usually have the number of replicas n in all data centers each write is sent to all replica but only w needs to acknowledge and these are usually all from local data centers so the client is unaffected by interdata center links Detecting concurrent writes Dynamostyle DBs allows several clients to write to the same key and conflicts can occur during writes read repair or hinted handoff If each node simply overwrote the value for a key whenever it received a write request from a client we could end up in an eventually inconsistent state The replicas should converge towards some value and if you as the application developer want to avoid losing data you usually need to know a lot about the internals of your databases conflict handling Last write wins Concurrent writes dont have a natural ordering so we force arbitrary orders on them eg attach a timestamp to each write and pick the biggest timestamp as the most recent This conflict resolution LWW is the only support resolution in Cassandra LWW achieves eventual convergence but at the cost of durability It may even drop writes that are not concurrent due to time drift If losing data is unacceptable LWW is a poor mechanism for conflict resolution the only safe way to use LWW is to ensure a key is written only once and immutable thereafter Eg a recommended way of using Cassandra is to use a UUID as the key thus giving each write operation a unique key Happensbefore relationship and concurrency How to decide if two writes are concurrent an operation A happens before another operation B if B knows about A or depends on A or builds upon A in some way Whether one operation happens before another is the key to defining what concurrency means Two can be said to be concurrent if neither knows about the other Note that concurrent here does not necessarily mean happening at the same physical time as with clocks in distributed system its usually quite difficult to tell two things happening at the same time From a physics perspective if information cannot travel faster than the speed of light then two events sufficiently far away from each other cannot possibly affect each other if the time delta of them happening is lower than the time light would take to propagate from one location to another Capturing happensbefore Imagine a single server with multiple writers Server maintains a version number for every key increments the version every time that key is written and stores the new version number along with the value written When a client reads a key the server returns all values that have not been overwritten as well as the lastest version number A client must read a key before writing When a client writes a key it must include the version number from the previous read as an indication of what Ive already seen and it must merge together all the values it received in the prior read When the server receives a write with a particular version number it can overwrite all values with that version number or below since it knows that they have been merged into the new value but it must keep all values with a higher version number because those values are concurrent with the incoming write Essentially when a write includes the version number from a prior read that tells us which previous state the write is based on Merging concurrently written values The above algorithm ensures nothing is silently dropped but it unfortunately requires the clients to do extra work to merge the concurrently written values siblings before writing another Merging siblings is essentially the same problem as conflict resolution in multileader replication as discussed before You could merge by taking one value based on a timestamp losing data take a union of all siblings or if you allow clients to also remove union wont do the right thing and the system must leave a marker tombstone with the appropriate version number to indicate that the item has been removed when merging siblings As merging siblings in application code is complex and errorprone some data structures try to perform this automatically eg CRDTs Version vectors Scaling the algorithm to multiple replicas we need to use a version per key as well as per replica Each replica increments its own version number when processing a write while also keeping track of version numbers it saw from other replicas This indicates what to overwrite and what to keep as siblings The collection of version numbers from all the replicas is called a version vector they are sent from the replica to the client when read and sent back to the database when a value is subsequently written Version vector allows the DB to tell which writes are causal and which are concurrent Riaks dotted version vector is probably the most used which it calls a causal context Similar to the singlereplica example the application may need to merge siblings Summary Replication can serve these goals high availability disconnected operations lower latency scale better Three major approaches to replication singleleader write all goes to leader read can be served from any Reads might be stale Easy to understand and implement multileader write sent to one of several leaders leaders propagate changes to each other and followers More robust but harder to reason and provides only weak consistency guarantees leaderless read and write both from several nodes to detect and correct nodes with stale data The effects of replication lag and different consistency models eventual readyourwrite readmonotonic consistentprefixreads In multileader and leaderless system ways to reconcile write conflicts LWW versionvectors based merge Chap 6 Partitioning A partition in this chapter is called a shard in MongoDB Elasticsearch SolrCloud a region in HBase a tablet in Bigtable a vnode in Cassandra and Riak and a vBucket in Couchbase In effect each partition is a small database of its own although the database may support operations that touch multiple partitions at the same time The main reason for wanting partitioning is scalability Large complex queries can potentially be parallelized across many nodes Different partitions can be placed on different nodes in a shared nothing cluster The fundamentals of partitioning apply to both kinds of workloads Partitioning and replication Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes Even though each record belongs to exactly one partition it may still be stored on multiple different nodes for fault tolerance Each partitions leader is assigned to one node and each node can be the leader for some partitions and a follower for other partitions The choice of partitioning scheme is mostly independent from the choice of replication scheme so this chapter ignores replication Partitioning of KeyValue data Goal spread the data and the query load evenly across nodes A skewed partitioning makes partitioning less effective in that some partitions serve more than others hot spot We can randomly assign which is unideal in that we dont know which node a particular queried item is on and we have to query all nodes in parallel Partitioning by key range Partitioning by a sorted key is used by Bigtable its open source equivalent HBase and earlier MongoDB With each partition we keep keys in sorted orders advantage being range scans are easy downside being certain access patterns can lead to hot spots think a process keeps writing real world clock time sensor readings to a timestamp partitioned database In this case you need something other than timestamp as the first element of the key eg the sensor name Now if you want all sensor levels over a time range multiple queries are needed Partitioning by key hash A good hash function takes skewed data and makes it uniformly distributed For partitioning purposes they need not be cryptographically strong MD5 murmur3 FowlerNollVo functions are all used Each partition would now serve a range of hashes as opposed to keys This technique is good at distributing keys fairly among partitions Boundaries are evenly spaced or chosen pseudorandomly Consistent hashing uses randomly chosen partition doundaries to avoid the need for central control or distributed consensus This approach actually doesnt work very well for databases so its rarely used in practice Using hash means the sorted order of keys is lost Range queries on primary keys are not supported by Riak Couchbase or Voldemort and in MongoDB enabling hashbased partitioning means range queries will be sent to all partitions Cassandra does a compromise between the two partitioning strategies a table can be declared with a compound primary key of several columns the first part of that key is hashed to determine the partition and the others are used as a concatenated index for sorting data in Cassandras SSTable A query can therefore specify the partitioning key and do range queries on the other columns with a fixed partitioning key This enables an elegant data model for onetomany relationships like a social media site where a user has many updates the partitioning key is the user ID and we could then query the users feed within a time range Skewed workloads and relieving hot spots Think of unusual cases of a celebrity on social media with user ID hashbased key all their query would still fall onto the same partition Todays DB typically doesnt handle this automatically application can reduce the skew if it knows one key to be very hot by appending a random number to the key 1 digit gives you 10 partitions with downsides being read now needs to do additional work to read from all partitions This also requires additional bookkeeping Partitioning and secondary indexes If data is over accessed by primary key we can determine the partition from that key The problem is more complicated with secondary indexes involved A secondary index usually doesnt identify a record uniquely but rather is a way of searching for occurrences of a particular value Secondary indexes are the bread and butter of relational DBS and they are common in document databases too Many keyvalue stores such as HBase and Voldemort have completely avoided them some started adding them and they are the raison detre of search servers such as Solr and Elasticsearch The problem is they dont map neatly to partitions and two main approaches are documentbased partitioning and termbased partitioning Documentbased partitioning Each partition maintains its own secondary indexes covering only documents in that partition ie a local index eg map each secondary keys value to the primary keys of records living in this partition So querying by secondary index would mean sending a query to all partitions and combine the results you get back This approach to querying a partitioned database is sometimes known as scattergather and it can make read queries on secondary indexes quite expensive Despite this it is widely used in Cassandra MongoDB Riak Elasticsearch and SolrCloud Termbased partitioning Rather than having local indexes as above we can a global index that covers all data in all partitions and partition that global index to different nodes by key range or hash The term fulltext indexes we are looking for decides which partition the global index is on we read the index from that partition and figure out the primary keys of records we need to query Reads now only needs to request from partitions containing the term it wants as well as the global index but writes are slower as a writing to a single document may now affect multiple partitions of the index every term in the document might be on a different partition This would also require a distributed transaction across all partitions affected by a write to keep data and indexes on different partitions in sync which many dont support In practice updates to a global secondary indexes are often asynchronous as in Dynamo Rebalancing partitions Data size change machine failure etc all calls for moving data from one node to another a process called rebalancing Requirements for rebalancing load should be shared fairly between nodes while rebalancing the DB should continue accepting reads and writes no more data than necessary should be moved when rebalancing Hash mod n This makes rebalancing expensive in terms of data that has to be moved around hence earlier the hashbased partition lets each node store a hash range Fixed number of partitions Create much more partitions than there are nodes assign multiple partitions to the same node If a new node is added the node can steal a few partitions from every existing node until partitions are fairly distributed again The number of partitions or the mapping from keys to partitions dont change the only thing that changes is the mapping of partitions to nodes While a rebalancing is ongoing the old node that this partitions is on continues serving read and write requests This approach is used by Riak Elasticsearch etc For simplicity some of these DBs dont implement splitting a partition so the number configured initially is the max number of partitions you are going to have which should be larger than your number of nodes You should then choose a number high enough to account for future growth but not too high as each partition has management overhead Dynamic partitioning For keyrange partitioning DBs a fixed number of partitions with fixed boundaries can be very inconvenient For this reason HBase creates partitions dynamically it splits and merges in process similar to Btree nodes After a split one half is transferred to another nodes and in case of HBase the transfer happens through HDFS the underlying distributed file system Advantage is the number of partitions adapts to the volume of data To not start out from one single partition empty DB HBase and MongoDB allow an initial set of partitions to be configured on an empty DB This requires you to know what the key distribution is going to look like Dynamic partitioning can be applied for key range partitioned data as well as hashpartitioned data Partitioning proportionally to nodes With dynamic partitioning the number of partitions is proportional to the size of the dataset as split and merge keep the size of each partition between some fixed min and max With fixed number of partitions the size of partitions is proportional to the size of the dataset In both these cases the number of partitions is independent of the number of nodes Cassandra makes the number of partitions proportional to the number of nodes ie to have a fixed number of partitions per node In this case the size of each partition grows proportionally to the dataset size while the number of nodes remains unchanged but when increasing the number of nodes each partition becomes smaller Since a larger data volume generally requires a larger number of nodes to store this also keeps the size of each partition fairly stable When a new node joins the cluster it randomly chooses a fixed number of existing partitions to split and takes ownership of one half of each of those split partitions while leaving the other half in place This introduces unfair splits but averaged over a large number of partitions the new node ends up taking a fair share of load Picking partition boundaries randomly requires hashbased partitioning so the boundaries can be picked from the range of numbers produced by the hash function Manual or automatic rebalancing Fully automated rebalancing can be convenient due to less operational work but can be unpredictable in that rebalancing is expensive and if not done carefully this can overload the network and harm the performance while rebalancing is in progress This could create cascading failure when used in combination with automatic failure detection Detect an overloaded node to be slow decides to move data away from it and further overloading that node Request routing How does a node know which partition to request from This is an instance of a more general problem called service discovery Several highlevel approaches Allow clients to contact any node via a roundrobin load balancer eg if that node does not own the partition it forwards the request to the appropriate node receives a reply and passes that on Send requests from clients to a routing tier first which determines the node to handle the request and forwards it The routing tier acts as a partitionaware load balancer Require clients be aware of the partitioning and the assignment of partitioning to nodes a client can connect directly to the appropriate node without any intermediary In all cases the key problem is how does the routing decision component learn about changes in partition assignment Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata Each node registers itself with ZooKeeper which maintains the authoritative mapping of partitions to nodes The routing component subscribes to ZooKeeper and gets a notification when a change in partition happens HBase SolrCloud and Kafka use ZooKeeper to track partition assignment MongoDB uses similar architecture but relies on its own config server implementation Cassandra and Riak take a different approach which uses a gossip protocol to disseminate any changes in cluster state Requests can be sent to any node and that node forwards them to the appropriate node for the requested partition This model puts more complexity in the database nodes but avoids the dependency on an external coordination service such as ZooKeeper Couchbase does not rebalance automatically which simplifies the design When using a routing tier or sending requests to a random node clients still need to find the IP addresses to connect to this usually isnt fast changing and DNS works just fine Parallel query execution The above focused on very simple queries that reads or writes a single key or scattergather in the case of working with documentpartitioned secondary indexes This is about the level of access supported by most NoSQL distributed data stores Massively parallel processing often used for analytical workload is much more sophisticated in the types of queries they support The query optimizer breaks a complex query into parallel stages Summary Goal of partitioning scalability spread the load evenly avoid hotspots Two main approaches Key range partitioning Efficient range queries risk of hot spots Hash partitioning Each partition owns a range of hashes Inefficient range queries more even data distribution This is often used in combination with fixed number of partitions although dynamic partitioning can also be used Or a hybrid of the two like the compound key in Cassandra To support secondary indexes Documentpartitioned indexes local indexes Secondary index stored in the same partition as the primary key and value A single partition to update on write but a read of the secondary index requires a scattergather across over all partitions Termbased indexes global indexes When write several partitions will need to be updated however a read can be served from a single partition Rebalancing strategies Routing techniques By design each partition operates mostly independently which allows a partitioned database to scale to multiple machines Chap 7 Transactions Many things can go wrong in a distributed data system failing in the middle of a write may crash at any time network can partition several clients can overwrite each others changes may read data thats partially written race conditions etc Transactions have been the mechanism of choice for simplifying these issues they simplify the programming model for applications accessing a DB Transactions group a series of reads and writes into one logical unit to be executed as one the entire batch either succeeds or fails Transactions make application not need to worry about partial failures Not every application needs transactions This chapter discusses read committed snapshot isolation and serializability These concepts apply to distributed storage as well as single node Transactions have been the main casualty of the noSQL movement There emerged a popular belief that transactions are the antihesis of scalability This is not necessarily true ACID The safety guarantees provided by transactions are described by ACID atomicity consistency isolation durability In practice one databases implementation of ACID does not equal anothers Eg isolation can be quite ambiguous Systems not meeting ACID vague are sometimes called BASE basically available soft state eventual consistency an even vaguer term Atomicity atomic operation in the context of multithreaded program means another thread cannot see the the halffinished operation of another In ACID atomicity does not have to do with multiple processes trying to access data at the same time that is isolation In ACID atomicity means if among several writes one fails at some point the database must discard or undo any writes it has made so far in that transaction Consistency is a terribly overloaded term Replica consistency refers to eventual consistency readafterwrite monotonicread consistentprefixread Consistent hashing is an approach to partitioning some systems use for rebalancing CAP theorem consistency means linearizability ACID consistency means an applicationspecific notion of database being in a goodstate ACID consistency means some invariants about your data must always be true like in an accounting system credits and debits must balance This then becomes an applicationspecific definition The application may rely on the DBs atomicity and isolation properties to achieve consistency this is not up to the DB alone and C from ACID should be tossed ACID isolation means concurrently executing transactions are isolated from each other The classic database textbooks formalize isolation as serializability meaning each transaction can pretend its the only transaction running on the DB though in reality there might be several running at the same time ACID durability is a promise that data a transaction wrote successfully will not be forgotten It usually involves writing to hard drive as well as a writeahead log for recovery and a DB reporting a transaction as successful only after successful log write Perfect durability reliability does not exist there are riskreduction techniques but take any theoretical guarantee with a grain of salt Durability historically meant writing to a disk but now has been adapted to mean replication Single and multiobject operations Atomicity and isolation describe what should happen if clients make transactions allornothing concurrent transactions shouldnt interfere with each other another transaction should not see half written results of this transaction This usually requires some way to tell which reads and writes are in the same transaction relational database uses begin transaction and commit Many nonrelational DBs dont have a way of grouping operations together even with a multiobject API in the same statement it doesnt necessarily mean that statement guarantees allornothing Atomicity and isolation apply to singleobject writes as well imagine a disk failure when halfway through writing a large object What does the storage engine guarantee in this case Itd be very confusing if no guarantees are provided so storage engines typically provide atomicity and isolation on the level of a single object on one node which can be implemented with a writeahead log atomicity and a lock on each object isolation Some databases also provide an atomic increment operation and a compareandset operation These singleobject atomicity and isolation guarantees arent the usual sense of ACID A and I they usually refer to grouping multiobject modifications into one unit of execution In many cases multiple objects need to be coordinated updates with foreign keys in relational model updating denormalized fields in document model and updating secondary indexes Implementing these without transactions is possible but error handling is made difficult A key feature of a transaction is that it can be aborted and safely retried if an error occurred Although retrying an aborted transaction is a simple and effective error handling mechanism it isnt perfect in that transaction may have actually succeeded but network failed to deliver the success message Youll be redoing the transaction in this case which would require an applicationlevel deduplication mechanism if error is due to overload retrying is going to make matters worse without reasonable exponential backoff number limits its only worth retrying after transient errors retrying after permanent error is pointless if the transaction is not side effect free the side effect could happen twice if a client fails while retrying what its trying to write is lost Weak isolation levels Race conditions come into play when one transaction reads writes data that is concurrently modified by another transaction Concurrency can be difficult to reason about and debug For this reason databases have long tried to hide concurrency issues from application developer by providing isolation Serializable isolation is a guarantee that transactions have the same effect as if run one at a time This comes at a performance price one which many DBs dont want to pay Read committed Read committed is the most basic level of transaction isolation It makes two guarantees when reading from DB you will only see data that have been committed no dirty reads when writing to the DB you will only overwrite data that has been committed no dirty writes Read committed does not prevent eg two transactions incrementing the same counter but the counter ends up being incremented only once first read second read both get the same value and did not dirty read first write incremented commit then second write incremented commit the later commit did not dirty write Most commonly databases prevent dirty writes by using rowlevel locks modifying a row document requires holding a lock on it and it must then hold the lock until the transaction is committed or aborted No dirty read can be enforced by having readers acquire the same lock while they read this affects performance And instead most databases prevent dirty reads by remembering the old committed value and the new value set by the transaction currently holding the write lock and while write transaction is ongoing it returns the old committed value to readers Snapshot isolation and repeatable reads Imagine a user transferring money between her 2 bank accounts one transaction does two writes acct1 100 1 acct2 100 2 commit another transaction does two reads of acct1 3 and acct2 4 commit with read committed isolation if we have sequence 3 1 2 4 the read is going to see acct1 without the 100 and acct2 with the 100 Such a read is unrepeatable or read skew in that doing the read transaction again after the write is committed will give the expected result Snapshot isolation is the most common solution idea being each transaction reads from a consistent snapshot of the database meaning the transaction sees all the data that was committed in the DB at the start of the transaction even if the data is subsequently changed by another transaction each transaction sees only the old data from the particular point in time This makes integrity checks possible which would otherwise be difficult with just read committed isolation Implementing snapshot isolation also uses a writer lock but read does not require locks Read is built upon a generalized approach in read committed as the DB must potentially keep several different committed versions of an object as various inprogress transactions may need to see the state of the DB at different points in time This is known as multiversion concurrency control MVCC Still readers dont block writers and writers dont block readers Each transaction is usually given a unique increasing ID and each write is tagged with transaction ID created by transaction X deleted by transaction Y An update is delete creates Deleted arent immediately gone but garbage collected later When a transaction reads the transactio IDs are used decide which objects are visible essentially a long enough history of writes to the object and find the latest committed point in history that was before your read transaction At the start of each transaction DB makes a list of all other transactions in progress at the time Any writes those transactions have made are ignored even if they become committed later Any writes made by aborted transactions are ignored Any writes made by transactions with a later transaction ID started after the current transaction are ignored All other writes are visible to the applications queries How do indexes work in a multiversion DB We could have it point to all versions of an object and require an index query to filter out versions not visible to the current transaction With a Btree implementation indexing multiple versions could look like a appendonlycopyonwrite Btree that does not overwrite pages of the tree when updated but creates a copy of each modified page Parent pages up to the root of the tree are copied to point to the new versions of child pages Pages not affected by a write need not be copied Every write then creates a new Btree root and a particular root is a consistent snapshot of the database at the point in time when it was created This approach requires a background process for compaction and garbage collection Repeatable read can be a confusing term in SQL standards Some use repeatable reads to refer serializability Preventing lost updates Dirty write is only one type of write conflicts that can happen Another is the lost update problem as illustrated in the incrementing two counters case an readupdatewrite cycle parsechangewriteback a json object two users editting the same wiki at the same time etc Atomic write operation is one solution which removes the need of readupdatewrite cycles in application code which are usually the best if your code can be expressed in such Mongo supports atomic operations for making local modifications of a json document and redis provides atomic operation to update a data structure like priority queue They are usually implemented with an exclusive lock on the object such that when read other reads are also blocked Another option is to force all atomic operations on a single thread Explicit locking is another approach if the DBs builtin atomic operations dont provide the needed functionality This works but can be hard to get right Automatically detecting lost updates as opposed to forcing serial like in previous approaches this allows parallel and tries to detect lost update and when detected forces one transmission to abort and retry This check can be performed efficiently in conjunction with snapshot isolation Some DBs instead provide a compareandset operation this avoids lost updates by allowing an update to happen only if the value has not changed since you last read it If current value does not match what you previously read this forces an abort and the readmodifywrite cycle has to be retried In a replicated scenario lost updates can happen on different nodes Locks and compareandset assume there is a single uptodate copy of the data which cannot be guaranteed in a multileader leaderless replication Instead they allow concurrent writes to create siblings and use application code or special data structures to resolve and merge them Atomic operations can work well in a replicated context especially if they are commutative they can be applied in different orders and get the same result Write skew and phantoms Imagine you have a hospital where at least one person has to be present oncall and a person can give up oncall if at least there is another oncall Now the only two persons oncall update their individual records to give up oncall at the same time check if there is another person oncall using a countselect on all if so give up its own oncall and the system could end up with 0 persons oncall This is a write skew not a dirty write or lost update since the two transactions are updating two different objects This can be thought of a generalization of the lost update problem two transactions read the same objects then update some objects different in this case same in the case of dirty writes or lost updates Automatically preventing write skew requires true serializable isolation Some DB provides constraints foreign keys constraints or restrictions on a particular value Most dont have support for constraint involving multiple objects but one may be implemented with materialized views or triggers Without true serializable isolation level your best option is to explicitly lock all the rows the transaction depends on Enforcing two users cannot claim the same username in a snapshot isolation DB has the same problem fortunately unique constraint is a simple solution here where the DB will reject the second transaction All these examples follows a similar pattern readcheck conditionwrite where the write could change the condition checked in step 2 In the first example we can lock everything read in step 1 and in the second we are checking for absence and we cant attach a lock to anything An approach called materializing conflicts would have us create locks in advance for nonexistent objects This is errorprone and leaks a concurrency mechanism into application model The effect where a write in one transaction changes the result of a search query in another transaction is called a phantom Snapshot isolation prevents phantoms in readonly queries but readwrite transactions can still have phantoms that led to write skew Serializability Looking at some application code its hard to tell if its safe to run at a particular isolation level and there are no tools to help detect race conditions The simple answer has been use serializable isolation which was usually regarded as the highest isolation level Historically on a single node serializable isolation was implemented with actual serial execution two phase locking or optimistic concurrency control techniques such as serializable snapshot isolation Actual serial execution Remove concurrency entirely Only recently have designers decided a singlethreaded loop for executing transactions was feasible Two developments caused this RAM has become cheap enough that its often feasible to keep the entire dataset in memory executions become much faster due to no disk involvement DB designers realized OLTP reads and writes are usually small By contrast analytical workloads are usually large and readonly they can run a consistent snapshot outside of the serial execution loop Redis Datomic has support for this In order to make the most of the single thread transactions need to be structured differently from their traditional form Systems with singlethreaded serial transaction processing dont allow interactive multistatement transactions instead the application must submit the entire transaction code to the DB ahead of time as a stored procedure to prevent the single thread waiting on the back and forth network transmission cost of interactive queries in the same transaction every transaction has to be small and fast Stored procedure has existed for some time in relational databases and theyve been part of SQL for long Theyve a bad reputaion for different vendors using their own languages Oracle PL SQL server TSQL Redis Lua Datomic Java code running in a DB being difficult to manage test deploy integrate with a monitor system and a DB is often much more performance sensitive than an application server and a badly written stored procedure can mean more trouble Executing all transactions serially made concurrency control simpler but limits the transaction throughput to the speed of a single CPU core on a single machine this can be a bottleneck for a high write throughput system To scale to multiple CPU cores you can partition your data such that each transaction only needs to read and write data in one partition served by one CPU core For any transaction needing to access multiple partitions the database must coordinate the transaction across all the partitions it touches and the stored procedure needs to be performed in lock step across all partitions to ensure serializability across the system Data with multiple secondary indexes is particularly difficult Two phase locking For around 30 years there was only one algorithm widely used for serializability 2PL Note that 2PL is completely different from 2 phase commit 2PC To prevent dirty writes we have when two concurrent transactions trying to write the same object the lock ensures the second writer wait till the first one has finished or aborted before it may continue Two phase lockings lock requirement is much stronger Several transactions are allowed to read the same object concurrently but as soon as anyone wants to write an object exclusive access is required If transaction A has read an object and transaction B wants to write that object B has to wait till A commits or aborts before it can continue If transaction A has written an object and transaction B wants to read that object B has to wait until A commits or aborts before it can continue In 2PL writers dont just block other writers they also block readers and viceversa This captures the key difference with snapshot isolation where readers never block writers and writers never block readers MySQL and SQL server use 2PL to implement serializable isolation level Blocking of readers and writers is implemented by having a lock on each object in the DB the lock can be either in exclusive mode or shared mode To read an object the transaction has to acquire the lock in shared mode not owned by any exclusive To write to an object the transaction has to acquire the lock in exclusive mode not owned by any exclusive or shared If a transaction first reads then writes it may upgrade its shared lock to an exclusive lock a process that works the same way as getting an exclusive lock directly After a transaction has acquired the lock it must continue to hold the lock until the end of the transaction This is where the name twophase came from first phase is when the locks are acquired second phase where all locks are released at the end of a transaction Deadlock can happen with 2PL they can also happen in lockbased read committed isolation level but much more rarely in which case the DB automatically detects and abort one of the transactions The aborted is later retried by the DB Big downside of 2PL is performance due to locking overhead and reduced concurrency To prevent phantom writes we may need predicate locks which rather than belonging to a particular row in a table it belongs to all objects that match some search conditions If a transaction wants to read write an object matching the predicate it must acquire the shared exclusive predicate lock as well Same goes for transactions trying to insert delete Predicate locks apply even to objects that dont yet exist in the database If twophase locking includes predicate locks the DB prevents all forms of write skews and other race conditions making it serializable Predicate locks dont perform well instead most DB with 2PL implement indexrange locking a simplified approximation of predicate locks they lock a bigger range of objects than necessary by locking all objects associated with an index range but they have lower overheads If there are no suitable index where a range lock can be attached then the DB can fall back on entire table locking its safe but bad for performance Serializable Snapshot Isolation Serializable isolation and good performance seem at odds with each other SSI may be able to change that 2PL is a pessimistic concurrency control mechanism based on the principle that if anything might possibly go wrong its better to wait until the situation is safe again before doing anything Serial execution in a sense is pessimistic to the extreme By contrast SSI is an optimistic concurrency control technique Optimistic in that it lets transactions continue and hope everything will turn out alright and when a transaction wants to commit the DB checks whether anything bad happened If so one has to be aborted and retried This is an old idea and performs badly if there is high contention However if there is enough spare capacity and contention is not too high optimistic might be able to outperform pessimistic ones Contention can be reduced with commutative atomic operations when it doesnt matter which one is committed first Earlier the write skew happened due to trasaction having acted on an outdated premise query result might have changed we could detect these transactions and abort them There are two cases detecting reads of a stale MVCC object version uncommitted write occurred before the read To prevent this anomaly the database needs to track when a transaction ignores another transactions writes due to MVCC visibility rules When the transaction wants to commit the database checks whether any of the ignored writes have now been committed If so the transaction must be aborted detecting writes that affect prior reads the write occurs after the read An index keeps track of ongoing transactions that have read it and when a transaction writes to the database it must look in the indexes for any other ongoing transactions that have read the affected data It notifies those transactions the data they read may not be uptodate consequently they may or may not need to be aborted and retried To decide the granularity at which transactions reads and and writes are tracked there is the tradeoff between bookkeeping overhead and aborting more transactions than necessary The big advantage over 2PL is one transaction does not need to block waiting for locks held by another transaction In particular readonly queries can run on a consistent snapshot without requiring any locks which is appealing for readheavy loads Compared to serial execution SSI is not limited to the throughput of a single CPU core Serialization conflict detection can be distributed Transactions can read and write data in multiple partitioning while ensuring serializable isolation Performance SSI is affected by the rate of aborts and SSI requires readwrite transactions to be fairly short SSI is probably less sensitive to slow transactions than 2PL or serial execution Summary Transactions are an abstraction layer that allows an application to pretend that certain concurrent problems and faults dont exist a large class of errors is reduced down to a simple transaction abort and the application just needs to retry Isolation level read committed snapshot isolation repeatable read serializable Dirty reads dirty writes guaranteed by read committed Read skew guaranteed by snapshot isolation usually implemented with MVCC Lost updates some snapshot isolation implementation prevent this others require a manual lock SELECT FOR UPDATE Write skew readcheck premisewrite only serializable isolation prevents this anomaly Phantom read one transaction reads results matching a condition another writes that affects the results of the query Snapshot isolation can prevent straightforward phantom reads but phantoms in the context of write skew needs the likes of indexrange locks Only serializable prevents all these issues when using a weaker isolation level application needs additional logic eg explicit locking to protect against these Three ways to implement serializable actual serial execution 2PL pessimistic SSI optimistic Chap 8 The Trouble with Distributed Systems This chapter turns our pessimism to maximum and assume everything that can go wrong will go wrong except Byzantine failures Faults and partial failures An individual computer with good software is usually fully deterministic either fully functional or completely broken not something in between This is a deliberate choice in the design of computers if an internal fault happens we prefer it to crash completely rather than returning a wrong results because the latter are hard to deal with it hides away the physical reality on which they are implemented Distributed systems are completely different nondeterministic partial failures are possible which makes distributed systems hard to work with Two extremes of building largescale computing systems highperformance computer with thousands of cores or cloud computing usually with multitenant datacenters commodity computers connected with IP network and elastic resource allocation The first approach deals with errors usually with regular snapshotting If a node fails the entire cluster halts and recovers from a snapshot this is more like a singlenode approach of error handling This chapter focuses on the failure handling of the second type Difference being the second type of applications is often expected to be online in that they need to be available to serve users with low latency at any time Unlike a supercomputer where each node is rather reliable the commercial hardware has much higher error rates It becomes reasonable to assume at any time in point something is always broken and the system needs to be able to tolerate failed nodes useful also for rolling upgrade restarts uninterrupted services Network is often IP and Ethernet based arranged in Clos topologies to provide high bisection bandwidth Geographically distributed deployment where interdatacenter is usually slow and unreliable We build a reasonably reliable system from unreliable components think errorcorrecting codes reliability in TCP etc In distributed systems suspicion pessimism and paranoia pay off Unreliable networks Sharenothing architecture not the only way but by far predominant Loss packet switched network Delay loss one end temporarily stop responding etc are all possible The sender cant tell whether the packet was delivered with a possibly dropped ack then yes The usual way to handle this is timeout Wait some time until you give up and try again Handling network faults doesnt necessarily mean tolerating them you could just deliver an error message but you do need to know how your software reacts to network problems and ensure they can recover It may make sense ti deliberately trigger network problems to test Chaos monkey Detecting faults Timeouts Undetected delays Many systems need to automatically detect faulty nodes Its sometimes hard to do TCP ports refusing connection can get you back a RST or FIN but the node can crash mid request If application process fails a script can detect and tell other nodes if the host OS is still working The router may give you back an ICMP destination unreachable if the node you are trying to get to is unreachable Hardware failure can also be detected at switch hardware level if you cam query the management interface of the switches If you want to be sure a request was successful you need a positive ACK from the application itself NACK is useful for speedy detection but they cannot be relied upon You can wait for timeout retry a few times and declare dead if no response There is no simple answer to how to set the timeout Premature declaration of a node being down is problematic places additional load cascading failure possibility potentially duplicated operation Imagine you have a network transmission time upperbound of d and processing time upperbound of r then 2d r seems a reasonable timeout but most systems in reality dont have these guarantees If your timeout is low it only takes a transient spike in roundtrip time to throw the system off balance Many can queue a network packet switch queueing receiver OS queueing sender queueing eg due to TCP flow control TCP packet loss can cause further delay in OS waiting for retransmission and retransmitted packet to be acknowledged UDP is a good choice in situations where delayed data is worthless eg videoconferencing and VoIP Better than a a configured constant timeouts systems can continuously measure response times and their variability jitter and automatically adjust timeouts according to observed response time distribution TCP retransmission timeouts works this way as does Phi Accrual failure detector in Cassandra Synchronous network vs asynchronous Telephone networks synchronous are circuit switched a circuit a fixed route is established between the two parties throughout the call much more reliable and does not suffer from queueing as the 16B space for the call have already been reserved in the next hop of the network during a call each side is guaranteed to be able to send exactly 16B of audio every 250ms Because the network has no queueing the maximum endtoend delay is fixed a bounded delay TCPIP network is packet switched as they are optimized for bursty traffic Audiovideo call has a stable data rate while web browsing can have a variable data rate TCPIP tries to deliver as fast as possible There has been attempts to build hybrid of packet switching and circuit switching such as Asynchronous Transfer Mode ATM network a competitor of Ethernets It implements endtoend flow control at link layer which reduces the need for queueing in the network though it can still suffer from congestion and cause delays With careful use of quality of service QoS prioritization and scheduling of packets and admission control ratelimiting senders its possible to emulate circuit switching on packet switching networks or provide statistically bounded delay More generally latency guarantees are achievable in certain environments if resources are statically partitioned dividing network link statically as described above or allocating a static number of CPU cycles to each thread These come at the cost of reducing utilization Multitenancy with dynamic resource partitioning provides better utilization making it cheaper but with the downside of variable delay Variable delays in networks are not a law of nature but simply the result of a costbenefit tradeoff Currently deployed technology does not allow us to make any guarantees about delays or reliability of the network Unreliable clocks Time is tricky business in a distributed system communication is not instantaneous each machine has its own clock usually a quatz crystal oscillator These are not perfectly accurate and each machine may have its own notion of time It is possible to sync time to some degree most commonly with NTP which allows the computer clock to be adjusted according to the time reported by a group of servers The servers in turn get their time from a more accurate time source such as a GPS receiver Monotonic clock and timeofday clock Modern computer has at least these two kinds and they serve different purposes Timeofday clock wall clock time gets you time according to some calendar clockgettimeCLOCKREALTIME call on Linux gets you number of seconds since the epoch UTC 1970 Jan 1 midnight according to Gregorian calendar not counting leap seconds a day may not have exactly 86400 seconds Timeofday clocks are usually synced with NTP some oddities include eg when a local clock is too ahead it may jump back in time to a previous point These jumps make timeofday clock unsuitable for measuring elapsed time historically they are also very coarse grained Monotonic clock is suitable for measuring time interval such as a timeout or a services response time clockgettimeCLOCKMONOTONIC on Linux is monotonic clock These clocks are guaranteed to move forward The absolute value of monotonic clock is meaningless and monotonic clocks are not synchronized across machines It also makes no sense to compare the monotonic clock time from two different computers On a multiCPU system there may be a separate timer per CPU which is not synchronized with other CPUs OS tries to compensate for this discrepancy but one should take this guarantee of monotonicity with a grain of salt NTP may adjust the frequency at which the monotonic clock moves forward by 005 if it detects that the computers local quartz is moving faster or slower than the NTP server but it cannot cause monotonic clock to jump forwards or backwards Clocks drift run faster or slower than it should depending on the temperature Google assumes a clock drift of 200 parts per million for its servers an equivalent of 6ms drift for a clock that is resynced with a server every 30s This limits the best possible accuracy you can have for wall clocks If a computers clock differs too much from an NTP server it may refuse to synchronize or the local clock will be forcibly reset consequently any applications will see a jump forward backward in time NTP synchronization can also only be as good as the network delay NTP clients are robust enough to query a number of configured servers and discard outliers Leap seconds result in a minute being 59s or 61s long which could mess up systems not designed with leap seconds in mind It is possible to achieve very good accuracy if you care about it sufficiently to invest significant resources eg mifid ii draft requires all HFT to synchronize their clocks to within 100ms of UTC to help detect market manipulation Such precision can be achieved using GPS receivers precision time protocol and careful deployment If you use software that requires synchronized clocks it is essential that you also carefully monitor the clock offsets between all the machines Any node whose clock drifts too far from the others should be declared dead and removed from the cluster Timestamp for ordering events Using synchronized wall clock to order events in distributed systems is not advisable In a multileader system drift between nodes can cause the replicas to not be eventually consistent without further intervention no matter if using leader timestamp or client timestamp if multiclients Use logical clocks for this purpose which are based on incrementing counters rather than an oscillating quartz crystal Clock reading as confidence interval Clock readings over the network compared with a server whose time this syncs to is more like a range of times within a confidence interval clockgettime return value doesnt tell you the expected error of a timestamp and you dont know the confidence interval Googles TrueTime API in spanner explicitly reports the confidence interval on the local clock It returns two values earliest latest whose width depends on how long it has been since the local quartz clock was last synced with a more accurate clock source Synchronized clocks for global snapshots transaction ID Recall that in snapshot isolation each transaction has a motonically increasing ID and if B reads a value written by A B should have a transaction ID higher than that of As Generating a monotonically increasing ID in a distributed system with lots of small rapid transactions can be challenging Synchronized clock with good enough accuracy can be used to generate this ID and Spanner implements snapshot isolation across data centers this way with time returning a confidence interval if two intervals dont overlap then we know in which order those two times are In order to ensure transaction timestamp reflects causality Spanner deliberately waits for the length of confidence interval before committing a readwrite transaction so any transaction that can read this data happens at a sufficiently later time so that their confidence intervals dont overlap Hence Spanner needs to keep the interval as small as possible to minimize wait time and for this reason Google deploys a GPS receiver or atomic clock in each data center allowing clocks to be synchronized within 7ms Process pauses Assume weve a single leader system how does a leader know its still leader and not proclaimed dead by others We can let the leader hold a lease a lock with timeout To be leader the node has to renew lease before it expires Imagine a process renews its lease 10s before expiry which should not be the wall clock time of a different node who set it but the last request processing took longer than that then by the time the request finishes this node would no longer hold the lease A pause like this can happen if markandsweep GC runs long enough virtual machine suspension and resume context switches slow disk access OS swapping inmemory and ondisk vram pages frequently thrashing a process receiving a SIGSTOP followed by a late SIGCONT A node in a distributed system must assume that its execution can be paused for a significant length of time at any time even in the middle of execution Response time guarantees limiting the impact of GC Processes can pause for unbounded time as shown before On the other hand hard real time systems have a specified deadline by which the software must respond Providing realtime guarantees in a system requires support from all levels of software stack a real time operating system RTOS that allows processes to be scheduled with a guaranteed allocation of CPU time in specified interval is needed Library functions need to document worstcase execution time Dynamic memory allocation may be restricted or disallowed algother An enormous of testing is required to guarantee the requirements being met These places a lot of constraints on programming languages libraries and tooling Realtime systems often have lower throughput as they have to prioritize timely response above all else For most serverside data systems realtime guarantee is not economical Consequently they must suffer pauses and clock instability To limit the pause from markandsweep GC an emerging idea is to treat GC pause like brief outage of the node and let other nodes handle requests If a node needs to GC soon it stops taking in requests and GCs after finishing up Some trading systems do this A variant of the idea is to only GC short lived objects cheap to GC and restart periodically Knowledge truth and lies Reasoning about distributed system can be hard as you dont know the state of other nodes for sure the only way is to ask them via a not always reliable network In a distributed system we can state the assumptions we are making about the behavior the system model and design the actual system in such a way that it meets those assumptions A node cannot necessarily trust its own judgment of a situation it may think itself alive as it can hear from other nodes but other nodes cannot hear from it and declare it dead similarly think itself a leader a holder of a lease etc Instead the truth is defined by the majority a quorum that requires a minimum number of votes Most quorums require a majority number of votes as there cannot be a differing quorum at the same time Frequently a system requires there be only one of something Eg a single leader only one node holding a lock globally unique username Say a node grabs a lease to write something then GCs itself lease expires during GC and is granted to another node the GCed node coming back may resume its write operation thinking itself still holding the lease This is problematic and happened for HBase Fencing token is a technique that can address this each time the lock server grants a lease it also returns a fencing token a number incremented by the lock service we then require every clients write request to the storage service to include the current fencing token and the storage service will reject old fencing tokens if it has already seen a newer one ZooKeeper can be used a lock service with the transaction ID or node version as monotonically increasing candidates for fencing token Byzantine faults This book assumes nodes are unreliable but honest Distributed system problems become much harder if there is a risk of nodes lying A system is Byzantine faulttolerant if it continues to operate correctly even if some of nodes are not obeying the protocol Flight control systems typically need to be Byzantine fault tolerant due to radiation corrupting physical hardware With multiple participating organization a system may need to be Byzantine fault tolerant blockchain tries to address such In most serverside data systems the cost of deploying Byzantine fault tolerant solutions make them impracticable In a clientserver architecture if we assume untrustworthy clients the servers can usually perform validation to decide whats allowed Most Byzantine fault tolerant algorithms require a super majority of more than twothirds of the nodes to be functioning correctly In scenarios if a malicious attacker can compromise software running on other nodes then Byzantine fault tolerance wont help and traditional mechanisms authentication access control encryption firewalls continue to be the main protection against attackers Weak forms of lying unreliability handling is pragmatic and doesnt require fullblown Byzantine fault tolerant solutions TCP checksums user input sanitization setting up redundant NTP servers are good examples System model and reality Algorithm correctness Safety and liveness A system model is an abstraction that describes what things an algorithm may assume formalizes the kinds of faults that we expect to happen in a system Timingassumptionswise 3 system models are in common use synchronous bounded network delay processes pauses and clock error partially synchronous most of the time bounded realistic for many systems asynchronous no timing assumptions in fact we dont even have a clock and cannot use timeouts Nodefailurewise 3 system models are in common use crashstop faults node fails in one way crashing and does not come back crashrecovery faults crash may happen at any time a node may also come back after some time Nodes are presumed to have persistent storage so precrash state can be captured but inmemory states are lost Byzantine faults nodes can do anything For modeling real systems partially synchronous model with crashrecovery is most common Correctness of an algorithm under these models are described by its properties eg sorting algorithm should have all elements sorted Fencing tokens generation should have uniqueness monotonic sequence and availability node who requests a fencing token and does not crash should eventually get a token An algorithm is correct if in some system model it always satisfies its properties in all situations that we assume may occur in that system model Two kinds of properties in the fencing token example unique and monotonic are safety and availability is liveness Safety means nothing bad happens if violated we can point at a particular point in time at which its broken and a violation cannot be undone and liveness mean something good happens eventually may not hold at some point in time but there is always hope that it may be satisfied in the future Distinguishing between safety and liveness helps with dealing with difficult system models Theoretical abstract system models are quite useful even though in practice a real system can violate the assumptions of an abstract model making empirical testing equally important Summary This chapter covers what could go wrong in a distributed system lossy network clock out of sync process pause Alternatives that guarantee these dont happen throughout the stack do exist but are usually costly Partial failure can occur is the defining characteristic of distributed systems If you can simply keep things on a single machine it is generally worth doing so However scalability fault tolerance and low latency can make distributed systems desirable Chap 9 Consistency and consensus Given the problems in distributed systems introduced in chap 8 one good way of tackling them is to find general purpose abstractions with useful guarantees implement them once and let application run under those guarantees Transaction is one such guarantee that hides underlying concurrency and crashes and provides acid to the application Consensus is another such guarantee getting nodes to agree on something Consistency models linearizability Recall linearizability atomic consistency strong consistency immediate consistency external consistency the strongest form of consistency where we make the system appear as if there is only one copy of the data and all operations on it are atomic Linearizability requires the data written by a completed write call to be immediately available for all subsequent read calls Read after write is done must then reflect the written result read concurrent with write can return the result before or after the write but once result after write is returned subsequent reads must return the result after write Linearizability vs serializability the former is a recency guarantee on reads and writes of a register one individual object so it does not prevent problems like write skew the latter is an isolation property of transactions where each transaction may read and write multiple objects It guarantees that transactions behave the same as if they had executed in some serial order It is Ok for that serial order to be different from the order in which transactions were actually run A database providing both serializability and linearizability is known as strict serializability or strong onecopy serializability 2PL and actual serial execution are typically linearizable Serializable snapshot isolation is not linearizable by design Linearizability is useful in the following scenarios locking and leader election In a single leader system one way to elect a leader is to use a lock Every node that starts up tries to acquire a lock and the one that succeeds becomes the leader No matter how this is implemented it must be linearizable all nodes must agree which node owns the lock otherwise the lock is useless Coordination service like ZooKeeper use consensus algorithms to implement linearizability in a faulttolerant way and are often used for locking and leader election implementation uniqueness guarantee The situation is similar to a lock when a user registers for a service with a unique username you can think of them acquiring a lock on their chosen username A hard uniqueness constraint typically requires linearizability Foreign key or attribute constraints can be implemented without requiring linearizability crosschannel timing dependencies Implementing linearizability To implement linearizability one way is to just have one copy of the data which is not fault tolerant We need replications and revisiting different replication mechanisms singleleader replication is linearizable if reading from leader or synchronously updated followers consensus algorithms bear a resemblance to singleleader replication They also implement linearizable storage safely multileader replication systems are generally not linearizable write conflicts resolution are typically an artifact of lacking a single copy of data leaderless replication systems like Dynamo claim strong consistency by requiring w r n This is not quite true LWW conflict resolution based on timeofday clock are not linearizable as clock timestamps cannot be guaranteed to consistent with actual event timing due to clock skews Sloppy quorum also ruins linearizability Even with strict quorum this is not necessarily true To make strict quorum linearizable a reader must perform read repair synchronously before returning results to the application and a writer must read the latest state of a quorum of nodes before sending its writes The cost of linearizability CAP theorem When partition happens pick one out of consistency linearizability or availability If your application requires linearizability and some replicas are disconnected from the other replicas due to a network problem then some replicas cannot process requests while they are disconnected they must either wait until the network problem is fixed or return error become unavailable If your application does not require linearizability then it can be written in a way that each replica can process requests independently even if it is disconnected from other replicas eg multileader In this case the application can remain available in the face of a network problem but its behavior is not linearizable All in all there is a lot of misunderstanding about CAP and many socalled highly available systems actually dont meet CAPs idiosyncratic definition of availability CAP as formally defined is of very narrow scope it only considers one consistency model linearizability and one kind of fault network partition It doesnt say anything about network delays dead nodes or other tradeoffs thus although historically influential CAP has little practical value for designing systems CAP has also been superseded by more impossibility and more precise results in distributed systems Although linearizability is a useful guarantee surprisingly few systems are linearizable RAM on a modern CPU is not in the face of multithreading race conditions unless a memory barrier or fence is used Reason for this is each CPU core having its own cache and memory access first goes to the cache then changes are asynchronously written to memory This creates multiple copies and with asynchronous updates linearizability is lost The reason to drop linearizability in this case has nothing to do with CAP but for performance The same is true for many distributed databases that dont provide linearizability they sacrifice it for performance not so much for fault tolerance Linearizability is slow Can it be made fast The answer is perhaps no Research has shown if you want linearizability response time of reads and writes is least proportional to the uncertainty of delays in the network In a network with highly variable delays the response time for linearizability is inevitably going to be high Weaker consistency systems however can be made faster Ordering guarantees The definition of linearizability behaves as if there is only a single copy of the data and every operation takes effect atomically implies that operations are executed in some welldefined order Ordering has been an important theme single leader addresses this serializability is about ensuring transactions are as if they are executed in some sequential order timestamps introduced during clock synchronization is another attempt at determining which happened first There is deep theoretical connection between ordering linearizability and consensus Ordering and causality Ordering helps preserve causality Consistency level consistent prefix reads is about causality One readwrite knowing about another is another expression of causality Read skew is a violation of causality the answer can be seen but not the question A consistent snapshot in Snapshot Isolation means consistent with causality if it contains an answer it must contain a question Causality imposes on an ordering of events cause comes before effect A system is causally consistent if it obeys the ordering imposed by causality eg snapshot isolation provides causal consistency A total order allows any two elements to be compared a causal order is not a total order integer space has a total order sets dont Sets are partially ordered by subset superset Linearizability has a total order of operations one copy all atomic Causality in distributed systems may have two operations being concurrent two events can be ordered if they are causally related otherwise they are incomparable this means causality defines a partial order Git history is very much like a graph of causal dependencies Linearizability is thus stronger than causal consistency linearizability implies causality Causal consistency is the strongest possible consistency model that does not slow down due to network delays and remains available in the face of network failure The technique for determining which operation happened before which is similar to earlier discussion in detecting concurrent writes in a leaderless datastore In order to determine causal ordering the database needs to know which version of the data was read by the application this is why earlier discussion had the version number from the prior operation being passed back to the database on a write SSI conflict detection uses a similar idea when a transaction wants to commit the DB checks whether the version of the data that it read is still up to date to this end the DB keeps track of which data has been read by which transaction Sequence number ordering If there is a logical clock that generates a unique sequence number for each operation then the sequence numbers define a total ordering In particular we can create sequence numbers in a total order that is consistent with causality In singleleader replication the replication log defines a total order of write operations that is consistent with causality The leader can generate a sequence number for each write event in the log Without a singleleader various methods are used in practice each node generates its independent set one odd another even sufficiently high resolution timeofday clock can be used preallocate a block of sequence numbers for each node These methods all have the problem of the sequence numbers they generate are not consistent with causality one node being faster than another breaks the first clock skew breaks the second and preallocating block breaks causality in that a later block is always ranked later The above are inconsistent with causality but Lamport timestamp a pair counter nodeID is Uniqueness is obvious total ordering is defined first order by counter value then node ID and the key idea is every node keeps track of the maximum counter value it has seen so far and includes that maximum on every request When a node receives a request or response with the maximum value greater than its own counter value it immediately increases its own counter to that maximum As long as the maximum counter value is carried along with every operation this scheme ensures that the ordering from the Lamport timestamps is consistent with causality because every causal dependency results in an increased timestamp Version vectors are different from Lamport timestamps version vectors can distinguish whether two operations are concurrent or causally dependent whereas Lamport timestamp always enforce a total ordering You cannot tell two operations are concurrent or causally dependent from Lamport timestamp but they are more compact than version vectors Total ordering as enforced by Lamport timestamp can be not sufficient when enforcing unique username you can use Lamport timestamp to decide which operation to register the same username came later and reject that but this happens after the fact you have to check with every other node to find out which timestamps it has generated and cannot guarantee if one node is unreachable This hurts availability The problem is total ordering emerges only after you have collected all the operations The idea of knowing when your total order is finalized is captured in the section below Total order broadcast Total order broadcast atomic broadcast is usually described as a protocol for exchanging messages between nodes where two safety properties need to be satisfied reliable delivery if a message is delivered to one node it is delivered to all nodes totally ordered delivery messages are delivered to every node in the same order ZooKeeper implements total order broadcast Total order broadcast is exactly what one needs for DB replication if every message represents a write to the DB and every replica processes the same writes in the same order then the replicas will remain consistent with each other This principle is known as state machine replication Total order broadcast can be used to implement serializable transactions if every message represents a deterministic transaction to be executed as a stored procedure ad if every node processes those messages in the same order then partitions and replicas of the DB are kept consistent with each other An important aspect of total order broadcast is that order is fixed at the time the messages are delivered a node cannot retroactively insert a message into earlier position in the order if subsequent messages have already been delivered This fact makes total order broadcast stronger than timestamp ordering Total order broadcast is also useful for implementing a lock service that provides fencing tokens request to acquire a lock is appended as a message to the log and all messages are sequentially ordered in the order they appear in the log and the sequence number can then serve as a fencing token as its monotonically increasing ZooKeeper zxid is one such sequence number Partitioned databases with a single leader per partition often maintain ordering only per partition and they cannot consistency guarantees across partitions eg consistent snapshots foreign key references Total ordering across all partitions is possible but requires additional coordination Linearizability and total order broadcast Linearizability is not quite the same as total order broadcast If you have total order broadcast you can build linearizable storage on top of it Imagine we have a total order broadcast log when writing to a key we first append the desire to write to the log and then when we commit the write we only do so if we see our attempt to write happens first since the last committed write happened in log If so we commit otherwise we abort a linearizable compareandset This ensures linearizable writes and a similar approach can be used to implement serializable multiobject transactions on top of a log This asynchronous update procedure does not guarantee linearizable reads one can read a stale value This write linearizability provides sequential consistency timeline consistency slightly weaker than linearizability To make reads linearizable we can do sequencing reads through the log by appending a message reading the log and performing actual read when the message is delivered back to you if the log allows you to fetch the position of the last log message in a linearizable way you can query that position wait for all entries up to that point to be delivered then perform the read you can make your read from a replica that is synchronously updated on writes and is thus sure to be up to date We can also build total order broadcast from linearizable storage Assume you have a lineariable register that stores an int and has an atomic incrementandget operation for every message you want to send through total order broadcast you incrementandget the linearizable integer and then attach the value you got from the register as a sequence number to the message Resend lost messages and let the recipients apply the messages consecutively by sequence number Unlike Lamport clocks the numbers you get from incrementing the linearizable register form a sequence without gaps Seeing a gap in the sequence number means the recipient need to wait and this is the key difference between total order broadcast and timestamp ordering If things never fail building a linearizable incrementandget is easy you could keep it in a variable on one node when dealing with failure in general you end up with a consensus algorithm to generate linearizable sequence number generator It can be proved that a linearizable compareandset or incrementandget register and total order broadcast are both equivalent to consensus and the solution for one can be transformed into that for another Consensus Getting nodes to agree is a subtle but important problem in leader election atomic commits etc FLP result claims no algorithm is always able to reach consensus if risking a node crash in the asynchronous system model where a deterministic algorithm cannot use clocks or timeout eg to detect crashes 2Phase Commit 2PC is a simple form of consensus which can be used for atomicity in multipleobject transactions eg when updating a secondary index In a singlenode write scenario atomicity can be achieved by first making the written data durable in a writeahead log then write the commit record Crashrecovery would consider the write committed aborted if seeing not seeing the commit record at the end In case of multinodes being involved eg termpartitioned secondary index where the secondary index can live on a different node from where the data is we cant just do write data commit record sequence for each node Note that most NoSQL DBs dont support distributed transactions but clustered relational DBs do Commits are irrevocable as implied by readcommitted consistency A node must commit when it is certain all other nodes involved in the transaction are going to commit This is where 2PC comes in Differentiate 2PC with 2PL latter is for achieving serializable isolation and former is for atomic commit to distributed nodes 2PC uses an extra component coordinator transaction manager The workflow is Write Coordinator write to individual nodes Prepare Coordinator asks each node to be ready for commit after which node promises to commit without actually committing If a node replies with yes ready to commit theres no turning back from this decision it must commit if later the Coordinator tells it to Commit Upon hearing back from all nodes that they are ready for commit the Coordinator tells all to commit If failure to commit happens at this stage the coordinator must retry until succeeds theres no turning back wait for a participant recovery if needed Two points of noreturn in the workflow a node cannot refuse to commit later on if it has replied to Coordinator that it will commit if told and once Coordinator makes the decision to commit the decision is irrevocable In case of a Coordinator crash if it happens before prepare participant can abort if it happens after participant replying yes then the participant cannot abort unilaterally it has to wait for Coordinator recovery This is why the Coordinator must write its commit abort decision to its writeahead log so that when recovering it knows what its decision was In other words commit point in 2PC comes down to a single node Coordinator deciding to commit or abort 2PC is blocking atomic commit as nodes potentially have to wait for coordinator recovery 3PC can make this process asynchronous but 3PC assumes bounded network response time and bounded node response time in general non blocking commit requires a perfect failure detector a reliable mechanism to detect crashes Distributed transaction 2PC provides an important atomicity guarantee but cause operational problems kills performance and promises more than it can deliver Distributed transactions comes in Database internal distributed transaction all nodes running the same software Heterogeneous distributed transaction nodes run different software or even some running DB others running message brokers A lot more challenging Exactly once message processing between heterogeneous systems allows systems to be integrated in powerful ways eg message from a message queue can be acknowledged as processed if and only if the database transaction for processing the message was successfully committed This can be implemented by atomically committing the message acknowledgement and the database writes in a single transaction With distributed transaction support this can be achieved when the two are not on the same machine Such a distributed transaction is only possible if all systems affected by the transaction are able to use the same atomic commit protocol XA is a standard for implementing 2PC across heterogeneous technologies A series of language API bindings interacting with a Coordinator and a Coordinator library implementation In case of a Coordinator crash participants are stuck in their transaction they cannot move on as database transaction usually take a rowlevel exclusive lock on any row they modify to prevent dirty writes and DB cannot release those locks until the transaction commits or aborts To counter potentially waiting for Coordinator forever many XA implementation allows a participant to unilaterally decide to abort which can break atomicity XA either has Coordinator being a single point of failure or Coordinator faces the same distributed transaction problem its distributed log becomes a database requiring replica consistency etc XA works across systems and is necessarily a lowest common denominator and cannot detect deadlocks or implement Serializable Snapshot Isolation Fault tolerant consensus The consensus problem is normally formalized as one or more nodes may propose values and the consensus algorithm decides on one of those values It must satisfy the properties uniform agreement no two nodes decide differently safety integrity no nodes decide twice safety validity if a node decides some value v then v was proposed by some node safety termination every node that does not crash has to eventually decide some value liveness If you dont care about fault tolerance then satisfying the first three principles is easy you can hardcode one node to be the dictator and let that node make all of the decisions Should it fail then the system is stuck fourth property is violated 2PC with its Coordinator does just the above and violated termination property Termination property formalizes the idea of fault tolerance If all nodes crash and none are running then it is not possible for an algorithm to decide anything It can be proved that any consensus algorithm requires at least a majority of nodes to be functioning correctly in order to assure termination Many consensus algorithm assume that there no Byzantine faults It is possible to make consensus robust against Byzantine faults as long as fewer than onethird of the nodes are Byzantine faulty Best known consensus algorithms are Viewstamped Replication Paxos Raft and Zab Most of these actually dont use the formal definition here of agreeing on one value while satisfying the properties instaed they decide on a sequence of values which makes them also total order broadcast algorithms Total order broadcast requires messages to be delivered exactly once in the same order to all nodes This is equivalent to performing several rounds of consensus each round nodes first propose what message they to send next and then decide on the next message to be delivered same message same order agreement no duplicate integrity message not corrupted validity messages are not lost termination Viewstamped Replication Raft MultiPaxos and Zab implement total order broadcast directly Paxos implements onevalueatatime consensus Single leader replication and consensus Isnt Chap 5s single leader replication essentially total order broadcast The answer comes down to how the leader is chosen If manually chosen then you have a total order broadcast of the notfaulttolerant way termination is violated as without manual intervention progress isnt made in case of leader failure Automatic leader election failover and promoting a new leader brings us closer to total order broadcast consensus Theres a problem however the split brain issue in which two nodes think themselves the leader at the same time We then have to have all nodes agree on who the leader is to achieve consensus and to have all nodes agree is itself a consensus problem All of the consensus protocols discussed so far internally use a leader in some form but they dont guarantee the leader being unique Instead they make a weaker guarantee the protocols define an epoch number ballot number Paxos view number Viewstamped Replication term number Raft and guarantee within each epoch the leader is unique Every time current leader is thought to be dead a vote is started to elect a new leader This election is given an incremented epoch number totally ordered and monotonically increasing If leader from the last epoch wasnt dead after all then leader with the higher epoch number prevails Before a leader is allowed to decide anything it must first check there isnt another leader with a higher epoch number To do so it must collect votes from a quorum of nodes for every decision a leader wants to make it must send the proposed value to the other nodes and wait for a quorum of nodes to respond in favor of the proposal The quorum typically consists of a majority of nodes A node votes in favor of a leaders proposal only if it is not aware of any other leader with a higher epoch Two rounds of voting choosing a leader then on its proposal Key insight is quorum for these two votes must overlap Thus if the vote on a proposal does not reveal any highernumbered epoch Difference with 2PC is that Coordinator is not previously selected and going ahead requires votes from a majority but not every node Limitations Consensus provides useful properties to distributed where everything is uncertain can be used to implement linearizable atomic operations total order broadcast in a fault tolerant way However they are not used everywhere because of the benefits coming at a cost The process by which nodes votes on proposals before they are decided is a kind of synchronous replication Consensus systems always require a majority to operate 3 to tolerate 1 failure 5 to tolerate 2 Most consensus algorithms assume a fixed set of nodes that participate in voting meaning you cant add or remove nodes in the cluster Dynamic membership extensions would allow the above but they are much less well understood Consensus systems generally rely on timeouts to detect failed nodes and can be hard to apply in environments with highly variable network delays such as geographically distributed systems Frequent reelection in such cases could cause the system to end up spending more time choosing a leader than doing useful work Sometimes consensus algorithms are particularly sensitive to network problems Raft has been shown to bounce leaders often if one particular network link is consistently unreliable Membership and coordination services ZooKeeper etcd are often described as described as distributed keyvalue stores or coordination and configuration services They offer APIs looking like reading writing value for a given key and iterating over keys As an application developer it is rare for one to directly interact with ZooKeeper instead one would rely on it via other projects HBase Kafka Hadoop YARN all rely on ZooKeeper running in the background ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory You wouldnt want to store all your applications data here and this small amount of data is replicated across all nodes using a faulttolerant total order broadcast algorithm each message broadcasted is a write to DB and applying writes in the same order provides consistency across replicas ZooKeeper is modeled after Google Chubby implementing total order broadcast and other features like Linearizable atomic operations A lock can be implemented using an atomic compareandset The consensus protocol guarantees the operation is atomic and linearizable even upon node failure A distributed lock is usually implemented as a lease which has an expiry so that its eventually released in case of client failure Total ordering of operations ZooKeeper can provide fencing token as a monotonically increasing number increases every time a lock is acquired ZooKeeper provides this by total ordering all operations and giving each a monotonically increasing transaction ID and version number Failure detection Clients maintain a longlived session on ZooKeeper servers and the client and server periodically exchange heartbeats to check the other node is still alive If heartbeats cease for a duration longer than the session timeout ZooKeeper declares the session dead Any locks held by a session can be configured to be automatically released when session times out Change notification Clients can read locks and values created by another and also watch for changes Eg it can find out when a client joins fails via notifications Out of these only linearizable atomic operations requires consensus but the rest makes ZooKeeper useful for distributed coordination One example where ZooKeeper Chubby model works well is when you want a leader elected from several instances of a process or service Useful for singleleader DB and also job scheduler etc Another example is when you have partitioned resource and need to decide which partition to assign to which node and rebalance load when new nodes join leave can be implemented with ZooKeeper atomic operations and notifications Libraries like Apache Curator provide higherlevel tools on top of ZooKeeper client API An application may grow from running on a single node to thousands of nodes Majority vote over this many would be inefficient so instead ZooKeeper is configured to run on a fixed number of nodes 3 5 Normally the kind of data managed by ZooKeeper is quite slow changing on the timescale of minutes hours Tools like Apache Bookkeeper can be used for faster changing state of the application Another use case is service discovery find the IP address to talk to for a service It is less clear whether service discovery actually requires consensus DNS is the traditional way of looking up IP address for a service name it uses multiple layers of caching and DNS reads are absolutely not linearizable Leader election does require consensus Some consensus systems support readonly caching replicas to log the decision of consensus but not actively participate in voting to help other nodes ZooKeeper etc can be seen as part of research into membership services deciding which nodes are currently active With unbounded network delay its not possible to reliably detect whether another node has failed But if you decide failure with consensus nodes can come to an agreement about which nodes should be considered alive With membership determined and agreed upon choosing a leader can be simply choosing the lowest numbered among current members Summary The consistency model linearizability where replicated data appears as though there is only one copy and all actions act on it atomically Causality is a weaker consistency model where not everything has to be in a single totally ordered timeline version history can be a timeline with branching and merging With causal ordering things like no two users can claim the same username still cannot be implemented distributedly which led to consensus A wide range of problem are equivalent to consensus linearizable compareandset register decides to set or abort based on comparison of given value and current value atomic transaction commit db deciding whether to commit or abort total order broadcast locks and leases deciding which client holds it membership coordination uniqueness constraint These are straightforward with a single leader but if the leader fails the system stops making progress To handle the situation we can wait for leader to recover 2PC manual failover choose a new leader automatically consensus problem If you find yourself wanting one of the above reducible to consensus and you want it to be fault tolerant try tools like ZooKeeper Not every system requires consensus leaderless and multileader replication systems typically dont use global consensus maybe its Ok when multiple leaders dont agree we may be able to merge branching version histories Part 3 Derived data Systems of records source of truth if there is any discrepancy between another system and the system of record system of records data is by definition the correct one Written once and typically normalized Derived data systems process transform existing data in some way If you lose it you can always regenerate it Eg cache Technically redundant but good for read performance Being clear about which is which helps bring clarity on a confusing system Whether the DB stores systems of records or derived data is up to your application and how you use such data Chap 10 Batch Processing The first two parts talk about request response in an online system triggered by user services Response time is important A different system is an offline system batch processing system taking in a large amount of input data runs a job to process it and may take a while Usually scheduled periodically Throughput is important Stream processing system is something in between a nearrealtime processing system Operates on events shortly after they happen MapReduce is a batch processing algorithm and was subsequently implemented in Hadoop CouchDB MongoDB Batch processing is a very old form of computing and MapReduce bears an uncanny resemblance to the electromechanical IBM cardsorting machines Batch processing with Unix tools awk print x uniq c sort r n sed grep xargs etc Powerful and performs well worth learning Linux sort automatically handles larger than memory workload by spilling to disk and automatically parallelizes sorting across multiple CPU cores A chain of Unix commands easily scales to a large dataset without running out of memory Note that mergesort has sequential access patterns that perform well on disks remember that optimizing for sequential IO was a recurring theme earlier Unix pipes reflects well the design philosophy of Unix make each program do one thing well to do a new job build afresh rather than complicating the old with features expect the output of every program to be the input to another yet unknown program dont clutter output with extraneous information dont insist on interactive input avoid stringently columnar or binary formats design and build to be tried early ideally within weeks dont hesitate to throw away the clumsy part and rebuild use tools in preference to unskilled help to lighten a programming task even if you have to detour to build the tools and expect to throw some of them out after using them automation rapid prototyping incremental iteration friendly to experimentation breaking down large projects into manageable chunks very much like Agile of todays Many Unix programs can be joined together in flexible ways Unix expects output of one to be input of another ie exposing a uniform interface to achieve this composability On Unix that interface is a file descriptor can be a device driver network socket communication channel to another program unix sockets stdin etc having all these share an interface is quite remarkable By convention many of Unix program will treat this sequence of bytes from a file descriptor as ascii Another characteristic of Unix tools is their use of stdin and stdout a program can read write files if it needs to but the Unix approach works best if the tool doesnt worry about file paths and instead interact with stdin stdout so that logic and writing are separated the program doesnt care about where the input is coming and where the output goes to somewhat similar to the ideas loose coupling late binding and inversion of control stdin and stdout have limitations eg program with multiple inputs outputs will be tricky and you cant pipe your output to a network connection etc Part of what makes Unix tools so successful is they make it easy to see whats going on input is immutable output can be piped to less for inspection you can write the output of pipeline stages to a file and use that as the starting point for the next stage allowing you to restart the later stage without rerunning the entire pipeline The biggest limitation of Unix tools is they only run on a single machine and this is where the likes of Hadoop come in MapReduce Similar to Unix tools but distributed MapReduce usually does not modify input and produces output as sequential file writes no random access and modification Reads and writes from distributed file system like Hadoops HDFS based on its proprietary counterparty GFS HDFS also follows a sharednothing architecture and a NameNode is in charge of keeping track of directory and block mapping like the single master in GFS HDFS scaled well to over tens of thousands machines storing over PBs of data Workflow Input is split into records For each record a Mapper handles each record independently and can generate any number of key value pairs from the record eg extracting some field from a record The MapReduce framework takes key value pairs generated by the Mapper collects all values belonging to the same key and calls the Reducer with an iterator over that collection of values the Reducer then produce output records Each input to a Mapper is typically hundreds of MBs in size the scheduler tries to run each mapper on a machine that stores a replica of the input file provided that the machine has enough spare RAM and CPU putting the computation near the data less copying over the network and better locality The number of Mappers is typically decided by the number of input blocks and the number of reducers is typically user specified To ensure same keys end up on the same Reducer the framework maps a hash of the key to the reducer task The key value pairs must be sorted since the dataset is likely too large to be sorted in memory instead each Map partitions its output by Reducer based on the hash of key then each of these partitions is written to a local sorted file using SSTable LSM trees Whenever a Mapper finishes writing its sorted output files the scheduler notifies the Reducer it can start fetching output sorted file from the Mapper The process of partitioning by reducer sorting and copying data from mappers to reducers is known as shuffle Reducer takes files from mappers and merge them together preserving the sorted order even if different Mappers produced the same keys for this Reducer Reducer is called with a key and an iterator that sequentially scans over records with the same key uses arbitrary logic to process these records and generate any number of output records which are written to a file on a distributed file system It is very common for MapReduce jobs to be chained into workflows Hadoop MapReduce does not have particular support for workflows chaining done implicitly eg by different directory names used by different MapReduce jobs This is slightly different from Unix pipes where output of one is immediately fed as input to the next over a small memory buffer This materialization of intermediate state has pros and cons MapReduce discards partial output of a failed job and dependent jobs can only start when their dependencies finish successfully A number of workflow schedulers are introduced for Hadoop to handle this dependency diagram These need good tooling support to manage complex dependency diagrams dataflows Reduceside joins and grouping Its common for a data record to have an association with another via foreign key in a relational model document reference in a document model or an edge in a graph model Join happens when you need both sides of the reference Denormalization can reduce the need for joins In a DB with index joins typically involve multiple indexbased lookups MapReduce jobs has no indexes conventionally it scans the entirety of input files Joins in the context of batch processing means resolving all occurrences of some association within a dataset An example problem for a MapReduce job is given user clickstream records identified by an user ID associate each record with user details stored in a remote users DB think denormalization stars schema is also related A common approach is for the job to take a copy of the user database and put it on the same distributed file system the map job is running on One set of mappers would go through partitioned clickstream records extracting the user ID as key and relevant info another set of mappers would go through the user DB also extracting the user ID as key and other relevant info The MapReduce framework would then partition mapper output by key in sorted order and user record with the same ID ends up on the same reducer adjacent to each other and the reducer can perform join logic easily Secondary sort can even make these key value pairs appear in a certain order This algorithm is known as a sortmergejoin The key emitted by mapper in this case is almost like an address designating which reducer this goes to MapReduce framework separates the physical communication and failure handling aspect from application logic Besides join another common pattern is groupby The simplest way to set this up is to let mappers use grouping key as key Grouping and joining look similar in MapReduce The pattern of bringing all records with the same key to the same place breaks down if very large amount of data are associated with the same key linchpin objects hot keys and the process of collecting all data related with that key leads to skew hot spot If join input has hot keys Pigs skewed join algorithm first runs a sampling to job to decide which keys are hot and mapper sends a record associated with a hot key to a randomly chosen reducer instead of a deterministic one records relating to the hot key would also be replicated over all reducers And you can add another MapReduce job to aggregate the more compact results that the randomly chosen reducers produced Hive takes a different approach to handling hot keys it requires hot keys to specified explicitly in table schema and it uses a mapside join for that key when joining Mapside joins The above performs join logic in the reducers and mappers prepare the input data This has the advantage of not needing to make any assumptions about the input datas properties structure Downside is sorting copying to reducers and merging reducer results can be expensive You can perform faster mapside joins if you know certain things about the data No reducers would be needed The simplest way broadcasthashjoin is when a large dataset is joined with a dataset small enough to be loaded entirely into memory of each mapper The mapper can then load the joindataset into memory and as it goes through its chunk of records it can perform join and produce joined output Pig Hive Impala etc all support this If the inputs to mapside joins are partitioned in the same way as user DB is then hash join can be applied to each partition independently Each mapper only needs to load the partition of DB that would contain records relevant to its input records This is a partitioned map join If input is not only partitioned the same way but also sorted the same way then a mapside merge join is possible and user DB partition does not have to fit entirely into mappers memory In the Hadoop ecosystem this kind of metadata about the partitioning of datasets is often maintained in HCatalog and the Hive metastore Note mapside join outputs chunks of output files sorted in the same order as the chunks of input and reduceside join is chunks of records partitioned and sorted by the join key Output of batch workflows Recall transaction processing workload with analytics workload have different characteristics Batch processing fits more in analytics but not quite analytics MapReduce was originally introduced to build indexes for Googles search engine and remains a good way to build indexes for Lucene Solr Google has moved away from this Building documentpartitioned indexes and building classifier systems and recommendation systems parallelizes very well Since querying a search index is a readonly operation these index files are immutable once created unless source documents change The output of such jobs is usually some database where a web interface would query and separate from the Hadoop infrastructure Writing directly to the DB from your Mapper or Reducer job may be a bad idea since Making a network request per record is not performant even if the client library supports batching MapReduce jobs often run in parallel and all mappers and reducers writing to the same DB concurrently may overwhelm it Finally MapReduce provides a clean allornothing guarantee for job output however writing to an external system from inside a job produces visible sideeffect and you have to worry about results of partial execution being available to the rest of the system MapReduce jobs output handling follows Unix philosophy by treating input as immutable and avoiding side effects such as writing to a DB This minimizes irreversibility makes rollback easier only code rollback is needed not database too Makes retrying a partial failed job easier The same set of files can be used as input for various jobs And like Unix tools this separate logic from wiring Comparing Hadoop to Distributed Databases Hadoop is like a distributed version of Unix where HDFS is the file system and MapReduce a sortedshuffling distributed implementation of Unix process The ideas of MapReduce has been present in socalled massively parallel processing MPP databases for a while But MapReduce distributed file system provides something much more like a general purpose OS Diversity of storage Some difference between MapReduce and MPP DBs include DBs require you to structure data according to a particular model Hadoop opens up possibility of indiscriminately dumping data into HDFS and only later figure out how to process it further The idea is similar to a data warehouse simply bringing data from various parts of a large organization together in one place is valuable and careful schema design slows this process down This shifts the burden of interpretting data to the consumer schemaonread the sushi principle raw data is better Diversity of processing models MPP databases are monolithic with query planning scheduling and execution optimized for specific needs of the DB eg supporting SQL queries If you are building recommendation systems and full text search indexes then merely SQL is usually not enough MapReduce provided the ability to easily run custom code over large dataset You can build SQL with MapReduce Hive did this and much more Subsequently people found MapReduce performed too badly for some types of processing so various other processing models have been developed over Hadoop The Hadoop ecosystem includes both randomaccess OLTP databases such as HBase open source BigTable with SSTable and LSM trees as well as MPPstyle analytic DBs like Impala Neither uses MapReduce but both use HDFS Designing for frequent faults MPP databases usually let user resubmit the entire query upon failure this is fine for typically seconds minutes long analytic jobs while MapReduce allows retrying at the granularity of a job MPP DBs also prefer keeping as much data in memory as possible while MapReduce is more eager to write to disk This is related with Googles environment of mixeduse datacenters in which online production services and offline batch jobs run on the same machines and every task has a resource allocation enforced using containers This architecture allows nonproduction lowpriority jobs to overclaim resources to improve the utilization of machines As MapReduce runs at low priority they are prone to being preempted by higher priority processes needing their resource relying less on inmemory states and more on writing to disk is preferable Current open source schedulers uses preemption less Beyond MapReduce MapReduce is only one programming model for distributed systems MapReduce provides a simple abstraction over a distributed filesystem but is laborious to use Pig Hive Cascading Crunch are abstractions over MapReduce to address the hardtouseness MapReduce is general and robust but other tools can be magnitudes faster for certain kinds of processing Materialization of intermediate state MapReduce chain jobs by writing the first to file and having the second pick up from where the file is written This makes sense if the result of first should be made widely available and reused as input to different jobs but not when output of one job is only ever used as input to one other job intermediate state This materialization of internal state differs from piping Unix commands This has following downsides A MapReduce job can start when all tasks in the preceding jobs have completed whereas processes connected by a Unix pipe can start at the same time with output consumed as soon as its produced This having to wait contributes to having more stragglers in execution and slows down the pipeline Mappers are often redundant they read back the same file that was just written by a reducer and prepare it for the next stage of partitioning and sorting if the reducer output was partitioned and sorted in the same way as mapper output then reducers could be chained together directly without interleaving with mapper stages Storing intermediate state in a distributed file system who replicates them is often overkill for temporary data Dataflow engines To address the above issues several execution engines for distributed batch computations were developed including Spark Tez and Flink They handle an entire workflow as one job rather than breaking it up into independent subjobs They explicitly model the flow of data through several processing stages hence they are known as dataflow engines Like MapReduce they work by repeatedly calling a userdefined function to process one record at a time on a single thread They parallelize work by partitioning inputs and then copy the output of one function over the network to become the input to another function Unlike MapReduce these functions operators dont have the strict roles of Map and Reduce but instead can be assembled in more flexible ways One option is to repartition and sort records by key like in shuffle stage of MapReduce This enables sortmergejoin and grouping as MapReduce would Another possibility is to take several inputs and to partition them in the same way but skip the sorting This saves effort on partitioned hash joins where partitioning is important but the order is not as hash randomizes it anyway For broadcast hash joins the same output from one operator can be sent to all partitions of the join operator This processing engine style offers advantages over MapReduce Expensive work like sorting only need to be performed where it is required No unnecessary Map tasks Because all joins and data dependencies in a workflow are explicitly declared the scheduler has an overview of what data is required where so it can make locality optimizations It is usually sufficient for intermediate state between operators to be kept in memory or written to a local disk which requires less IO than writing to HDFS MapReduce uses this optimization for Mapper output but dataflow engines generalize it to all intermediate state Operators can start as soon as input is ready no need to wait for for the entire preceding stage to finish before the next one starts Existing JVM processes can be reused to run new operators reducing startup overheads compared to MapReduce which launches a new JVM for each task They can implement the same thing as MapReduce and usually significantly faster Workflows implemented in Pig Hive or Cascading can switch from MapReduce to Tez or Spark with simple configuration changes Tez is a fairly thin library relying on YARN shuffle service for copying data between nodes whereas Spark and Flink are big frameworks with their own network communication layer scheduler and userfacing API Faulttolerance is easy in MapReduce due to full materialization of intermediate states The dataflow engines avoid writing intermediate states to HDFS so they retry from an earlier latest stage possibly from original input on HDFS where intermediate data is still available To enable this recomputation the framework must keep track of how a given piece of data is computed inputs and operators applied to it Spark uses Resilient Distributed Dataset RDD to keep track of ancestry of data while Flink checkpoints operator state When recomputing data its important to know whether computation is deterministic Partial result was computed and delivered to downstream then upstream fails and if the delivered results can vary between retries then the downstream needs to be killed and restarted as well Its better to deterministic but nondeterministic behavior creeps in via unordered container iteration probablistic algorithms system clock usage etc Returning to the Unix analogy MapReduce is like writing the result of each step to a temporary file dataflow engines look more like Unix pipes A sorting operation inevitably needs to consume the entire input before producing any output any operator that requires sorting will thus need to accumulate state In terms of job output storage like MapReduce HDFS is still usually the destination instead of building DB writes into user defined functions Graph and iterative processing Think graphs in analytics workload Eg PageRank estimate the popularity of a webpage based on what other web pages link to it Many graph algorithms are expressed by traversing one edge at a time joining one vertex with an adjacent vertex in order to propagate information and repeating until no edges to follow or some convergence This repeating until done cannot be expressed in plain MapReduce and is thus often implemented in an iterative style External scheduler runs a batch process to calculate one step of the algorithm Batch finishes scheduler checks whether the iterative algorithm has finished If not run another round of batch process This is often inefficient Bulk synchronous parallel model of computation has become popular as a result Apache Giraph Sparks GraphX and Google Pregel model In Pregel one vertex can send a message to another vertex and typically those messages are sent along the edges in a graph in each iteration a function is called for each vertex passing the function all the messages that were sent to that vertex a bit similar to the actor model if you think of each vertex as an actor except that vertex state and messages between vertices are faulttolerant and durable Fault tolerance in Pregel is achieved by periodically checkpointing the state of all vertices at the end of an iteration The framework decides which vertex executes on which nodes when a vertex sends messages to other vertices it simply sends a vertexID High level APIs and languages Since MapReduce the execution engines have matured by now the infrastructure has become robust enough to store and process petabytes of data on over 10k machines Physically operating batch processes at such scale has been considered more or less solved attention has turned to other areas improving programming model efficiency of processing and broadening the set of problems these technologies can solve Eg declarative query languages specialization for different domains Summary Unix tools and philosophy MapReduce and data flow engines Two problems each distributed batch processing frameworks need to solve Partitioning MapReduce partitions according to input file blocks output of mappers is repartitioned sorted and merged into a configurable number of reducer partitions post MapReduce dataflow engines try to avoid sorting unless required but otherwise take a broadly similar approach to partitioning Fault tolerance MapReduce frequently writes to disks and materializes internal states Dataflow engines perform less materialization deterministic operators heko reduce the amount of data that needs to be recomputed Join algorithms in MapReduce Sort merge joins Broadcast hash joins Partitioned hash joins Distributed batch processing engines have a deliberately restricted programming model callback functions such as mappers and reducers are assumed to be stateless and have no visible side effects Batch processing job derives some output from bounded immutable input and a job knows when it has finished reading the entire input Next chapter turns to stream processing where the input is unbounded and a job is never complete Chapter 11 Stream Processing In general a stream refers to data that is incrementally made available over time TCP stream stdin stdout etc A stream as a sequence of events which is similar to records in the bounded filebased input output case from previous chapter In batch processing a file identified by filename is read by multiple jobs analogously events grouped into a topic or a stream in stream processing is generated by one producer and consumed by multilpe consumers In principle a file or database can connect producers and consumers periodic polls by consumers which is expensive Or database triggers but specialized tools have been developed for delivering events notification Messaging systems A Unix pipe or TCP connection between producer and consumer would be a simpple messaging system but they connect onetoone Different systems have been developed for this pubsub model and to differentiate them its generally helpful to ask What happens if producers produce faster than consumer can consume Approaches are drop backpressure flow control as in TCP Unix pipes to block the producer or buffering in a queue What happens if the queue grows beyond memory limit Crash or dump to file system What happens if nodes crash or go offline Are any message lost Durability has a cost Not all losses are unacceptable Batch processing system also provide strong reliability guarantee in retrying failed jobs automatically Later well see how to do the same with stream processing Direct messaging between producers and consumers Without any intermediate nodes UDP multicast especially where low latency is important Application level protocol can optionally build retransmission on error Brokerless messaging like ZeroMQ implement pubsub over TCP IP multicast Producer making a direct RPC HTTP request to consumer if consumer exposes a service on the network webhooks with callback url when registering These generally require the application code to be aware of possibility of loss And when consumer is offline some protocol retries delivery but producer may break down from buffering up too much Message brokers Aka message queue a kind of DB optimized for handling message streams It runs a server with producers and consumers connected to it as clients producers write to it consumers read from it By centralizing data in the broker these systems can more easily tolerate clients that come and go Broker handles durability some are stored in memory others in persistent storage Generally allow unbounded queueing to accommodate slow consumers sometimes configurable to backpressure A consequence of queueing is consumers are generally asynchronous producer only cares about brokers response that it got the event not knowing consumer side of things Message brokers vs DB Some brokers can participate in 2PC protocols for distributed transactions making them more similar to DBs Key differences include Brokers typically keep the message until its delivered to consumers and are not geared towards long term storage DB only deletes when explicitly told to Due to the above brokers typically assume the working set is small DBs typically support secondary indexes and various ways to search for data while brokers often support some way of subscribing to a subset of topics matching some pattern The mechanisms are different Querying DB typically returns a pointintime snapshot of the data and the client is not notified of data changes By contrast message brokers do not support arbitrary queries but instead notify clients when data changes Standards like AMQP and JMS are implemented in RabbitMQ Google Cloud PubSub etc Multiple consumers When multiple consumers are present for the same topic two patterns of messaging are usually present Load balancing each message to one consumer for parallel processing Fanout each message to all consumer the streaming equivalent of having multiple jobs read the same input and not affecting each other The two can be combined eg fanned out to two groups where each group runs load balancing inside Acks and redelivery Consumer can crash while processing a message so a broker waits for consumers confirmation acknowlegding it has finished processsing a message to delete a message The ack could be lost making the broker redeliver an already processed message Due to the possibility of crashing a consumer may see brokers messages outoforder in a load balancing scenario a message being processed by a crashed consumer when redelivered to another consumer may already be behind the chronologically later messages the consumer has processed Even if the standard requires the broker to try to preserve ordering combination of redelivery and loadbalancing workload makes reordering possible One can use one queue per consumer not do load balancing to avoid this or its possible for this to not be an issue for messages that are not causally related Partitioned logs Due to the transience of data mindset in message brokers a key feature for batch processes replayable derived data reprocessing runs no risk of damaging input is not the case with AMQPJMSstyle messsaging Typically receiving a consumer ack and the broker deletes the message but we can do a hybrid of durble storage of DBs and lowlatency notification of messaging logbased message brokers Logbased message broker This is essentially partitioned and distributed tail f Producer produces to a log and consumer tails it For higher throughput the log is partitioned different partitions hosted on different machines A topic can then be defined as a group of partitions that all carry messages of the same type With each partition the broker assigns a monotonically increasing sequence number offset to each number as a partition is appendonly and messages within a partition are totally ordered There is no ordering guarantee across partitions Kafka Amazons Kinesis Streams and Twitters DistributedLog are logbased message brokers Google Cloud PubSub is architecturally similar but exposes a JMSstyle API Even though these message brokers all write to disk they are able to achieve throughput of millions of messages per second by partitioning across machines and fault tolerance by replicating messages The logbased approach trivially supports fanout as reads are independent and dont delete from the log Load balancing is achieved by instead of assigning individual messages to consumer clients in a group the broker can assign entire partitions to nodes in the consumer group Each client then consumes all the messages its assigned vs JMSstyle message brokers This coarsegrain load balancing has some downsides The number of nodes sharing the work of consuming a topic can be at most the number of log partitions in that topic because messages within the same partition are delivered to the same node even though messages of the same partition get to all nodes serving that partition one could let one node in that group ignore oddnumbered messages and another ignore evennumbered In general having more partitions is scalable If a single message is slow to process it holds up the processing of subsequent messages in that partition headofline blocking Thus in situations where messages may be expensive to process and you want to parallelize processing on a messagebymessage basis the JMSAMQP style of message broker is preferable For high throughput where each message is fast to process and ordering is important the logbased approach works well Consumer offsets Consumers consume a partition sequentially so all sequence numbers smaller than consumers current sequence number have been processed Thus the broker does not track acks for every message it only needs to periodically record the consumer offsets This reduced bookkeeping overhead helps scale logbased systems This offset is similar to the log sequence number in singleleader database replication Broker leader database consumer follower If one node in the consumer group fails another from the group picks up from the last recorded offset leading to some messages potentially processed twice Disk space Only append and youll eventually run out Over time older segments of the partition is deleted Eventually if a slower consumer cannot keep up itll have offsets pointing to a deleted segment This boundedsize buffer and discarding old messages is usually implemented as a ring buffer circular buffer usually quite large in size This contrasts with inmemory message storage and only spilling to disk when memory is used up When consumers fall behind Logbased with circular buffer is essentially a buffer drop approach If one consumer falls behind others are not affected When a consumer shuts down crashes it stops consuming resources only its offset remains Whereas in traditional message brokers you need to be careful to delete any queues whose consumers have been shut down as they otherwise continue accumulating messages in memory unnecessarily Replaying old messages In JMSAMQPstyle brokers processing ack is destructive it causes messages to be deleted In a logbased message broker consuming is just reading from a file it is readonly the offset moves but the consumer is in charge of the offset and gets to specify it to any point Logbased is then more similar to batch processing in the sense that derived data is clearly separated from input data the process is repeatable and input data is immutable This allows easier experimentation and recovery making logbased message brokers a good tool for integrating dataflow within an organization Databases and streams Log based message broker applies the idea of databases to message brokers the reverse is doable too The event in the message broker in question is now a DB write A replication log is a stream of database write events produced by the leader as it processes transactions State machine replication in total order broadcast is another case of a stream of events in which each replica processes the same set sequence of events in the same order to arrive at the same final state Keeping systems in sync There can be multiple copies of the same data stored to serve different needs in a cache a fulltext index a data warehouse etc With data warehouse this synchronization is usually performed by ETL processes often by taking a full copy of a database transforming it and bulk loaded into the data warehouse by a batch processing job If periodic dump batch processing is too slow an alternative is dual write the application code explicitly writes to each system when data changes sometimes concurrent updates to them Dual writes can have race conditions arising from multiple clients trying to write at the same time leaving different subsystems different views which wont converge over time Another is fault tolerance problem writing to one failed but another succeeded leaving the two subsystems permanently outofsync Ensuring both succeed or fail here is a case of the atomic commit problem which is expensive to solve eg with 2PC These problems could be avoided if only one leader exists among the subsystems This can be achieved with change data capture the process of observing all data changes written to a database and replicating the change to other systems with changes as a stream This essentially makes one database the leader and others are followers listening to fanout from a message broker The communication is asynchronous database does not wait for consumers response before committing the data DB triggers or change log parsing can be used to implement change data capture LinkedIn Databus Facebook Wormhole Kafka Connect framework all offer CDC for various databases Keeping the entire DB change log for this reason would be too much space and too much time to play through instead we use a snapshot offsets One can also do log compaction on records with a CDC whenever it sees the same key the previous value is replaced More and more DBs are exposing CDC API as opposed to retrofitting their implementation Kafka Connect is an effort to integrate CDC tools for a wide range of database systems with Kafka Event sourcing CDC is similar to event sourcing in domain driven design community While both having a log event sourcing applies the idea at a different level of abstraction CDC users use the DB in a mutable way low level log is extracted and its effects replicated writer tothe DB does not know its doing CDC underneath Event sourcing application logic is built on the basis of immutable events written to the log Events are designed to reflect things at the application level and not low level state changes Event sourcing is a powerful technique by making it easier to evolve application over time and understanding why something happened Replaying the event log gives the current state of the system Log compaction are handled differently in CDC and event sourcing systems CDC log events usually contain brand new values for a key and log compaction can discard previous events Event sourcing events are modeled at a higher level later events usually dont overwrite previous events The event sourcing philosophy is careful to distinguish between events and commands User request comes as a command which may fail integrity condition checks if command validation is successful it becomes a durable and immutable event fact A consumer of event streams is not allowed to reject events thus any validation of commands need to happen synchronously before it becomes an event eg by using a serializable transaction that atomically validates and publishes State streams and immutability Immutability of input files enables replaying in batch processing immutability also makes event sourcing and change data capture powerful States change is the result of events that mutated it over time If you store the changelog durably that simply has the effect of making the state reproducible and it becomes easier to reason about the flow of data through a system From this perspective the database is a cached subset of the log ie the latest record values in log and the truth is the log Immutability in DBs is an old idea An accountants ledger is immutable Immutable events also capture more information than just current state You can also derive different readoriented representations from the same log of events making it easier to evolve your application over time eg building new optimized view of some data Storing data becomes easier if you dont have to worry about its access pattern complex schema design indexing storage engines are results of access pattern Having an immutable log gives you a lot of flexibility by separating the form in which data is written from the form it is read by allowing several different read views This is known as command query responsibility segregation The traditional approach to database and schema design is based on the fallacy that data must be written in the same form as it will be queried Debates about normalization and denormalization become largely irrelevant if you can translate data from writeoptimized event log to a readoptimized application state It entirely makes sense to denormalize data in the readoptimized views as the translation process will keep data consistent with event log The biggest downside to event sourcing change data capture is building derived view is usually asynchronous Readyourwrite consistency cannot be achieved if user writes to log and reads from derived view You could have the log update and derived view update in one transaciton but this would require both to be in the same system or have distributed transaction support in heterogeneous systems or one could use total order broadcast Concurrency is also made simpler in that multiobject transactions can now be one event that describes the user action and different derived views build from it If the event log and application state processing an event for user on partition A only requires partition A of the application state then a straightforward singlethreaded log consumer needs no concurrency control for writes actual serial execution removes the need to define a serial order in a partition Limitation of immutability Maintaining an immutable history for all changes can be expensive for workloads with lots of updates and deletes on a small dataset in size and fragmentation Compaction and garbage collection becomes key for robustness There may also be regulatory cases requiring data to be deleted shunning excision one cant just append a delete event history actually needs to be rewritten True deletion is surprisingly hard due to replication and hardware storage mechanism Its more like making it hard to retrieve the data yet one still has to try Processing Streams How are streams used The above discusses where streams come from and how they are transported Common things that happen in processing the stream include writing to DB cache search index or some storage system push events to users realtime dashboard emails etc produce an output stream from inputs and pipe it to another Use case 3 is in many ways similar to the batch workflow dataflow engines with one crucial difference being stream never ends Implications include sorting no longer makes sense and sortmergejoins cannot be applied Fault tolerance mechanism also need to change batch job running for minutes can be restarted stream cannot simply be replayed from start which may be years ago Common uses include Complex events processing where queries are stored long term and events from input streams continuously pass through queries and find matching ones like Herald rules Stream analytics like complex events processing but usually measuring the rate stats of some type of events like GUTS Apache Storm Spark Streaming Kafka Streaming Google Cloud Dataflow Maintaining materialized views a derived alternative view for certain query patterns Difference from above being the view usually stretches back to the beginning of time as opposed to stats in a time window which Kafka Streams also supports Search on streams like complex events processing multievent patterns there is also complex search for individual events Elasticsearch offers this Index queries such that the number of queries run is lower than number of events times the number of queries RPC systems eg in the actor model for managing concurrency and distributed execution of communicating modules Reasoning about time The average over last 5 minutes can be surprisingly tricky A batch process rarely cares about system wall clock time the time at which the process is run has nothing to do with the time when the events happened On the other hand many stream processing frameworks use local system clock on the processing machine to determine windowing which is simple but can break down if there is significant lag between event occurrence and being processed Confusing event time and processing time can lead to bad data A tricky problem with defining window in terms of event time is that you can never be sure when you have received all of events for a particular window or whether there are more events to come delays stragglers outoforder Broadly you have two options ignore stragglers and track the amount of stragglers dropped and warn if that number gets high publish a correction you may also need to retract the previous output In some cases it is possible to use a special message to indicate from now on there will be no messages with a timestamp earlier than t but tracking such from multiple producers stamping with their own local time might be tricky Taking a step back which point in time should we use If we rely on users device clock they might be wrong drift etc We could log three timestamps device time at which the event occurred device time at which the event is sent to server in case user device was not connected for a long time server time of when the server receives the event Use the difference of the latter two to estimate the server time of the first one assuming network delay is negligible compared with the difference between device and server clocks Types of window Tumbling window Fixed length nonoverlapping 0 4 5 9 etc Each event belongs to exactly one window Hopping window Fixed length overlapping 0 4 1 5 etc This provides smoothing over several windows Sliding window Fixed length any events occuring within windowsize of each other would occur in the same window This is typically implemented with a buffer of events sorted by time and removing them when they expire Session window Variable length a window of all events in a session as defined by the application Stream joins Streamstream join eg correlating user search keyword and url click in the same session Variable delay in between Note that annotating url click with search query isnt enough as that does not include cases where search resulted in no click important for your clickthrough rate To achieve this stream processor requires states eg keyed by session ID and kept for an arbitrary 1 hour Streamtable join stream enrichment Consider caching a copy of the database into the stream processor Similar to hashjoin in mapside joins The cached copy likely requires updates which can be achieved with listening to change data capture This is actually similar to streamstream join where the CDC stream reaches back to the beginning of time Tabletable join materialized view maintenance Imagine the goal is to maintain a materialized view ones twitter feeds a timeline cache where a stream of tweets is joined with follow relationship the timeline corresponds to the join of tables in a relational database a view updated by stream change These all require the stream processor to maintain some state on one join input and query that state on messages from the other join input Time order of events that maintain the state can be important if the user updates his profile which activity events are joined with the old profile and which with the new If ordering of events across streams is undetermined the join becomes nondeterministic This issue is known as slowly changing dimension and is often addressed with explicitly versioning each time the user profile changes it is given a new version identifier and events include the version identifier to join on This makes join deterministic but log compaction is no longer possible as all versions are retained Fault tolerance Restarting for fault tolerance in a sideeffectfree batch processing achieves exactlyonce semantics more accurately effectivelyonce as each record is processed effectively once Microbatching and checkpointing Break the stream into small blocks and treat each like a miniature batch process Microbatching in Spark Streaming creates implicit tumbling window Or you can trigger checkpointing without a fixed time window which Apache Flink does Side effects will not be exactlyonce Atomic commit revisited St side effects are exactlyonce This is an atomic commit approach which is used in Google Cloud Dataflow and Apache Kafka This was efficient enough within the system itself not needing to be heterogenous keep the transactions internal by managing both state changes and messaging within the stream processing framework Overhead of transaction amortized by processing several input messages within a single transaction Idempotence Alternative to distributed transactions we can have operations be idempotent or made idempotent with some metadata eg Kafka stream offset and consumer of stream only update on offset values not seen before this requires replay on crash in the same order processing to be deterministic and no other node may concurrently update the same value When failing over from one processing node to another fencing may be required to prevent interference Rebuilding state after a failure States such as windowed aggregations counters averages etc can be kept in a remote datastore or local to the stream processor and replicated periodically Kafka streams replicate state changes by sending them to a dedicated Kafka topic with log compaction similar to CDC Just replay may be faster and the tradeoffs such as this depend on performance characteristic of your system Summary Stream processing is similar to batch processing on unbounded input From this perspective message brokers and event logs serve as the streaming equivalent of a filesystem JMSstyle message broker Broker assigns individual messages to consumers consumers acks broker then deletes Old message cannot be read after processed Logbased message broker Broker assigns all messages in a partition to the same consumer node and always delivers in the same order Parallelism through partitioning and consumers track their progress by checkpointing the offset of the last message they have processed Messages are retained on disk and can be reread if needed Logbased is like DB replication log and LSMtrees Particularly appropriate for derived view generation from input streams Where streams come from Change data capture Event sourcing Representing database as streams Purposes of stream processing processing streams section Reasoning about time Different types of stream joins Fault tolerance with microbatching checkpointing transactions or idempotent writes The Future of Data Systems Even so called general purpose database is designed for a particular usage pattern Know your usage pattern and how it maps to the tools you use Cobbling together different pieces to achieve certain functionality is important In my experience 99 of people only need X probably says more about the experience of the speaker than the actual usefulness of the technology Combining specialized tools by deriving data If it is possible for you to funnel all user input through a single system that decides on an ordering of all writes generate the rest derived via CDC or event sourcing then it becomes much easier to derive other representations of the the data by processing the writes in the same order Plus idempotence makes it easy to recover from faults CDC vs distributed transactions Both can achieve the goal of keeping data in different systems in sync distributed transaction usually provide linearizability which implies the likes of reading your writes while CDC are usually updated asynchronously and make no such guarantee In the absence of widespread support for a good distributed transaction protocol logbased derived data is probably the most promising approach for integrating different data systems However different levels of consistency guarantees can be quite useful Single leader partitioned if needed in which case order of events in two partitions can be ambiguous Geographically distributed likely requires a leader in each location which implies undefined ordering of events that originate in two different datacenters In formal terms deciding on a total order of events is known as total order broadcast which is equivalent to consensus Most consensus algorithms are designed for situations in which the throughput of a single node is sufficient to process the entire stream of events and these algorithms do not provide a mechanism for multiple nodes to share the work of ordering of events Batch and stream processing Batch processing has a quite strong functional flavor it encourages deterministic pure functions whose output depends only on input and has no side effects Inputs are immutable and outputs are appendonly This makes reasoning about dataflow as well as faulttolerance easy Reprocessing data for application evolution Stream and batch processing are both useful in schema evolution Derived views allow gradual migration you can maintain both versions of the derived view as you migrate its almost like railway gauge migration pythonpolyglot where different track distances are standardized by having a third rail such that trains for both gauges can run The lambda architecture Combining batch processing and stream processing say run Hadoop MapReduce and Storm sidebyside Stream processor consumes the events and quickly produces an approximate update to the view and the batch processor later consumes the same set of events and produces a corrected version This somewhat duplicated work has its shortcomings Recent work overcomes this by having batch computations and stream computations implemented in the same system Unbundling databases The similarities between operating system and databases are worth exploring At their core both are information management systems stored at file or records level Hadoop is presented earlier as somewhat like a distributed Unix OS hides away the hardware provides files pipes whereas distributed databases hides away the disk storage concurrency crash recovery and provides distributed transactions SQL etc The author would like to interpret NoSQL movement as wanting to apply a Unixesque approach of lowlevel abstractions to the domain of distributed OLTP data storage Composing data storage technologies Secondary indexes materialized views replication logs fulltext search indexes There are parallels between these database features and the derived data systems that people are building with batch and stream processors In a sense they are almost like elaborate implementations of triggers stored procedures and materialized view maintenance routines Federated database unifying reads One unified query interface for a wide variety of underlying storage engines and processing methods Unbundled database unifying writes Make it easy to plug together storage systems eg through change data capture and event logs is like unbundling a databases indexmaintenance features in a way that can synchronize writes across disparate technologies The author thinks the traditional approach to synchronizing writes requiring distributed transactions across heterogeneous systems is the wrong solution compared with change data capture The big advantage of logbased integration is loose coupling between the various components Outages are easier to isolate and handle and different components can be developed improved and maintained independent of each other The advantage of unbundling and composition only come into picture when there is no single piece of software that satisfies all your requirements Whats missing The tools for composing data systems are getting better but the author thinks a major part is missing an unbundled equivalent of Unix shell ie a highlevel language for composing storage and processing system in a simple and declarative way Like mysql elasticsearch which is like an unbundled way of CREATE INDEX Designing applications around dataflow Like formula in an excel sheet derived view that automatically updates when a dependent cell changes derived data view should have the same Application code as a derivation function triggers stored procedures userdefined functions which often came as an afterthought Separation of code and state Like web servers who typically put states in DB and have code hosted by a web server running in cluster management tools like Kubernetes Docker etc Thinking about applications in terms of dataflow implies renegotiating the relationship between application code and state management Instead of thinking DB as passive variable modified by the application we think much more about the interplay and collaboration between states state changes and code that processes them Comparing this with a microservice approach eg when a user makes a purchase of goods priced in one currency but paid in another currency the microservice approach the code that processes transaction would query an exchangerate service to obtain the current exchange rate in the dataflow approach purchase processing would be one stream processor exchange rate would be another stream processor and the above becomes a join local cached rate query timedependent join Observing derived state The read path and write path encompass the whole journey of data Write path caching eager evaluation Read path lazy evaluation The derived dataset materialized view cache is the place where the two paths meet and there is a tradeoff between the amount of work needing to be done at read or write time Stateful offlinecapable clients Ondevice state becomes a cached version a materialized view of that on the server The offline device once reconnected will behave like reading off of an offset in a logbased message broker Websockets EventSource API gave http server pushing capability who otherwise had been a simple polling protocol This extends the write path all the way to the end user in which case we need to rethink building of many of our systems moving away from requestresponse interaction and toward publishsubscribe dataflow Keep an open mind on both as a designer of data systems Reads are events too Its possible to have event log only store writes to the database but also possible to have it store read events as well In some cases reads queries contain information to be stored and analyzed Writing reads makes it easier to track causal dependencies Using stream processors to implement multipartition data processing Probably simpler to use a DB that provides multipartition query joins but more customizable and flexible to implement this with stream processors Aiming for correctness is no dirty writes an atomicity consistency linearizability and isolation guarantee is linearizability the strongest form for replica consistency The furthest in line in eventual readyourwrite consistencyprefixread causal does linearizability imply atomicity how does it not imply serializable isolation the weakest form of isolation read committed also implies no dirty writes Is atomicity any more than no dirty reads no dirty writes read committed are distributed transactions and replica consistency connected problems does full write broadcast and rwtotalnodes achieve linearizability does 2PC give you anything on top of synchronous singleleader replication"},{"title":"\"Effective C++\"","href":"/notes/ecpp","content":" View C as a federation of languageseffectivesecppit1 Prefer const enum inline to defineeffectivesecppit2 Use const whenever possibleeffectivesecppit3 Make sure objects are initialized before they are usedeffectivesecppit4 Know what functions C silents writes and callseffectivesecppit5 Explicitly disallow the use of compiler generated functions you dont wanteffectivesecppit6 Declare dtors virtual in polymorphic base classeseffectivesecppit7 Prevent exceptions from leaving dtorseffectivesecppit8 Never call virtual functions during ctor or dtoreffectivesecppit9 Have assignment operators return a reference to thiseffectivesecppit10 Handle assignment to self in operatoreffectivesecppit11 Copy all parts of an objecteffectivesecppit12 Use objects to manage resourceseffectivesecppit13 Think carefully about copying behavior in resourcemanaging classeseffectivesecppit14 Provide access to raw resources in resource managing classeseffectivesecppit15 Use the same form in corresponding uses of new and deleteeffectivesecppit16 Store newed objects in smart pointers in standalone statementseffectivesecppit17 Make interfaces easy to use correctly and hard to use incorrectlyeffectivesecppit18 Treat class design like type designeffectivesecppit19 Prefer passbyreferencetoconst to passbyvalueeffectivesecppit20 Dont try to return a reference when you must return an objecteffectivesecppit21 Declare data members privateeffectivesecppit22 Prefer nonmember nonfriend functions to member functionseffectivesecppit23 Declare nonmember functions when type conversions should apply to all parameterseffectivesecppit24 Consider support for a nonthrowing swapeffectivesecppit25 Postpone variable definitions as long as possibleeffectivesecppit26 Minimize castingeffectivesecppit27 Avoid returning handles to object internalseffectivesecppit28 Strive for exceptionsafe codeeffectivesecppit29 Understand the ins and outs of inliningeffectivesecppit30 Minimize compilation dependencies between fileseffectivesecppit31 Make sure public inheritance models isaeffectivesecppit32 Avoid hiding inherited nameseffectivesecppit33 Differentiate between inheritance of interface and inheritance of implementationeffectivesecppit34 Consider alternatives to virtual functionseffectivesecppit35 Non virtual interface idiomeffectivesecppit36 The strategy pattern via function pointerseffectivesecppit37 Strategy pattern via stdfunctioneffectivesecppit38 The classic strategy patterneffectivesecppit39 To recapeffectivesecppit40 Never redefine an inherited nonvirtual functioneffectivesecppit41 Never redefine a functions inherited default parameter valueeffectivesecppit42 Model hasa or isimplementedintermsof through compositioneffectivesecppit43 Use private inheritance judiciouslyeffectivesecppit44 Use multiple inheritance judiciouslyeffectivesecppit45 Define nonmember functions inside templates when type conversions are desiredeffectivesecppit46 Uses traits classes for information about typeseffectivesecppit47 Be aware of template meta programmingeffectivesecppit48 Understand the behavior of the new handlereffectivesecppit49 Understand when it makes sense to replace new and deleteeffectivesecppit50 Adhere to conventions when writing new and deleteeffectivesecppit51 Write placement delete if you write placement neweffectivesecppit52 Pay attention to compiler warningseffectivesecppit53 Familiarize yourself with the standard library including tr1effectivesecppit54 Familiarize yourself with Boosteffectivesecppit55"},{"title":"\"Effective Modern C++\"","href":"/notes/emcpp","content":" Understand type deductioneffectivesemcppit1 Understand auto type deductioneffectivesemcppit2 Understand decltypeeffectivesemcppit3 Know how to view deduced typeseffectivesemcppit4 Prefer auto to explicit type declarationseffectivesemcppit5 Use the explicitly typed initializer idiom when auto deduces undesired typeseffectivesemcppit6 Item 7 distinguish between and when creating objectseffectivesemcppit7 Item 8 prefer nullptr to 0 and NULLeffectivesemcppit8 Item 9 prefer alias declarations to typedefseffectivesemcppit9 Item 10 prefer scoped enums to unscoped enumseffectivesemcppit10 Item 11 prefer deleted functions to private undefined oneseffectivesemcppit11 Item 12 declare overriding functions overrideeffectivesemcppit12 Item 13 prefer constiterators to iteratorseffectivesemcppit13 Item 14 declare functions noexcept if they wont emit exceptionseffectivesemcppit14 Item 15 use constexpr whenever possibleeffectivesemcppit15 Item 16 make const member functions thread safeeffectivesemcppit16 Item 17 understand special member function generationeffectivesemcppit17 Item 18 use stduniqueptr for exclusiveownership resource managementeffectivesemcppit18 Item 19 use stdsharedptr for sharedownership resource managementeffectivesemcppit19 Item 20 use stdweakptr for stdsharedptr like pointers that can dangleeffectivesemcppit20 Item 21 prefer stdmakeunique and stdmakeshared to direct use of neweffectivesemcppit21 Item 22 when using the pimpl idiom define special member functions in the implementation fileeffectivesemcppit22 Item 23 understand stdmove and stdforwardeffectivesemcppit23 Itemm 24 distinguish universal references from rvalue referenceseffectivesemcppit24 Itemm 25 use stdmove on rvalue references stdforward on universal referenceseffectivesemcppit25 Item 26 avoid overloading on universal referenceseffectivesemcppit26 Item 27 familiarize yourself with alternatives to overloading on universal referenceseffectivesemcppit27 Item 28 understand reference collapsingeffectivesemcppit28 Assume that move operations are not present not cheap and not usedeffectivesemcppit29 Familiarize yourself with perfect forwarding failure caseseffectivesemcppit30 Avoid default capture modeseffectivesemcppit31 Use init capture to move objects into closureseffectivesemcppit32 Use decltype on auto parameters to stdforward themeffectivesemcppit33 Prefer lambdas to stdbindeffectivesemcppit34 Prefer taskbased programming to threadbasedeffectivesemcppit35 Specify stdlaunchasync if asynchronicity is essentialeffectivesemcppit36 Make stdthreads unjoinable on all pathseffectivesemcppit37 Be aware of varying thread handle destructor behavioreffectivesemcppit38 Consider void futures for oneshot event communicationeffectivesemcppit39 Use stdatomic for concurrency volatile for special memoryeffectivesemcppit40 Consider passbyvalue for copyable parameters that are cheap to move and always copiedeffectivesemcppit41 Consider emplacement instead of insertioneffectivesemcppit42"},{"title":"\"Effective STL\"","href":"/notes/estl","content":" Know your optionseffectivesestlit1"},{"title":"\"Fooled by randomness\"","href":"/notes/fooled-by-randomness","content":"Thoughts on fallibility of human knowledge by a former options trader Part I Luck is often mistaken for skills probability mistaken for certainty Croesus wanted to impress Solon with his fortune and asked Solon who the happiest man is Solon suggested one who lived a noble life and died for their duties Reason being You asked me a question concerning a condition of humankind happiness I reckon 70 years to be a long life Of those 26250 days no two will be the same We are wholly accident Good fortune is always mixed with misery In the journey of our lives there is an infinity of twists and turns and the weather can change from calm to whirlwind in an instant We can never know what might come next The gods are jealous and like to mess with mortals Sometimes we get a glimpse of happiness and then are plunged into ruin Yes you are fortunate wonderfully rich lord of many peoples But with respect to the question you asked I have no answer until I hear that you have closed your life happily Monte carlo random process alternate history Why me It is not natural for us to learn from history Hindsight bias Over a short time increment one observes the variability of the portfolio not the returns A trader making money does not mean he is good Similarly passing a particular interview does not mean you are good Dont get your causality reversed Our emotions may not be designed to understand such Ultimately news media is an entertainment industry We often mistake probability for expectation high chance of market going up does not mean you should long if in the rare case of it going down its going down by a lot This asymmetry skewness is often times counterintuitive considering that we are used to symmetric bell curves in everyday lives Consider this is market statistics even stationary as we observe the distribution itself may shift If the distribution is not stationary can past data even be used to forecast future performance if so to what extent Consider this statement I have just completed a thorough statistical analysis of the life of president Bush For fiftyeight years close to 21000 observations he did not die once I can hence pronounce him as immortal with a high degree of statisical significance Rare events do happen If the past by bringing surprises did not resemble the past previous to it then why should our future resemble our current past Extreme empiricism competitiveness and an absence of logical structure to ones inference can be quite a explosive combination A scientific hypothesis is falsifiable Newtons Laws are science and falsified astrology is not as its not falsifiable Karl Popper would claim there are only two types of theories those that are known to be wrong falsified and those that have yet been known to be wrong Part II Survivorship bias Ergodicity roughly under certain conditions very long sample paths would end up resembling each other Remember that nobody accepts randomness in his own success only his failure There is a high probability of the investment coming to you if its success is caused entirely by randomness Say an actively managed fund with an outstanding track record due to pure randomness Is it really a small world The world is much larger than we think it is just that we are not truly testing for the odds of having an encounter with one specific person in a specific location at a specific time Rather we are simply testing for any encounter with any person we have ever met in the past and in any place we will visit during the period concerned Think birthday paradox get 23 persons in a room there is about 50 chance of two having the same birthday Seems large no Remember we are testing for any pairs Consider backtesting particular strategies on historical data you might be fitting the rule on the data and the more you try the more likely by mere luck youll be able to find a rule that worked on past data eg correlating stock markets with length of womens skirts or finding that one instrument whose movement is perfectly correlated with weather in Ulan Bator You might be throwing monkeys at typewriters not telling them what book to write If you throw enough monkeys one of them will produce Illiad But how likely is he to then produce Odyssey Stock analysts have both a worse record and higher idea of their past performance than weather forecasters The sample size matters Our emotional apparatus is designed for linear causality for instance you study everyday and learn something proportional to your studies If you dont go anywhere you will be demoralized But the reality rarely gives us the satisfaction of a linear positive progression you may study for a year and learn nothing then unless you are disheartened and give up something will come to you in a flash This summarizes why there are routes to success that are nonrandom but few very few people have the mental stamina to follow them Those who go the extra mile are rewarded Most people give up before the rewards An example of biases in understanding probabilities a test of disease has 5 false positive the disease strikes 01 of the population We test people at random and someone turns out positive Whats the probability of someone actually having the disease By Bayesian formula its about 2 Usually smaller than our estimates On Bloomberg it acts as a safe email service a news service a historical data retrieving tool a charting system an invalubable analytical aid and a screen where I can see the price of securities and currencies I have gotten so addicted to it that I cannot operate without it as I would otherwise feel cut off from the rest of the world I use it to get in contact with my friends confirm appointments and solve some of those entertaining quarrels that put some sharpness into life Somehow traders who do not have a Bloomberg address do not exist for us Part III Wittgensteins Ruler when a criteria such as our opinions of something or a book review speaks more about the criteria itself rather than the subject being reviewed Randomness and personal elegance Flexibility around a schedule and happiness"},{"title":"\"Nineteen eighty four\"","href":"/notes/nineteen-eighty-four","content":"1984 Newspeak Ingsoc Doublethink Thought Police the Party Big Brother Oceania Eurasia and East Asia Airstrip One Ministries of Truth Love Plenty and Peace Goldstein two minutes of hate Inner party Proles Victory gin Chestnut Tree Cafe Mr Charringtons Shop Part I Winstons life as a forger in the ministry of truth Part II Winston and Julia Mr Charringtons place Part III OBrien Torture and rehabilitation"},{"title":"\"The great ideas of philosophy\"","href":"/notes/philosophy-ideas","content":"Questions of Knowledge Conduct and Governance Did the Greeks invent it all What differentiates the works of Pythagoras Plato etc from those of Homer Sophocles Confucius Buddha etc In philosophy there is a critical disinterested skepticism towards the principles we hold and even mandates of God The pursuit would be wisdom itself not wisdom for the sake of something else And a philosophical realization made through skeptical analysis and debates would not end with an exclamation mark but with semicolon The Greek civilization of 6 Century BC would not have been more powerful than the Egyptians or Persians yet the latter were not considered to have laid the foundations of Western civilization How developed the thoughts are of a people country culture does not necessarily reflect how wealthy powerful the group is their ability to solve practical problems and influence others Nor would philosophy be the product of a leisurely Hellenistic upper class built upon labor of slaves and indentured servants Egyptians and Persians would have been better suited in that regard Greek religion tends to have an estrangement Gods are revered but aloof and indifferent towards our matters The fundamental questions of being would not be something you consult the Oracle for but rather left to reason about by ourselves We ourselves need to make order classification out of the chaos of this world The state would be religious but without a state religion Imagine us as puppets made by the Gods with various cords in it these cords reflecting pleasure pain and emotions pull us in different directions One cord is sacred and golden that of reason and calculations And when one follows this cord one is virtuous Plato The Laws Pythagoras and the divinity of number Abstract transcendance philosophy vs natural philosophy would be the focus of later Greek Aristotle philosophy Pythagoras rich family shrouded in mystery Learnt maths fractions calendar predicting Nile flood from his Egyptian travels Rumor has it he traveled and taught in Babylon India Spent most of his life traveling came back to Greece as a governor and founded his sect Pythagoras did poorly as a governor A philosophicallyguided state may not be well suited to the brutal reality Cynics are an important PreSocratic philosophical school Diogenes Nothing too excess Pythagoreanism abstract idea is the ultimate creator of material and physical realities and that idea is numbers hence divinity of numbers in particular 1 point 2 line 3 plane 4 solid tetrahedron special place in Pythagorean teaching sum is 10 also sacred in Pythagoreanism The lawfulness with which we enter social life is based off of numbers Also known for his theorem on right angle triangle and himself his schools contribution to discovery of harmonic structure of music harmony matches with something in the soul what creates a harmony is the relationship between notes part of the intended design of the cosmos Over time we wont exist but rectilinear triangle will mathematical abstractions will and will remain correct precise in describing the relationships in physical world Also believes in transmigration of souls death liberates something in us Pi e sqrt1 has no place in Pythagoreanism Sophia wisdom itself and Pythagoras considered himself a philosopher as he strived to and have befriended wisdom itself Elegance and simplicity expressed in mathematical formulaic equation What is there metaphysics A metaphysical question Real object perception and distortion what is beyond there If our senses always lie how would we know reality Aristotle notes that they have just completed a systematic consideration of nature the physical world and what is natural Centuries later people notes his work after the physics treatise as metaphysics after the physics natural science work Metaphysics deals with the subject of real existence What is there ontology being the branch of metaphysics that deals with reality and existence After change does the original still exist Cause and effect Are there minds and thoughts or just peculiar states of the brain are they merely terms hankering to superstitution Does what really exists vary being by being ie sensory apparatus What sort of being am I What am I made of made for Our modes of knowing epistemology Epistemological systematic organization and principle of facts questions are senses wrong If so what is right Abstract rationality of Pythagoras Customs Religion Epistemology is the study criticism and refinements of our modes of knowing Epistemic justification Is there heaven Is there such a thing as goodness Is there a moral reality and objectivity Is there real beauty Is there truth Is there right or wrong The conclusion we reach is more or less based on the method we choose often times blindly by habit social condition or conventional wisdom There tends to be a vicious circularity between the claim ontology and the methods epistemology Certainly the harmony and balance Greek perceived as beautiful is not the same as what medieval artists found beautiful in their depiction of human form Yet the Greeks philosophers were unaware of the shifting Democritus ultimate reality is but an incredibly large of atoms Atoms and the space in between the void Everything is ultimately reducible to that level What we see as flowers buildings animals are but ephemeral different in atomic composition The soul is a finer kind of atomics structure Earth air fire and water Can we even answer the ontological questions Beautiful or ugly True or false Moral or not Heraclitus no one descends twice in the same river Sees nothing but flux and change Protagoras preSocratic Founding father of Sophist thought Man is the measure of all things Judgment of any form must have some grounding and that grounding can only be the experiences of a lifetime we cannot take an epistemological position external to our own human ways of thought and feeling If there is a standard independent of human nature we cannot even comprehend it Hence is pursuing truth a misguided objective Each person is the measure of all things Truth is subjective We are not equipped to comprehend it even if there might be objective About the gods I cannot say if they are or how they are constituted in shape the unclarity of the subject and the shortness of human lives I can write you about what I see and hear and touch but I cannot write you about the gods Socrates spends much of his work disputing the Sophists much can be questioned about the claim Each man not only is not the measure of all things we are generally very poor in understanding ourselves With him philosophy becomes a humanizing and humanistic enterprise the human condition from which there is no retreat Greek tragedian on mens fate Dance is important in Homer Ovid and other classical authors Dance of cranes Theseus Chorus Dance Participants and preliterate history and moral thought Out of possibility of drama comes the dispute and dialogue Philosophy may have come from cultural activities such as drama The thought refining itself The human condition as understood by the dramatists Is Medea a murderess or are her motives irresistable Her sorcery practices are of Chthonic and preOlympian When we surrender reason to passion chaos ensues What about the trial of Orestes The debate on Nomos norm natural law Antigone Is the law of kings higher or the nature that sisters should bury brothers Euripides heroic characters are very much human At the end of day character is destiny Aristotles view on women vs that of Greek tragedians Aristotles view on tragedy bad things happening to good people If theres an all seeing and all loving god why such evil and injustice around us Herodotus and history Herodotuss Histories attributes war to irreconcilable differences in value Legions are moved with words and symbols Histories describes various peoples dress weapon food economonies and religious belief to give a full perspective on events of historical significance as opposed to mythology where such accounts on humanistic details are often glossed over Herodotus would suggest to account for events of historical significance perspectives on sociology pyschology beyond mere chronology is required And his teaching or that of history would be meaningful beyond ethnicity and calls onto the roots of humanity itself Histories has clear distinctions for Herodotuss own opinions opinions he has heard and facts His work however has many inaccurate accounts to establish some morals on the question of conduct Eg the emblematic meeting between Croesus and Solon where the happiest men are established to be Kleobis and Biton two sons who yoked themselves to their mothers cart such that their mother a priestess would not be late for the festival of Hera They died peacefully after the deed Solon would claim only after ones story is complete can one be judged whether their lives being happy or not and human happiness is not dependent on wealth Socrates on the examined life Teaching of Socrates is preserved by Zenophon and Plato broad shoulders who claims to be the mere scribe but his works are much more a portrayal of his and Socratess thoughts via Socratess mouth than mere description Socrates lived in a time when Athens lost the Peloponnesian War against Sparta and as a loyal faithful soldier of Athens his thoughts are very often on the practical side what went wrong with Athens in matters of educating the youth of conduct and government The Dialogues are conducted by the losing side which often explains him taking a Spartan view on the matters above Socrates described himself as a gadfly asking those confident in their thoughts reminding us of the gadfly sent by Zeus to unseat Bellerophon from Pegasus an analogy for unseating the confident writers He was well trained by his Sophist teachers sophistical teaching known for their skills in debate and rhetorics and to expose the ignorance of the interlocuter via whimsical and deft conversations Yet Socrates aimed to defeat the skepticism and cynicism claiming that there is something we can truly know and knowing that our methodology of knowing is sound contrasting his thoughts from those of Pythagoras St Augustine would consider Socrates the only true philosopher one committed to living and dying by his own philosophy the inquiries he perceived as true and the cause of reason not like Pyrrho Pyrrhonism a school of skepticism Socrates was found guilty of failure to respect the Gods despite him being a reverential person and corrupting the youth The Symposium which has inquires on the nature of love ended with Socrates returning home alone Neither charges were truthful and Socrates had the choice of death or exile ostracism of at least 10 yrs on which Pericles was also charged and choose the former hemlock poisoning in defense of his thoughts and the rule of law the public expression of rational thoughts reason without passion A Homeric ending think Hector breaker of horses where upon death he uttered will you remember to pay the debt Socrates would teach the unexamined life is not worth living What is wrong with the unexamined Socrates would describe it as a screen on which events are laid out and not a lived life as prisoners living in a cave watching parapets on which the shadows of puppets are cast where all they see is shadows and illusion Examine in the sense of interpretation of meaning of events integration of experience subject to the refinements through self criticism and introspection and making the thought whole This connects back to the motto at the Oracle of Delphi know thyself meaning knowing what it means to be a human being Beyond the biology constituents etc The Socratic agenda would argue against skepticism and cynicism and realizing the interconnectedness between matters of knowledge conducts and governance Dealing first with knowledge and establishing a good philosophical basis that there is something we can know one kernel of truth we can know if we cant know anything as the skepticists claim then the life is uselessly examined prejudices and self deceptions If it is possible to know something know that you know it and know the methodology of knowing it then we can inquire into what kind of life is right for beings such as ourselves Happiness and pleasure Are they the same for all people all cultures What about our values Are they just relative opinions and prejudices What about governance how should we be governed Are the core precepts of Athenian democracy to be questioned Socrates would question Athenian democracy which Aristotle would later defend claiming the collective wisdom is more likely to make good decisions as opposed to the strength of one and the weaknesses are likely to cancel each other out Plato and the search for Truth To be able to arrive at any truth one first defeats the skepticism claim that nothing can definitively be known Platos Meno deals with the search for Truth where Socrates converses with Meno a noble from an area with strong Sophist presence on whether virtue comes from teaching or nature Socrates in Meno instructs by guiding the barbarian servant boy of Menos to discovering the Pythagoras theorem And claims knowledge is a form of reminiscence where through philosophical guidance one is no longer clouded by the sensory systems of the material world and recalls the Truth that the soul had always known Hence Plato establishes that Truth is different from facts where the former is eternal not sensory and cannot come by perceptions and the latter is ephemeral material and as Heraclitus would claim always in flux as the material world would be Facts are various forms of rectilinear triangles drawn on the sand flux ephemeral and material and Truth is Pythagorean theorem that a2 b2 c2 is the true form of rectilinear triangle Aristotle would argue this is Platonic and not Socratic Platos work is also divided by later scholars into early mid and late where some ideas came earlier are later revised How is Truth acquired One has it intuitively The gift of rationality Plato draws much from Pythagoras in illustrating Truth with his discovery and in transmigration of souls where he would claim with death the soul is liberated Plato considers mathematical Truth an example of knowledge we know for certainty and where the skepticists are wrong but the debate would continue with evolution on both sides To Plato the problem then becomes identifying mathematicslike Truth and it can be discovered by the dialectical approach to achieve something abstract and uncover what one has always possessed but became clouded by the material world To this end it is also thought that the discovery of Truth has naught to do with experience and the experience of many do not necessarily draw us any closer to Truth than the experience of one On this ground Socrates would question the basis of Athenian democracy Can virtue be taught Platos Protagoras Problem of conduct to each his own Vicious hedonistic Is there truth to ethics morality Is there moral objectivity The Socratic Platonic school would combat the skepticists in the above domain claiming the problem of conduct is not to be solved at the level of merely personal desire The same argumentative approach to knowledge should be applied How life should be lived What kind of life is right for beings such as ourselves Platos Protagoras Setting Protagoras is in town a young 30s Socrates put several questions to Hippocrates not the physician who woke him up to this news Socrates then raises the questions to Protagoras who eloquently discourses on virtue conduct etc Socrates and Protagoras then engaged in Socratic dialectical method and hard to say who had the upper hand What do you want to be that makes you want to study under Protagoras Protagoras claims moral excellence What is virtue Justice temperance courage wisdom etc what are they Parts of virtue Parts in what sense Is virtue teachable Both men agree knowledge is the greatest and there must be knowledge first before there is virtue Knowledge known in some nonsensory way How do we teach anybody anything We teach by providing sensible visible ostensive objects Anything taught by showing cannot be universal What do you point to to say that is virtue to teach virtue The teaching of technique vs the teaching of Sophia You cant show the universal but you can show an instance of it Perhaps we can teach virtue by pointing to persons practicing virtue Point Leonidas at Thermopylae to a one year old and he wouldnt be able to comprehend Socrates argues to the conclusion virtue cannot be taught as such Perhaps the knowledge wisdom attained by philosophical reflection is the grounding of virtue This is teachable The oneyearolds arent ready for it many arent ready for it in their entire lives The students must have been prepared to be receptive of such lessons then you may find such persons resonating to the virtuous act when it presents itself What kind of preparation is necessary for one to be considered ready for instructions by the virtuous This is discussed in The Republic Theaetetus is another piece where Protagoras is a central figure and claims men is the measure of all things I will do things that please me At the end of the day the problem of conduct is a problem of principle something universally right The Socratic approach to knowledge would claim each man is not the measure of all things There is a measure of all things and its the task of each man to understand that measure and apply it properly Platos Republic Latemiddle dialogue Russell on the Republic fascist and imperialistic Foundational work in political science Tied to the considerations of statecraft are observations on moral justice weakness of human hence somewhat foundational in psychology Sepheles near death and ponders on wealth Polymarchus what it means to be just is to pay ones debt Can an unjust person be happy and make great progress Thrasymachus strong should prey on the weak Glaucon what is goodness Three classes Things that are good in and of themselves Harmless pleasures Things that are good in themselves but also for the consequences that arise from such good Knowledge health Pursued for the same of their results Gymnastics money making Where is justice placed Socrates the first where things are pursued for their intrinsic value not for consequences Glaucon Adeimantus Those seem just may not be just Virtue may arise purely from fear of punishment and desire of reward not for the sake of being virtuous Example of Ring of Gyges invisibility used to commit unjust Being just may be for reputation not for the sake of justice Socrates suggests to inspect the state instead so that we can seek justice on an enlarged scale rather than on each flimsy individuals No claim can be made towards politics without understanding human nature Guardians Elimination of greed no payments Shared partners Marriage lottery in festivals but actually chosen breeders Breed the guardians Pure eugenics Problem of knowledge Allegory of the cave Being shackled and faced with parapets with projections shadowy illusions Each man his own measure Imagine one breaks out discovers the outside and realized what theyve perceived as true are merely halluciatory experience This one goes back to share his experience the others wouldnt be able to comprehend and instead thought him as blinded by the light Being bound by our material selves resisting transcendance believing whatever we see and only what we see Ignorance is darkness See the reality behind the appearance of shadowy illusions To appreciate comprehend Leonidas at Thermopylae one needs to be preconditioned The eyes and ears never record the truth and what they do pick up will never be parlayed into the truth Being experientialist while missing the point of life not even understanding life has a point There are some things that only be seen under the light of philosophical examination the wise mans guidance Problem of knowledge then gets us back to the search of truth of the relationship as the relationships constitute the true form of things What is a good government is then answered in terms of relationship between the ruled and the rulers laws where authority is vested and for what purpose Justice is also perceived as the the harmonious relationship among the rational the passionate the emotional dispositions of the soul Also behavioral theory eg children has to be protected from vulgar music but exposed to martial music How should we behave How should we live our lives We are corporeal inclined towards pleasure and avoid pains We are like charioteers standing on two horses a good horse and a vicious one a metaphor also found in India A will capable of resolving us to follow the right course The will itself cannot decide what the right course is desire knows only one course to fulfill itself How then do we discover the right course through rational power Supremacy of reason in determining what we ought to do Reason as shown in mathematical proportion harmony balance The guide and goal of life Avoid excess The Pythagorean numerological balance is the resemblance of truth Hippocrates of Kos and Greek medicine Contemporary of Socrates Plato Aristotle whose father was a physician movement in the direction of natural science Secularize and analyze a subject that is generally related with religion and customs Greek medicine is close to modern practices in its being scientific realizes diet hereditary disposition as causes for conditions Health as related with the state of the body as opposed to religion and customs Few or perhaps none of the writings of Hippocratic school were by Hippocrates himself Pythagoras sect of medical views treat the body with vegetarianism ritual performance musical harmony and exercises Hellenic civilization objectified itself and the natural world for the purpose of critical scrutiny Secularizing of knowledge as opposed to realizations brought by prophets Hippocratic writers are religious but they hold the religious account constant They provide a science that is liberated from religious orthodoxy Hippocratic medicine is not necessarily superior to Egyptian medicine which dates even earlier but it is based on an entirely different set of suppositions Once a society confers upon a selected group ultimate epistemological authority on core questions arising from the problem of knowledge the nearly inevitable result is philosophical paralysis positions become quite hardened and the only work left for scholars is to interpret the words of the wise The debate is no longer about knowledge and truth but how a text or holy maxim is to be understood Empirical Empiricist The Greek word observation Science is an empirical enterprise Hippocratic writers describe themselves as empirical as their treatment is based off of clinical observation naturalizing the natural world How is apoplexy understood Eastern Christianity influenced by Greek thoughts perceives it as a viral attack on the body empirical Western Christianity much more theological theory driven perceives it as retribution for the victims wickedness divine spiritual intervention Hippocratic writers correctly identified the brain as the central organ that processes sensory information being correct whereas Aristotle was not heart while brain controls the temperature of the blood The celebrated figures of this age Socrates Hippocrates Aristotle etc were a conspicuous minority This would be an uncommon group in any age The mass of the people has a deep suspicion towards philosophers The perspective developed by these few was not widely shared There was a perfectionist idea wide spread in Hellenistic civilization In rhetorics tragedy physical competitions architecture works of art etc Similarly in mathematics and philosophy Hellenism bequeathed this pursuit to later societies Aristotle on the knowable Aristotle has so much original hence so much criticism from those who came after His role in logic biology physics natural science political science and metaphysics are fundamental He spent 20yrs in Platos Academy where some small fraction of his teachings are present in Platos dialogues Aristotles Metaphysics proposed many thoughts different from Socratic and Platonic view It offered a presentation of preSocratic and Socratic schools of thoughts serving as a good account of the history of philosophy and Hellenic thoughts in general Aristotle usually inspects the historical work done what is left wanting and goes from there On senses Metaphysics opens with All men by nature desire to know An indication of this is the delight we take in our senses This sets a fundamentally different tone from a Platonic dismissiveness towards senses Nature produces nothing without a purpose and senses do not exist to deceive creatures Perception must be the starting point of knowledge On classification Aristotle proposes different perceptual modes of knowledge Whats the fundamental power Greek word Dynamics in which a living thing has life Nutritive A living creature has the means to absorb nutritions from its surroundings to sustain itself Reproductive A living species can reproduce Locomotive Some living creatures have the power to move Some plants dont Sensation The ability to act knowingly consciously Animals possess this Rationality The soul psyche power of reason a special rationality cognitive power to allow beings to grasp universal propositions all men must die vs that men died Our laws is another example of generality Aristotles view on knowing also differs from Platos As opposed to Platos Meno in which he talks about facts and Truth with the latter being awakening something we inherently know via dialectical methods Aristotle claims there are two modes of knowing eg the angles of a triangle sums up to 180 By measurement This is by experience and factual By definition This is episteme and appeals to generality On the cause Happy is the man who knows the cause of things Whats the cause of a statue On the material level theres the stone It couldnt have existed otherwise A statue possesses certain features to differentiate it from just any stone This is the cause on the formal level Then theres the efficient cause of one blow after another by the sculptor where each blow is the cause for the next Finally how do you know where to hit the material You have to know what you aim to achieve Then the ultimate final cause is that intelligent design Final in time first in conception Unless you have an intelligent plan to begin with otherwise nothing will come about Greek telos A teleological explanation identifying the purposes plans designs and goals Teleological does not assume sentient beings Evolutionary theory is teleological where wings mating behavior etc serve specific purposes We do not understand something fully unless we know all four modalities The number of things we know are based on the questions we ask does it exist If it does to what degree In what relation does it stand to other things What is it for The central point of Aristotelian program are these four causes questions in domain of knowledge politics ethics etc In politics what is the polis for In ethics what kind of being am I How do the actions of my mine either realize what is potential within me or stultify such potentials What am I here for my potentials How do I live my life to honor the central fact of my being Living things of the universe fit in a plan Nothing with pattern or design comes about accidentally If the art of shipbuilding were in the wood ships would exist by nature Ask the ultimate question what is it for To know in this sense is to comprehend far more than anything conveyed by the mere composition of the object What are atoms for Knowing all are made of atoms is pure materiality and does not rise to the level of episteme Aristotles explanation is usually universal by and large in general and deterministic Eg his view on scientific theory needs to be general where the explained event is a particular instance Aristotle on Friendship Men are by nature social and political Bees and swans are social by nature too The natural Aristotle question what is it for then becomes Whats the point of us grouping ourselves together What purpose is served by the tendency towards complex and larger scale social and political lives Why go beyond small tribal communities Why a polis Selfdefense Large communities are destroyed left and right Trade Tribal communities too trade Discussed in the Nicomachean Ethics and the Eudemian Ethics Friendship is first inspected in the two works of Ethics Friendship can be formed for pleasure including sensual pleasure which Aristotle is not against These pleasure have an ephemeral feature about them Friendship can be based on utility A and B are useful for each other These too are often ephemeral These friendships are selfregarding selfishly motivated There is also friendship formed on Teleiaphilia completed perfected friendship Shared by the friends is a set of morals and values What A wants for B and is good for B for the sake of B A constructive relationship that is preserved and celebrated Friendship conduced to goodness moral excellence itself and usually lasts a lifetime Aristotle considers this kind of friendship to be not common The above is obtained by two who are equally worthy few among the general populace Some would argue then the polis would be too small with those that meet this moral standard How unequal would the two parties need to be for such friendship to be impossible The inequality between humans and gods An example an audience with good taste enjoying a performance The audience does not have the training of the performers but there is mutual respect and each understands what is best and good in the other for the other in the others sake Then you could form a polis without people being morally equal What is virtue Its a disposition inclination to act towards certain ends and avoid other ends Virtue is to be understood in two senses Intellectual excellence Science artistic technical central to a creative life These are not innate and come from learning discipline and practice Moral excellence These are habitual forms of activity and are reinforced by discplined practice as well Receding Platonics view of men of gold silver and bronze borrowed from Hesiod A program of research directed at the world as we know it Is anger first word in Illiad good or bad Natured equipped us with such emotions itd be wrong to say such emotions are pathological The right question then becomes what should be ones disposition towards anger What should one be angry for To become angry when virtue triumphs vice is bad The virtue is the middle ground of extreme excess and extreme defect Courage is the mid point between heedlessness and cowardice The middle route is the best opposing excess common in Greek thoughts Those who are fit to rule are those of virtue who are fit for friendship Aristotle claims in the order of things the polis precedes families the pattern of obligation parents have towards children and vice versa and entities identification of the individual bound up with familial and political life The life of the hearthless stateless lawless men is the worst imaginable life A productive life involves familial and political duties and obligations Aristotle displays a suspiciousness towards radical democratic forms of rule His ideal model could be that a constitutional monarchy where the rule of law is determinated and those who determine the law function as friends"},{"title":"\"Random walk down wall street\"","href":"/notes/random-walk-down-wall-street","content":" Part I stocks and their valuation Firm foundation theory intrinsic value Stock valuation depends on the estimations of earning power of the company many years in the future PV of cashflows a company is able to earn in the future for its investors Castleintheair theory mass psycology Styles and fashions in investors evaluations of securities can and often do play a critical role in the pricing of securities The stock market at times conforms well to the castleintheair theory For this reason the game of investing can be extremely dangerous Another lesson that cries out for attention is that investors should be very wary of purchasing todays hot new issue Most initial public offerings underperform the stock market as a whole And if you buy the new issue after it begins trading usually at a higher price you are even more certain to lose Investors would be well advised to treat new issues with a healthy dose of skepticism Certainly investors in the past have built many castles in the air with IPOs Remember that the major sellers of the stock of IPOs are the managers of the companies themselves They try to time their sales to coincide with a peak in the prosperity of their companies or with the height of investor enthusiasm for some current fad In such cases the urge to get on the bandwagoneven in highgrowth industriesproduced a profitless prosperity for investors Res tantum valet quantum vendi potest A thing is worth only what someone else will pay for it Dutch tulips bubble Example how futures contract made money The market engineers complex contracts to serve those seeking capital or investment outlets or investors trying hard to part with their money The tronics biotech and dotcom bubble Concept stocks the issues of a company with few or no earnings but a good story to tell and great potential Speculative and risky Example how merger creates illusion of growth The housing bubble Mortgage backed security For banks handing out mortgages instead of the originateandhold system 30 years ago its now originateanddistribute mortgage loans were originated by the banks who only holds them for a few days until they could be sold to an investment banker The investment banker would then assemble packages of these mortgages and issue mortgage backed securities derivative bonds securitized by the underlying mortgages These collateralized securities relied on the payments of interest and principal from the underlying mortgages to service the interest payment on the new mortgagebacked bonds that were issued Making matters more complicated there are multiple bonds issued against a package of mortgages Different tranches with different claim priorities Same goes for credit card loans and automobile loans Credit default swaps Secondorder derivatives such as CDS were sold on the mortgagebacked bonds CDS were sold as insurance policy on the mortgagebacked bonds where one party would pay the other in order to be paid back in case of a default of the particular bond CDS traded in the market can be 10 times the value of the underlying making todays financial system very much riskier and interconnected CDS being unregulated and the mortgage originator facing a smaller risk they only hold it for a few days before passing it on led to looser and looser borrowing standards Increasingly lenders did not even bother to ask for documentation about ability to pay Money for housing was freely available and housing prices rose rapidly How it happened Loosening borrowing standard made borrowing for housing easier The government itself played an active role its regulations required loans be easily available to lowincome borrowers Housing price in turn inflated due to buyers having easy access to capital beyond their value The inflation in housing prices encouraged more loans to be taken to be invested in buying houses further jacking up housing prices The positive loop in building up a bubble Eventually mortgages buyers took became far more valuable than the house and some buyers defaulted returning the keys and causing the lenders to hold these toxic properties without a demand The loop reversed in bursting a bubble Are the markets efficient Markets can occasionally be irrational but eventually it corrects itself true value is recognized and this is the main lesson investors must heed And true value as stated in firm foundation theory depends on the estimations of earning power of the company many years in the future The estimation cannot be accurately made and market prices must always be wrong to some extent And even professional investors cannot estimate well enough to always hold the undervalued stocks and sell the overvalued ones Part II how the pros play the biggest game in town Technical analysis Investment by castleintheair theory Chartists Think a stock chart line with a bar on yaxis for each point on xaxis where it marks daily high low closing prices and volumes traded Make decisions based on patterns in the chart eg headandshoulders and ignore factors like income statement balance sheet etc Does it really work The history of stock price movements contains no useful information that will enable an investor consistently to outperform a buyandhold strategy in managing a portfolio In other words you are unlikely to beat buying index funds by staring at charts The claim is that stock price movement in the short term is a random process hence the name random walk Technical methods cannot be used to make useful investment strategies and not one has consistently outperformed the simple buyandhold with fees and taxes in mind This is the foundamental conclusion of the random walk theory Stock prices cannot be predicted on the basis of past stock prices Fundamentals analysis Investment by firm foundation theory Rules of pricing a stock a rational investor is willing to pay more if given the same conditions otherwise growth rate of dividends and earnings are higher a larger proportion of a companys earning is paid out in cash dividends or to buy back stock risk is lower eg volatility as a measurement interest rate is lower Future growth risk interest rate cannot be determined or deterministically inferred from present Thus the precise price cannot be determined and pricing becomes a matter of assumptions and scenarios When comparing different aspects above of two companies and deciding if ones price is undervalued its useful to compare price earning ratio PE ratio price per share divided by earnings per share With one companys PE ratio as a benchmark compare the above aspects to decide if another companys PE ratio is higher or lower than expected Does it really work Very few active fund managers outperform the market consistently Security analysis or predicting the future is fundamentally difficult due to random events company creative accounting mistakes of analysts losing analysts to sales or portfolio managers and conflict of interest between analysts and investment banking department With these we introduce the efficient market hypothesis The weak form suggests technical analysis looking at past price patterns cannot help investors The semistrong form suggests looking at public information fundamental analysis cannot help investors either due to the market being efficient and stock pricing has already taken into account all the public information available The strong form suggests even having insider information fundamental analysis but against the law doesnt help nothing helps This tends to be an overstatement View on HFT HFT does not hurt individual investors Technology has always facilitated trading One way they can help is in pricing of ETFs any discrepancy between the pricing of an ETF with its underlying stocks will quickly be arbitraged away thus improving liquidity better prices and making the market more efficient That said it would be illegal if optimally positioned traders can see the orders of others and drive prices up or down before such orders are executed This kind of insider trading called frontrunning is regulated by SEC Combining fundamental and technical Buy only companies that are expected to have aboveaverage earnings growth for five or more years double the factor price grows and PE ratio grows Never pay more for a stock than its firm foundation of value Look for stories whose stories of anticipated growth are of the kind on which investors can build castles in the air Avoid trading too much incurs taxes and brokerage fees Simply buying and holding a diversified portfolio suited to your objectives will enable you to save on investment expense brokerage charges and taxes Over the past 30 years 95 percent of stock market gains came from 1 of the total amount of trading days Modern portfolio theory Given an efficient market risk and risk alone determines the degree to which returns will be above or below average Risk can be measured as the standard deviation volatility of price a random variable Modern portfolio theory shows how to combine diversified instruments to give the least risk possible with the return investors seek Its possible to reduce risk by diversification as long as the individual instruments are not completely positively correlated Due to the possible negative correlation between different instruments and different currencies economies its possible to obtain lower volatility and higher rate of return to a certain degree at the same time by diversification Globalization increases the correlation of instruments especially in times of financial crisis Capitalasset pricing model Quantifying risk beta Beta of a portfolio measures the relative price change of the portfolio to that of the market if market goes up by 10 portfolio A goes up by 20 then the portfolio is said to have a beta of 2 The theory is that in an efficient market an investor wont be rewarded for risk that can be mitigated by diversification unsystematic risk eg particular events of a company But the systematic risk measured by beta representing the basic variability of stock prices and the tendency of stock prices being positively correlated cannot be diversified away And the higher the beta meaning the more systematic risk an investor takes the higher that investors reward should be A portfolio manager then chooses the stocks with different beta characteristics to maximize his returns within his risk tolerance Popular in the 70s Does beta really work Not asis beta for individual stocks are not stable over time and they are very sensitive to the market proxy against which they are measured eg SP 500 does not account for the worlds stock market nor bonds nor human resource assets for that matter The likes of arbitrage pricing theory and FamaFrench three factor model are introduced to bring in more factors like interest rate inflation national income in the former or market capitalization and markettobook value ratio in the latter Behavioral finance Previous theories assume a perfectly rational investor which is not the case Efficient market claims the random actions of irrational investors will cancel each other out as a whole and that the arbitrageurs would smooth out irrational fluctuations in stock pricing However behavioral finance suggests the irrational behavior of investors is continual rather than episodic And the systematic errors investors are prone to defeats the cancelling each other out claim also effective arbitrage is difficult The irrational behavior can be attributed to these observations in psychology Overconfidence study shows majority of peoples selfassessment is that they are in the top quartile in many aspects Making money is because of skill and losing money is because of luck Biased judgment the illusion of being in control and able to beat the market Herding statistically a group is more likely to make a rational decision than a single person Not all the time and a persons often times under pressure to conform to a groups erroneous view Loss aversion people are more sensitive to loss than to income rarely would someone accept a statistically fair bet the expectation needs to be in his favor and study shows usually by a large margin How the possibility of loss is phrased to them matters as well Pride and regret people tend to sell their best performers whose selling is often times nonsensical due to incurring income taxes and hold their worst ones until prices bounce back which sometimes makes more sense to sell Plus efficient arbitrage is difficult due to its risk in LTCMs case what if spread continues to widen sometimes unable to find similar instruments This results in being sophisticated traders like hedge funds often end up not being a pricing correction force in bubbles but rather they helped the prices move in irrational direction as in DotNet bubble Insights from behavioral finance bow to the wisdom of the market buy and hold an index dont be your own worst enemy and avoid stupid investor tricks In particular avoid herd behavior avoid overtrading the correct holding period for stock market is forever if you do trade sell losers hold only if you have reasons to believe the company is still successful not winners be wary of new issues you are not going to get the best IPO deals After the first six months when insiders are allowed to sell to the public new issues generally perform worse than the market distrust foolproof schemes if something seems too good to be true it is too good to be true Market timing can only be accomplished by liars Smart beta strategies Smart beta goal is to gain excess returns by using a variety of relatively passive investment strategies that tilt your asset allocation towards certain characteristics Some of these tilts flavors include Value wins Favor low PE ratio low pricetobookvalue Risk those two are often indication of crisis eg bankruptcy Vanguard offers its value index fund and growth index fund Smaller is better Favor small company stocks Risk smaller companies are intrinsically more risky than established bigger ones and survivorship bias ETF IWB small cap fund is an example Russell 1000 2000 stock index tracks the largest US companies by market cap Momentum and reversion to the mean For a short period price shows momentum inertia and will then adjust to the fundamental value oscillation Behavioral explanation and investors taking time to react Weakness not uniform across studies and weaker in particular periods of time Low volatility can produce high returns buy margin Blended flavor mixandmatch the above recall FamaFrench three factor model Keep in mind that The reason why smart beta can generate excessive return in a short period is most likely due to assuming extra risk maybe not measured by beta which is imperfect by tilting your allocation in favor of certain factors you are eg sacrificing diversification These portfolios need to be actively managed and rebalanced incurring additional management fees and trading fees Over longer periods smart beta strategies underperform the market In conclusion capitalization weighted indexing is unlikely to be deposed as the favorite The core of every portfolio should consist of low cost tax efficient broad based index funds Part IV practical guide for random walkers and other investors Gather the necessary supplies save and invest early and regularly do not touch money set aside Do not dream of get rich quick Buy and hold index funds Dont be caught emptyhanded cover yourself with cash reserves and insurance Assuming you are covered by medical and disability insurance at work have enough cash reserve for three months Fund the rest with investment whose maturity matches the date fundings required Home auto health and disability insurances are a must for ones with family life insurance is a must as well Shop around for the best insurance deal Advice is to buy renewable term life insurance Be competitive let the yield of your cash reserve keep pace with inflation invest in money market mutual funds bank certificate of deposit Internet banks T bills Learn how to dodge the tax collector IRA Roth IRA works well if you expect to pay more taxes after you retire or if your income is low enough for pretax Roth IRA contribution pension plans 529 college Understand your investment objectives the risk you are willing to take Ordered in increasing risk and returns bank accounts money market funds bank certificate of deposit TILs high quality corporate bonds diversified portfolio of bluechip US or developed country stocks real estate and REIT diversified portfolio of smaller growth company stocks diversified portfolio of emerging market stocks gold collectibles Own your own home if you can possibly afford it Also consider real estate investment trusts REITs They have produced returns comparable to common stocks provide good diversification and hedge against inflation When navigating bond investment zerocoupon bonds can be useful to fund future liabilities noload bond fund can be appropriate vehicles taxexempt bonds can be useful to high tax bracket investors and inflation linked bonds can be a good way to maintain real purchasing power Historically junk bonds gross yield premium has more than compensated for actual default experience Tiptoe through the fields of gold collectibles and other investments The book also recommends steering clear of hedgefund private equity and venturecapital funds These are not for individual investors Remember investment costs arent random some are lower than others Avoid sinkholes and stumbling blocks diversify your investment steps Five asset allocation principles Risk and returns are related Actual risk in stock and bond investing depends on the length of time you hold your investment a substantial amount of risk can be eliminated by adopting a program of longterm ownership and sticking to it through thick and thin Imagine you are going to eat hamburgers for the next 10 years youd want hamburger prices to go down Same goes for stocks if you are not speculating and holding them for value when you are young you are purely a consumer of stocks and you should want stock prices to go down Dollar cost averaging can reduce the risk in stocks and bonds investment This is controversial but it simply means instead of looking for market timings invest regularly a portion of your income no matter if the price is high or low Rebalancing to your target stocks bonds percentage can reduce risk and possibly increase returns Distinguish your attitude toward and your capacity for risk Your age earning ability outside investment should decide your capacity for risk An aggressive portfolio is recommended for the young and high earning power Guidelines to tailoring a lifecycle investment plan Specific needs requiring dedicated specific assets eg housing down payment college tuition Zero coupon bonds and certificate of deposits could be useful Recognize your tolerance for risk Persistent savings in regular amounts no matter how small pays off For those in their twenties a very aggressive portfolio is recommended as investors age start cutting back on risky investments Lifecycle funds offer this rebalancing if you want to avoid the hassle yourself Annuities can be useful for investment after retirement where you pay a lump sum and gets back an amount for the next x years or as long as you are alive sometimes inflation adjusted amount but there are drawbacks in being unable to leave a bequest and they can be costly and taxinefficient How do you go about buying stocks Three ways indexing investing in a noload index fund is recommended for most Indexing however should use a broader definition not just US SP 500 major corporation in US market US Total Stock Market international stock markets emerging markets should be a part of your indexing portfolio Indexing is extremely effective in emerging markets where risk and returns are generally higher than SP 500 ETFs can be used in lieu of indexing mutual funds though ETFs require the cost of transactions brokerage fees and crossing bidask spread When indexing by world stock market aggregated consider adding in more funds indexing Chinese stocks YAO TAO HAO as they are underrepresented due to cannot be traded outside China or significant government holding do it yourself follow the four suggestions in section combiningfundamentalandtechnical use a manager few consistently outperform the market and their past performance cannot be used to estimate future performance The book suggests that investors never buy funds with expense ratios above 050 and with turnover more than 50 and if you buy actively managed funds look for closedend funds traded at a discount that are only available through a broker Any particular investment advice faces a paradox if enough people know about it it wont be a good investment any more But pick up the 100 bill quickly because if its really there surely someone will take it A final word Broad diversification annual rebalancing using index funds and staying the course The first decade of 2000 was a trying time where US total stock market funds lost money But follow the simple rules and timeless lessons espoused in this book you are likely to do just fine even during the toughest of times Appendix derivatives Futures and options allow one to transfer risk hedge and speculate at a great leverage and thus great risk Atthemoney means the striking price of the derivatives contract is the same as the current price of the underlying Outofthemoney means striking price of the contract is higher Inthemoney means striking price of the contract is lower The book suggests individual investors can benefit from derivatives by using options buying as an adjunct to investing in index mutual funds ETFs an adjunct to portfolio management or as hedging instrument Essentially leverage its utility in transfering risk hedging Derivatives pricing Commodity futures Factors interest rate term storage cost convenience yield Options Factors exercise price underlying stock price expiration date valid exercise period volatility of the stock the more volatile the stock the more valuable the option interest rates One can use binomial formula to decide the price of an option by finding the perfectly hedged portion of a share such that the payoff is the same if the price goes up or down BlackScholes model is a multiterm binomial where a treelike priceprobability is used to determine the price"},{"title":"\"Sapiens\"","href":"/notes/sapiens","content":" Cognitive revolution What separates homo sapiens from homo erectus or neanderthal The former is the predominant human race while the DNA of the latter can hardly be traced to modern human beings some neanderthal DNAs were able to be found to modern day human beings Homo sapiens were hunter gatherers foragers Ache society in Paraguay are hunter gatherers living to this day with the tradition of killing off the old and the infants who cant keep up with the tribe Yet do we live in a happier society than them The cognitive revolution saw homo sapiens brains being able to process something purely fictional like laws or the United States of America This power to process something imaginary lets us grasp far more things than whats encoded in our DNA The course of natural evolution runs much slower than that of our cognitive revolution In turn such understandings are not preserved by the species its not in our DNA rather it dies as the individual bearing it dies Cognitive revolution allows homo sapiens to band together in much larger numbers under shared myths eg that of religion or nationalism and collaborate in ways far more flexible than whats encoded in our DNA Agricultural revolution Did homo sapiens domesticate the wheat or did wheat domesticate homo sapiens Our bone structures were not evolved naturally to take care of wheat nor our lifestyle Is the success of a species to be judged by the average of individual happiness or by how many replicas of its DNA it was able to reproduce Homo sapiens especially after the agricultural revolution are hugely successful in terms of the latter but may be questionable in terms of the former There is no going back in the agricultural revolution The collective myths Objective subjective intersubjective Something prevalent in the minds of a multitude of subjects does not make it objective The myth of modern time could be considered as that of consumerism and romanticism broadening our horizon via consuming different kinds of experiences Eg that of traveling abroad An Egyptian noble would probably not consider travelling to Babylon with his wife a desirable form of entertainment theyd much rather be building an expensive tomb perhaps Summerian writing Catalogging A scribe system Scientific revolution We know that we dont know Falsifiability Monarchs social elites and the like believe in the benefits of technological advancement and find their investment in such worthwhile Imperialism an amalgam of different cultures and without clear definition of borders Think Athenian Roman British empires Capitalism whereas those before Adam Smith would have us believe the economy pie is limited in size and an increase of my share comes at a cost of decreasing those of others capitalism would have us believe the pie grows by reinvesting earnings into growth By encouraging consumption and reinvestment the economy grows Humanism that of the liberal individuality socialism collectivism and evolutionary facism branches Religion Animism Polytheism is generally acceptive to foreign beliefs deities eg the Romans didnt mind adding the Egyptian Isis to their Pantheon What about Roman prosecution of Christianity It was mostly a halfhearted effort and early Christianity sometimes come with political insurgence Polytheism usually features one ultimate power to whom there is no use in praying eg as in Greek mythology each god has their speciality and take an interest in human affairs but they all are subject to fate which has no particular interest in humanity or gods Or in Norse that of Ragnarok Monotheism is far more exclusive and usually features one ultimate power who takes an interest in human affairs eg the love of God for human By the way one key difference in Protestantism vs Catholicism is in how to ascend to heaven after death Protestants would claim that Gods love is so great that one can go to heaven as long as one has faith Catholics would emphasize the importance of deeds church activities etc Missionary Natural order religion like Buddhism Craving leads to lives being trapped in the cycle of karma By giving up craving treating pain and joy with indifference one can achieve nirvana the ultimate inner peace World War II arguably is very much a war of religions rather than different sects of a traditional religion such as Hussites vs Catholics or Sunnis vs Shiites its liberal humanism vs evolutionary humanism and Cold War being liberal humanism vs social humanism Seeing whatis often times make us take whatis as granted there is no other way Ones lifespan is usually so short and horizon so limited that there is every other way which one may not have the eyes to see"},{"title":"\"Six easy pieces\"","href":"/notes/six-easy-pieces","content":"The principle of science the definition almost is the following the test of all knowledge is experiment Experiment is the sole judge of scientific truth Where do the laws to be tested come from Experiment itself helps to produce these laws in the sense that it gives us hints But also needed is imagination to create from these hints the great generalizations Atomic hypothesis All things are made of atoms little particles that move around in perpetual motion attracting each other when they are a little distance apart but repelling upon being squeezed into one another Another way to remember molecules size is this if an apple is magnified to the size of the earth then the atoms in the apple are approximately the size of the original apple 108cm Pressure the molecules being separated from one another will bounce against the walls Imagine a room with a number of tennis balls a hundred or so bouncing around in perpetual motion When they bombard the wall this pushes the wall away If we consider the true nature of the forces between the atoms we would expect a slight decrease in pressure because of the attraction between the atoms and a slight increase because of the finite volume they occupy Nevertheless to an excellent approximation if the density is low enough that there are not many atoms the pressure is proportional to the density If we increase the speed of the atoms what is going to happen to the pressure Well the atoms hit harder because they are moving faster and in addition they hit more often so the pressure increases What happens when an atom hits a compressing piston It picks up speed from the collision So the atoms are hotter when they come away from the piston than they were before they struck it Therefore all the atoms which are in the vessel will have picked up speed This means that when we compress a gas slowly the temperature of the gas increases So under slow compression a gas will increase in temperature and under slow expansion it will decrease in temperature The interesting point is that the material ice solid has a definite place for every atom and you can easily appreciate that if somehow or other we were to hold all the atoms at one end of the drop in a certain arrangement each atom in a certain place then because of the structure of interconnections which is rigid the other end miles away at our magnified scale will have a definite location The difference between solids and liquids is then that in a solid the atoms are arranged in some kind of an array called a crystalline array and they do not have a random position at long distances the position of the atoms on one side of the crystal is determined by that of other atoms millions of atoms away on the other side of the crystal Why ice shrinks when it melts The particular crystal pattern hexagon of ice shown here has many holes in it as does the true ice structure When the organization breaks down these holes can be occupied by molecules Most simple substances with the exception of water and type metal expand upon melting because the atoms are closely packed in the solid crystal and upon melting need more room to jiggle around but an open structure collapses as in the case of water What is the heat in the case of ice The atoms are not standing still They are jiggling and vibrating So even though there is a definite order to the crystal a definite structure all of the atoms are vibrating in place As we increase the temperature they vibrate with greater and greater amplitude until they shake themselves out of place We call this melting As we decrease the temperature the vibration decreases and decreases until at absolute zero there is a minimum amount of vibration that the atoms can have but not zero Helium even at absolute zero does not freeze unless the pressure is made so great as to make the atoms squash together If we increase the pressure we can make it solidify Atomic processes What happens at the surface of the water Above the surface we find a number of things First of all there are water molecules as in steam This is water vapor which is always found above liquid water In addition we find some other molecules here two oxygen atoms stuck together by themselves forming an oxygen molecule there two nitrogen atoms also stuck together to make a nitrogen molecule The molecules in the water are always jiggling around From time to time one on the surface happens to be hit a little harder than usual and gets knocked away Thus molecule by molecule the water disappears it evaporates But if we close the vessel above after a while we shall find a large number of molecules of water amongst the air molecules From time to time one of these vapor molecules comes flying down to the water and gets stuck again Why do we see no change Because just as many molecules are leaving as are coming back If we then take the top of the vessel off and blow the moist air away replacing it with dry air then the number of molecules leaving is just the same as it was before because this depends on the jiggling of the water but the number coming back is greatly reduced because there are so many fewer water molecules above the water Therefore there are more going out than coming in and the water evaporates Hence if you wish to evaporate water turn on the fan Which molecules leave When a molecule leaves it is due to an accidental extra accumulation of a little bit more than ordinary energy which it needs if it is to break away from the attractions of its neighbors Therefore since those that leave have more energy than the average the ones that are left have less average motion than they had before So the liquid gradually cools if it evaporates Of course when a molecule of vapor comes from the air to the water below there is a sudden great attraction as the molecule approaches the surface This speeds up the incoming molecule and results in generation of heat So when they leave they take away heat when they come back they generate heat Of course when there is no net evaporation the result is nothing the water is not changing temperature If we blow on the water so as to maintain a continuous preponderance in the number evaporating then the water is cooled Hence blow on soup to cool it Of course you should realize that the processes just described are more complicated than we have indicated Not only does the water go into the air but also from time to time one of the oxygen or nitrogen molecules will come in and get lost in the mass of water molecules and work its way into the water Thus the air dissolves in the water oxygen and nitrogen molecules will work their way into the water and the water will contain air If we suddenly take the air away from the vessel then the air molecules will leave more rapidly than they come in and in doing so will make bubbles In passing we mention that the concept of a molecule of a substance is only approximate and exists only for a certain class of substances It is clear in the case of water that the three atoms are actually stuck together It is not so clear in the case of sodium chloride in the solid There is just an arrangement of sodium and chlorine ions in a cubic pattern There is no natural way to group them as molecules of salt Where the atoms do change combinations chemical reaction where they do not physical processes but there is no sharp distinction between the two Burning carbon Carbon attracts oxygen much more than oxygen attracts oxygen or carbon attracts carbon Therefore in this process the oxygen may arrive with only a little energy but the oxygen and carbon will snap together with a tremendous vengeance and commotion and everything near them will pick up the energy A large amount of motion energy kinetic energy is thus generated This of course is burning we are getting heat from the combination of oxygen and carbon The heat is ordinarily in the form of the molecular motion of the hot gas but in certain circumstances it can be so enormous that it generates light That is how one gets flames Chap 2 Is the sand other than the rocks That is is the sand perhaps nothing but a great number of very tiny stones Is the moon a great rock If we understood rocks would we also understand the sand and the moon Is the wind a sloshing of the air analogous to the sloshing motion of the water in the sea What common features do different movements have What is common to different kinds of sound How many different colors are there And so on In this way we try gradually to analyze all things to put together things which at first sight look different with the hope that we may be able to reduce the number of different things and thereby understand them better A few hundred years ago a method was devised to find partial answers to such questions Observation reason and experiment make up what we call the scientific method The role of physics and the scientific method with the analogy of watching gods play chess and trying to guess the rules We may derive that a bishop always stay in a red square until we find one on a black square eg pawn queening That is the way it is in physics For a long time we will have a rule that works excellently in an overall way even when we cannot follow the details and then sometime we may discover a new rule From the point of view of basic physics the most interesting phenomena are of course in the new places the places where the rules do not worknot the places where they do work That is the way in which we discover new rules How do we tell if our guesses of the rules are correct First there may be situations where nature has arranged or we arrange nature to be simple and to have so few parts that we can predict exactly what will happen and thus we can check how our rules work A second good way to check rules is in terms of less specific rules derived from them The third way to tell whether our ideas are right is relatively crude but probably the most powerful of them all That is by rough approximation While we may not be able to tell why Alekhine moves this particular piece perhaps we can roughly understand that he is gathering his pieces around the king to protect it more or less since that is the sensible thing to do in the circumstances In the same way we can often understand nature more or less without being able to see what every little piece is doing in terms of our understanding of the game Amalgamation of various seemingly disparate aspects eg of heat and mechanics relation between electricity magnetism and light is what a theoretical physicist does the process itself just like the confluence of seemingly disjoint areas The question is of course is it going to be possible to amalgamate everything and merely discover that this world represents different aspects of one thing Nobody knows All we know is that as we go along we find that we can amalgamate pieces and then we find some pieces that do not fit and we keep trying to put the jigsaw puzzle together Whether there are a finite number of pieces and whether there is even a border to the puzzle are of course unknown Before 1920 our world picture was something like this The stage on which the universe goes is the threedimensional space of geometry as described by Euclid and things change in a medium called time The elements on the stage are particles for example the atoms which have some properties First the property of inertia if a particle is moving it keeps on going in the same direction unless forces act upon it The second element then is forces which were then thought to be of two varieties First an enormously complicated detailed kind of interaction force which held the various atoms in different combinations in a complicated way which determined whether salt would dissolve faster or slower when we raise the temperature The other force that was known was a longrange interactiona smooth and quiet attractionwhich varied inversely as the square of the distance and was called gravitation This law was known and was very simple Why things remain in motion when they are moving or why there is a law of gravitation was of course not known What kinds of particles are there There were considered to be 92 at that time 92 different kinds of atoms were ultimately discovered They had different names associated with their chemical properties The next part of the problem was what are the shortrange forces Why does carbon attract one oxygen or perhaps two oxygens but not three oxygens What is the machinery of interaction between atoms Is it gravitation The answer is no Gravity is entirely too weak But imagine a force analogous to gravity varying inversely with the square of the distance but enormously more powerful and having one difference In gravity everything attracts everything else but now imagine that there are two kinds of things and that this new force which is the electrical force of course has the property that likes repel but unlikes attract The thing that carries this strong interaction is called charge To give an idea of how much stronger electricity is than gravitation consider two grains of sand a millimeter across thirty meters apart If the force between them were not balanced if everything attracted everything else instead of likes repelling so that there were no cancellations how much force would there be There would be a force of three million tons between the two With this picture the atoms were easier to understand They were thought to have a nucleus at the center which is positively electrically charged and very massive and the nucleus is surrounded by a certain number of electrons which are very light and negatively charged Now we go a little ahead in our story to remark that in the nucleus itself there were found two kinds of particles protons and neutrons almost of the same weight and very heavy The protons are electrically charged and the neutrons are neutral If we have an atom with six protons inside its nucleus and this is surrounded by six electrons the negative particles in the ordinary world of matter are all electrons and these are very light compared with the protons and neutrons which make nuclei this would be atom number six in the chemical table and it is called carbon Electric force electromagnetic field electromagnetic waves waves of different frequency and when at very high frequency a wave behaves much more like particles This is quantum mechanics discovered just after 1920 which explains this strange behavior In the years before 1920 the picture of space as a threedimensional space and of time as a separate thing was changed by Einstein first into a combination which we call spacetime and then still further into a curved spacetime to represent gravitation So the stage is changed into spacetime and gravitation is presumably a modification of spacetime Then it was also found that the rules for the motions of particles were incorrect The mechanical rules of inertia and forces are wrongNewtons laws are wrongin the world of atoms Instead it was discovered that things on a small scale behave nothing like things on a large scale That is what makes physics difficultand very interesting It is hard because the way things behave on a small scale is so unnatural we have no direct experience with it Here things behave like nothing we know of so that it is impossible to describe this behavior in any other than analytic ways It is difficult and takes a lot of imagination Quantum mechanics has many aspects In the first place the idea that a particle has a definite location and a definite speed is no longer allowed that is wrong This rule is the explanation of a very mysterious paradox if the atoms are made out of plus and minus charges why dont the minus charges simply sit on top of the plus charges they attract each other and get so close as to completely cancel them out Why are atoms so big Why is the nucleus at the center with the electrons around it It was first thought that this was because the nucleus was so big but no the nucleus is very small An atom has a diameter of about 108 cm The nucleus has a diameter of about 1013 cm If we had an atom and wished to see the nucleus we would have to magnify it until the whole atom was the size of a large room and then the nucleus would be a bare speck which you could just about make out with the eye but very nearly all the weight of the atom is in that infinitesimal nucleus What keeps the electrons from simply falling in This principle if they were in the nucleus we would know their position precisely and the uncertainty principle would then require that they have a very large but uncertain momentum ie a very large kinetic energy With this energy they would break away from the nucleus They make a compromise they leave themselves a little room for this uncertainty and then jiggle with a certain amount of minimum motion in accordance with this rule Remember that when a crystal is cooled to absolute zero we said that the atoms do not stop moving they still jiggle Why If they stopped moving we would know where they were and that they had zero motion and that is against the uncertainty principle We cannot know where they are and how fast they are moving so they must be continually wiggling in there One of the consequences is that things which we used to consider as waves also behave like particles and particles behave like waves in fact everything behaves the same way There is no distinction between a wave and a particle So quantum mechanics unifies the idea of the field and its waves and the particles all into one Now it is true that when the frequency is low the field aspect of the phenomenon is more evident or more useful as an approximate description in terms of everyday experiences But as the frequency increases the particle aspects of the phenomenon become more evident with the equipment with which we usually make the measurements The new view of the interaction of electrons and photons that is electromagnetic theory but with everything quantummechanically correct is called quantum electrodynamics This fundamental theory of the interaction of light and matter or electric field and charges is our greatest success so far in physics The nuclei What are the nuclei made of and how are they held together It is found that the nuclei are held together by enormous forces When these are released the energy released is tremendous compared with chemical energy in the same ratio as the atomic bomb explosion is to a TNT explosion because of course the atomic bomb has to do with changes inside the nucleus while the explosion of TNT has to do with the changes of the electrons on the outside of the atoms Thus we are confronted with a large number of particles which together seem to be the fundamental constituents of matter Fortunately these particles are not all different in their interactions with one another In fact there seem to be just four kinds of interaction between particles which in the order of decreasing strength are the nuclear force electrical interactions the betadecay interaction and gravity"},{"title":"\"Starting strength\"","href":"/notes/starting-strength","content":"neuromuscular properly performed fullrange motion barbell exercises are essentially the functional expression of human skeletal and muscular anatomy under a load Barbell Posterior chain training Good technique in barbell training is defined as the lifters ability to keep the bar vertically aligned with the balance point midfoot Femur thigh bones Tibia lower leg bones Calf lower leg muscles Ham string upper leg muscles Adductors skeletal muscles located in the thigh"},{"title":"\"The story of human language\"","href":"/notes/story-of-human-language","content":"Language is not the same as communication imitation both in the latter animals can do but the former seems unique and genetically programmed to homo sapiens A gradual evolution 100k years ago Chomsky hypothesis language is something we are genetically programmed with as opposed to culture societyspecific phenomenon Speaking a language is not particular to certain areas all humans speak some languages not the same can be said for writing The ability to learn new languages degrades as one gets older another characteristic of something a species is genetically programmed to do There are over 6000 spoken languages and over 200 written Language comes in various forms some exotic features include clicks Khoisan languages of Botswana and Namibia X C Q pronounced with a tongue click where having one to four clicks before a word mean different things or having one word that others would express in a long sentence Language change over time The writing changes slower and the sound changes happen faster typically in following ways Assimilation least effort to make the different sounds Example being inpossibilis in Latin to impossibilis as m is closer to the following p Consonants devolve they become softer or omitted over time Examples being Latin into French Italian Spanish Portuguese Romanian The change of t and d k and g removal of consonant sound etc aqua Latin agua Spanish eau French Vowels devolve and shift over time The Great Vowel Shift starting in the 13th Century the vowels shifted up ah ay name as nahmeh to its modern day pronouciation is a result of this shift and devolve the last vowel Modern written English still reflects how the language was pronounced before the shift oo as in food was a long ou but now an u feed was an ae but now i The language we speak today also shifts eg aw ah in some areas They started out as accents and gradually the general populace talk in different ways Meaning of words too shift awesome and lame in the 60s vs now With the sounds devolving over time a language does not become fumbling of vowels over time as new words are constantly being created Some languages work with tones others with suffixes prefixes Tones presumably could have come from words devolved over time Imagine pa pas pak meaning different things in a language over time the consonant devolves and the only thing differentiating these would be tone Grammatical words and concrete words Walk eg is concrete nepas eg is grammatical Grammaticalization over time concrete ones devolve into grammatical ones Example being the French pas where it used to be that ne by itself means negation and pas had a concrete meaning of a step like il ne marche pas used to mean he doesnt walk a step Over time this concrete meaning devolved and pas was appended meaning general negation with ne ne can be downplayed in Spoken French imagine someone else transcribing this language they can derive the impression that pas alone means negation Suffixes and prefixes and tones too are results of grammaticalization Latin has different verb forms for I speak you speak and he speaks which is resulted from the phrase speak I will etc and the Iyouhe will parts were different They were glossed over over time and the verb speak with different suffixes became prevalent Rebracketing is another common sound change where the boundary of two words became different My Ellie turned into Nelly My Edward turned into Ned all one turned into alone Semantic change Semantic drifts meaning change over time too Silly used to mean blessed and the German root still means blessed Silly Virgin Mary means blessed innocent Virgin Mary in the 13th Century Gradually it shifted to mean innocent in Shakespeares time Innocent is connected with needing some compassion and in our time it means not very bright Nice used to mean finegrained small pieces as in nicety but means pleasant in our time In this way bear to carry a burden shares the same root as birth th is grammaticalization of another word like warmth for warm and transfer prefer borrowed from Latins protoIndoEuropean root of bear fer Semantic widens and narrows meat used to mean all food but has specifically come to mean animal flesh Bird used to be specific and foul was general but now the semantic of bird is widened Word order change Modern languages have all combinations of subject verb object order Both SOV and SVO are very common historically change from SOV to SVO has been common while the other direction has been rare Modern German Turkish old English biblical Hebrew are all SOV languages while modern English and modern Hebrew are SVO Welsh Irish Gaelic Polynesian languages are VSO OVS is rare some South American native language uses it An Australian native language has no fixed order Suffix and context differentiate subject and object Language studies words sounds grammar and meaning change over time Patterns of change Language is constantly morphing not for good or bad Some directions of change are predictable eg th in English to t f or s Some semantic decay can be seen in lielay inferimply hitherthitherwhither thou lookstthee look is shotshotten spitspitten in modern English Earlier pronounciation books recorded sound became available only in the 1870s suggested dismaydismiss should be pronounced dizmaydizmiss and balcony has emphasis on co Modern English says a house is being built while 19c English may say a house is building The Roman Empire is unique in its efforts of Romanization ancient empires eg that of the Persians rarely impose that their language be the only one taught in conquered territory Latin uses noun suffixes to indicate object and doesnt have to a the etc Modern Romance languages dropped these noun suffixes Some common patterns of change eg grass in Latin herba h is usually fragile in Romance language French herbe Spanish hierba pronounced yierba Italian erba Portuguese relva Romanian iarba English is protoGermanic and has undergone lots of changes too Shakespeare writes in a language that is very different from what is spoken today Due to semantic drifts Shakespeare translated to modern foreign languages can often be easier to understand Eg wherefore meant why not where wit meant knowledge not as in witticism humor is then fragile Jane Austens language is also different from ours eg you are come at last or it would would not it is perfectly normal in her time Archaic Chinese evolved as well several branches of modern Chinese can sound like completely different languages 7 major Chinese languages in mandarin cantonese shangainese fujianese taiwanese gan xiang hakka language kejiahua In English present progress tense vs present habitual tense is pretty unique nice the ing form of a verb like I am eating is not present in languages like French or German In Semitic languages of AfroAsiatic language family Hebrew Arabic Ethiopian etc Triconsonantal root with vowel variations is rather unique In Japanese Southeast Asian languages and Chinese some features include numeral classifier sounds like la ba me to help convey meaning no distinction between he she it in sound generally more contextual than English ProtoIndoEuropean Studies showed the similarity between Sanskrit Latin and ancient Greek none of these are spoken now They trace back to the same ancestor a protoIndoEuropean language which is the ancestor of most languages in Europe except Hungarian Basque Finnish and a few others Iran and upper twothirds of India lower third was Dravidian language family This was not a written language and its original forms can be deduced from variations in todays descendant languages sound and more so in grammar ProtoIndoEuropean is thought to have started from Kurgan language of southern Russian steppes others may claim Anatolia Turkey as genetic traces would suggest as well as certain patterns living descendants of this language tend to have eg common words in descendants for horse wheels but not oak vine or palm trees Germanic branch of IndoEuropean has common deviations from Latin including p f pater to father t th etc This is described in Grimms Law Latvian and Lithuanian the Baltic branch of IndoEuropean is thought to have undergone more conservative changes and remain closer to the IndoEuropean root More so in Slavic languages too Armenian and Albanian although IndoEuropean in origin have borrowed more from neighboring languages and have fewer words in common with the other branches of IndoEuropean languages Comparative reconstruction decuding the original form of a protoIndoEuropean word from several modern day extinct descendants The general methodology seems to combine the more uncommon features from later variants some common forms of decay patterns of change have been noted as those especially if appearing in multiple descendants in different geolocations are more unlikely to be the results of coincidental change the multiple descendants all went through Tracing language families Tracing the source of a language family usually the area with the largest diversity Austronesian languages spoken east of Australia Polynesian Indonesia Malaysia and Madagascar is thus thought to have originated from Taiwan as three of the four sub families in Austronesian have their presence in Taiwan Similarly Bantu languages Swahili Xhosa etc spoken south of the Sahara are thought to have started from Cameroon and east Nigeria Khoisan languages click languages spoken in southwestern Africa may have been the descendent of the first human language as its families are very diverse and the raison detre of some features like clicks would otherwise be hard to deduce Native American languages are extremely diverse in regions like California and further south but not in Alaska even though Archaeological evidence points to native Americans having migrated across the Bering Strait This correlates with climate change theory in that native Alaskans languages are those spoken by the people from down south who repopulated that area after the ice age Cases for and against a protoworld language Language dates back 150k to 50k years writing only dates back 6k the further back the more difficulty in comparative reconstruction The protoworld language theory may be on to something in that there might have been one language that was the ancestor of all or a protoeuroasian ancestor but so far the theory and deduction seem shaky Some similarities are purely by chance eg in English and Japanese while others are harder to explain eg a dialect of Nepal and a Papua New Guinea language Often times a grammatical structural similarity demonstrates a higher likelihood of two languages sharing the same root than pure pronunciation coincidences Dialects Each language is a bundle of different variants What became canon for a country is often times more geopolitical than intrinsic to the language itself Italian of the Tuscany variant English of the London variant French of the IledeFrance variant became canon not because they are superior to that of Sicily Kent or Provencal in any way rather from decisions of the past eg nationalism in forging a unified national identity one particular variant is picked and standardized that of the ruling class with the most political military power became canon Such canon changes over time court Russian was once Ukrainian due to the power center being in Kiev USSR time court Russian becomes the Moscow dialect and Ukranian was considered a peasant dialect fast forward to modern times where Ukrainian is considered its own language Written records suggest 500 years ago it wouldve been really difficult for a London English speaker to understand a Kent English speaker even though the grammar underlying structure are very much shared the pronunciation can be fairly different Similarly those we consider different languages today sometimes sit better as variants of the same bundle Scandinavian languages Swedish Danish and Norwegian are quite similar to each other Erdu and Hindi are quite similar grammar and sometimes vocabulary but backed by different writing systems Erdu uses an Arabiclike writing Hindi uses Devanagari writing system and considered different due to historical enmity Moldovan and Romanian are similar when Moldova was an SSR of the Soviet Union its tasked to come up with a Moldovan language backed by Cyrillic alphabets as opposed to Romanias Latin alphabets but the underlying are very similar On the other hand different seven dialects of Chinese are often as different as French Spanish is Or as different as Arabic spoken in Morocco vs in Syria In the Chinese and the Arabic case the writing system is the same but pronunciation can be vastly different and sometimes even grammar They are considered dialects more often due to geopolitical nationalism reasons than anything intrinsic to the language itself Dialect continuum its sometimes hard to decide the line between one dialect or another as each one differs with the one next door only by a little but looking far enough the two become distinct enough These then become a collection of dialects whose diversification happens on a continuous scale Diglossia A condition where two languages or two variants of the same language are used in the same geolocation by the same people usually divided into a highlow variants Not that the high is superior in any sense just that having different variants spoken in different contexts is a common phenomenon Examples include standard Arabic as the Quran is written in and Egyptian Arabic which is used colloquially and very different Hochdeutsch and Swiss German English h and French l in old Quebec French h and Russian l in Anna Karenina in English itself where some words are more formal while others more colloquial but they mean the same thing kids children bag parcel go back return etc Some states have triglossia eg Java Island where high middle and low Javanese may sound very different and used in different contexts Diglossia is often the effect of one version of the language being written in works of great importance and that version becomes fixed Uusally the H version is a historical snapshot of the language Eg standard Arabic being what the Quran is written in and Egyptian and other variants of Arabic kept on changing Writing slows down the pace a language changes it might be easier for us to converse with Shakespeare than him with a person in the 11th century The H is not necessarily always more complicated than the L it is so in some cases eg spoken French dropping ne in nepas but in spoken Chinese dialects there can be more tones than Mandarin Writing is almost orthogonal to speaking the former being something one goes through formal training to acquire and the latter being more of a biological instinct The written language is almost always very different from the spoken one more composition more structure bigger vocabulary longer etc The start of Hebrew Bible may be a good example of when writing mimicing the spoken language as the former was not yet too developed The written language in Gibbons work on the other hand would demonstrate a style that is very distinct from a spoken language Not that either is superior in any way as both can be elegant English as it is today is influenced by rules people like Lowth introduced somewhat arbitrarily Examples include peculiarities like you were was arent I am not I to whom to who meaning of double negative aint got nothing These peculiarities are often based on their notion of Latin and Greek being the superior languages any change decay in a language is undesirable and that grammar should always be logical Borrow and mixing Languages borrow and mix both words and grammar English in particular is a bastard language in the sense that most of the vocabulary today is borrowed 1 percent from old English and the rest from old Norse dating back to the Viking invasions Norman dating to William the Conqueror and Latin Greek dating to later when English became a language of learning The shorter and more widely used words often are different from other languages and rooted in old English while the longer ones for something specific and complex are usually quite similar with French etc This borrowing creates interesting effects like doublets of words and the high low variant eg beef and cow pork and pig There isnt a language quite like English having a level of similarity like those between different Slavic languages or Portuguese with Spanish the closest is probably Dutch but thats still a long way off Grammar too are borrowed An example being the IndoEuropean language like Hindi coming into contact with Dravidian language in the south and being impacted by its SOV ordering Codeswitching switching languages midsentence following a particular set of rules is a sign of a bilingual person with dual cultural background and another process in which grammar of one language can be introduced into another and new languages be born Eg a language in Ecuador that is a mix of Quechua and Spanish How the modern education teaches languages books on grammar memorizing words speaking after some teacher is not how language has been learnt in the past And the learning process of hearing and repeating in the past presumably contributed to the much borrowing and mixing we see today as well Languages mix and borrow in a symbioticlike fashion The branches of a bush mix and form a net Language areas where languages of different family draw features from each other and became closer in terms of grammar and words An example is the Bulkan area where Romanian is of Romance subfamily of IndoEuropean Albanian of Albanian subfamily and Bulgarian of Slavic subfamily yet Romanian adopted having thea behind the noun unlike other Romance languages but like Albanian and Bulgarian got thea in the first place which is unlike other Slavic languages Another area is Sinosphere where TaiKadai Laotian and Thai AustroAsiatic drew from SinoTibetan Complexities in languages Languages naturally develop complexities that are not quite necessary for being able to clearly express For example has done perfect tense in English event happened in the past having impact on current a peculiar feature to mostly European languages gender masculine and feminine forms factual marker needing to differentiate a fact you see heard or hearsay with an ending to the word etc Such usually comes from grammaticalization There is no defined standard what is needed to clearly express in a language but each language picks up a few features such as tones endings factual markers tenses etc All languages are complex but not equally so The case usually is that the more secluded the language the more complicated it gets An example being Tsez a language spoken by tens of thousands in the Caucasus where the sound involves waggling the back of your throat has four genders and variations on verb and adjectives when they dont start with a consonant Children has amazing imitation skills in picking up these complexities in languages and beyond a certain point such a language would be very difficult to learn Languages simplify due to external influence eg a particular group of adults learning the language Real Javanese was formed around the time of different ethnic groups coming to learn the language as was Mandarin Chinese which was deemed simpler than Chinese dialects for its reduced variants in tones and possible consonant endings Mandarin came around a time when groups of Tibetans Mongolians etc adults came to northern China to learn the language The Mandarin simplification was not as extreme as that of Real Javanese ProtoGermanic and English German English Dutch Faroese and Frisian are derived from protoGermanic family One of the characteristics of this family compared with other branches of IndoEuropean is its emphasis is usually on the first syllable making endings particularly vulnerable to being dropped Modern English keep very few case endings including s and more commonly used ones like him whom etc ProtoGermanic traces two thirds of its words back to protoIndoEuropean and the other third is unknown including words like sheep One theory attributes the rest to Semitic influence There were records of Semitic speaking sailors in Northern Europe Some similarity in words exist and sometimes the verb transformation reflects the triconsonantal root with vowel variations seen in Semitic languages eg think thought Grims law could also be related with Semitic influence Old English is much like German also being SOV the change from old English likely was due to invasion of old Norse speakers They settled in learnt the language halfheartedly and following a learning methodology very different from todays and resulted in changes simplifications in the English grammar Language and culture psycology of a people SuppearWarf hypothesis claims grammar channels the way people think This is luring but unproven The current belief is that language is largely not indexed to culture and its transformation is unconnected with culture Warfs example was standard average European language Germanic Romance languages vs native American language Hopi The latter has one word for all flying objects except birds differentiates water you use and water you see doesnt have tense which supposedly reflects native Americans cyclical view of time The theory largely thought the likes of English being crude and Hopi sophisticated the typical trend in linguistic study of his time Examples for the theory includes empirical observations but not proofs Honorific distinctions in Japanese Korean reflecting the respect hierarchies in these cultures Also more often than not French speakers think of tables as female in cartoons due to gender markers in the language Speakers of a language that does not differentiate bluegreen glu seem to take more effort differentiating these two colors and those who have two yellows take less effort to differentiate different yellows Problems with the hypothesis include Warf being wrong about Hopi whose view of time is not cyclical but recordbased and the language does have some time markers And counter examples include inalienable possessive marking usually not found in languages spoken by the capitalist world but rather those in the Amazon jungles Another being fly crawl float are all move with different particles as opposed to their own words in Navajo a traditionally nomadic people Navajo differentiates handling of objects depending on the shape and form of something this fact did not seem to make the speakers more likely to differentiate objects by form and shape as opposed to color and size Something else does The theory of languages reflecting culture has never been proven Pidgin A particular group of people learning a language to get by also resulting in a much simplified combination of the two languages Them making systematic mistakes which evolved into implicit rules in the bastard version of the language This includes Cantonese traders speaking English where Pidgin came from 18th Century native American English a RussoNorsk language spoken by Russian traders spending their summers in Norway The RussoNorsk language in particular has no markings verb conjugations one preposition no gender very simple pronunciation 300 words with a few for different alcohol Creoles Every language traces back to something and Creoles are instances where new language gets created from pidgins Note that Creole doesnt specifically refer to Creoles of European and native American origins When pidgin speakers do not go home and keep speaking their pidgins over time due to the need to express themself in nuanced ways a language gets built back up from stripped down versions that is a pidgin having a full language is part of being human Example include Tokpissen business talk in Papua New Guinea its Lingua Franca British South Sea plantation brought over population from New Guinea and this language is developed propagated back to the islands and became a way people from different islands speaking different aberiginal languages can communicate Lousiana French Creole Haitian English Creole Guyana English Creole West African and dying Indian Portuguese Creole Uganda Arabic Creoles are all examples of such Most Creoles are created from colonization and slave trade They differ from dialects of the language in that the grammar and how the vocabulary are used are different enough that listeners to a Creole cannot tell what the original language was and how it evolved to the point of the Creole Saramarkan Creole in Suriname Carribean Creole resulted from slave trade has a hybrid European vocabulary Slaves on the planatation run by the English then the Dutch with a mix of Portuguese Jews fleeing the inquisition and an Angola West Africa influence developed this language It developed simple tones something no pidgins have a particular grammar that involves different forms of to be and nuances that are hardly necessary for conveying meaning unambiguously and a marker indicating something fallen being thrown finishing its trajectory Hawaiin English Creole has a grammar that developed from the Chinese Japanese and Phillipino planatation worker pidgin Bioprogramme hypothesis Bickerton claims if there is a universal grammar Creole languages would reflect this grammar without all the complexities of a traditional language developed over time almost all of which has eg either tones or prefixes suffixes This is again controversial Creoles are often times more streamlined than traditional languages that developed maybe accidental gunks over time and could reflect what a language needs or even the first language if there was one Creole is a continuum concept as languages and dialects there is the language that gave it its words a deepest Creole and many variants in between Afrikaans is eg more streamlined than Dutch eg no gender but not a full Creole Black English A dialect of English not any bastardization nor a Creole or an African language Slangs are just the icing its also a unique sound system th and f and grammar dropping to be be being a habitual marker not a present marker Language death Today we are at a point where there are fewer languages than any other point in human history Over 10k languages became about 6k and 96 of worlds population speak one of the big 20 languages Chinese Spanish English Hindi Arabic Portuguese Bengali Russian Japanese Lahnda Punjabi Javanese German Korean French Telugu Indian Marathi Indian Turkish Tamil Indian Vietnamese Erdu Projection suggests 500 of the 6k languages would remain in 2100 Among the 300 native American languages over 200 are not passed on or spoken by anyone any more The process of death for a language is usually it reverting to pidgin vocabulary shrinks structure melts away finally it devolves into dust think immigrants children It takes about one generation for a language to not be passed on and die Why should one care Is diversity always nicetohave Languages may project things abbout our culture and mind assumption which is a complex issue Language revival artificial languages Once it needs revival its already dead Known efforts include Celtic languages like Welsh Gaelic Irish Gaelic Breton New Zealand Maori Hawaiian Mohawk In the near future chance of all people speaking English as their first language is very unlikely for most as a second language To preserve all 6K languages is also unlikely urbanization hastened the death of smaller languages due to them not being written and sometimes not seen as a language Also languages are hard especially dying small ones in reality there is little context to use them Hebrew saw a successful revival effort from being only for liturgical uses to an everyday language this has to do with Israel being a special situation and Hebrew being well preserved in literature Artificial languages including sign languages are created since 1800s with a universal language goal in mind Most have died Volapuk invented by a Bulgarian priest is one such example based on Romance and Germanic languages and very complex The creator thought the complexity in languages a necessity and not accidental Esperanto is still sortof alive this day with 1 million speakers Its based off of the creator witnessing animosity between Russian Polish Yiddish and Germanspeaking ethnic groups Creator also has the notion that Romance and Germanic languages should be the base for a universal language Esperanto is much easier with 16 rules nouns end with o adjectives end with a verbs with i present tense with as past with is future with os conditional us command u etc Finale The series try to focus more on how change is inherent to a language with the opinion that systems are more important than collections and individual etymology Languages are dynamic symbiotic and hallmarks of being human"},{"title":"\"The mythical man month\"","href":"/notes/the-mythical-man-month","content":" The mythical man month Why is programming fun Joy of making things Making things useful for other people The fascination of fashioning complex puzzlelike objects of interlocking moving parts and watching them work in subtle cycles Joy of always learning sprung from the nonrepeating nature of the task The delight of working in such a tractable medium Its fun because it gratifies creative longings built deep within us and delights sensibilities we have in common with all men The woes of programming One must perform perfectly Other people set ones objectives etc Dependence upon others Finding nitty little bugs is just work Work over which one has labored can be obsolete upon before completion Menmonth as a metric men and months are interchangeable commodities only when a task can be partitioned among many workers with no communication among them eg reaping wheat or picking cotton which is apparently not true for system programming Authors usual estimation for software projects planning testing unit testing and early integration test integration and system test Adding manpower to a late software project makes it later Brookss law Team division Best performers and worst performers may have a 101 productive ratio Many would prefer a small team of elites Though the cruel fact is for a system large enough albeit more efficient it would probably still take too long for the small team of elites making the product obsolete by the time its complete One effective way could be to organize the software team as a surgical team Mills concept The surgeon chief programmer defines the specs designs the system and is able to do any part of the work Needs great talent and experience The copilot is the alter ego of the surgeon table to do any part of the job but less experienced His main function is to share in the design as a thinker discussant and evaluator He serves as insurance of disaster to the surgeon The administrator surgeon is in charge but his effort should not be wasted on money personnel space etc The administrator takes care of that The editor edits the documentation the surgeon writes An administrator secretary and an editor secretary The program clerk maintaining technical records The toolsmith The tester The language lawyer master of the programming language in question The whole system is the product design of one mind or at most two acting uno animo As opposed to the conventional team where each part of the system is the design and implementation of one person Specialization of function and a lack of the division of the problem and a superiorsubordinate relationship are key in this concept In a larger team the conceptual integrity of each piece should be maintained Meanwhile a sharp distinction need to be made between architecture design and implementation Aristocracy democracy and system design Conceptual integrity is the most important consideration in system design as determining factor of a systems easetouseness Designing or architecturing especially setting the external specifications the manual of the product is not more creative than implementation the design of implementation The former process is an aristocracy which requires no apology Finishing architecturing does not necessarily block implementation design efforts The secondsystem effect What happens when architect proposes something more than the implementation can achieve resolved by two parties communication An architects first work is apt to be spare and clean He knows he doesnt know what hes doing so he does it carefully and with great restraint Once done building the first system this second system is the most dangerous a man ever designs the general tendency is to overdesign the second system using all the ideas and frills that the were cautiously sidetracked on the first one and extra caution is usually required when designing the second system Passing the word The manual describes what the user sees does it should refrain from describing implementation details whose freedom should be left to the implementers Pros and cons of formal definition or implementation as definition prose definition in the manual Having effective weekly architecture meetings annual supreme courts for remaining minor decisions Telephone log QA of the architect Why did the Tower of Babel fail Communication is key Communication happening informally via meetings and via a workbook Workbook keeps the external specs the internal implementation design and documentations etc Treelike organization in large software systems Possible relationship between the producer and the technical director Thinkers are rare doers are rarer and thinkerdoers are the rarest Calling the shot Effort required is roughly proportional to size of program15 Ten pounds in a fivepound sack Size control budget memory resident space as well as harddrive usage define exactly what a module should do as you place limit on its size the system architects must maintain continual vigilance to ensure continued system integrity Programming manager totalsystem useroriented Representation of data is key to managing spacetime tradeoffs the essence to programming The documentary hypothesis Required documents What objectives specification first to come last to finish When How much Where Who Plan to throw one away Prototype Be prepared to redesign and reimplement Plan ahead to build a throwaway you will anyhow Design and be prepared for change Structure an organization for change Program maintenance is an entropyincreasing process Hence at some point itll be a one step forward and one step back process Sharp tools Toolmaker of each surgical team tools documentation system performance simulator Gradual adoption of highlevel languages and interactive programming The Whole and the Parts General paradigms to avoid bugs Testing the specification externally Topdown design Structured programming avoid gotos Hatching a Catastrophe Milestones need to be concrete Fuzzy milestones boosts chronic schedule slippage which is a morale killer and leads to catastrophe One characteristic of programming teams hustle running faster than necessary moving sooner than necessary trying harder than necessary The other piece is late anyway is no excuse PERT chart critical path scheduling identifying scheduling dependencies Plans and Controls group on a large software project watchdog of the schedule Boss those higher than the first line managers should encourage truthful status reports and try to refrain from acting on them unless necessary The Other Face What documentation is required Overview purpose environment domain and range input and output functions realized and algorithms what it does input and output formats operating instruction how to use and what to do if abnormal options what choices are given to user running time accuracy Every program shipped should have small test cases that serves as proof of working More thorough test cases should include mainline cases barely legitimate cases edge cases within the domain of input data barely illegitimate cases Flow charting every detail statement is not useful flowchart system diagram on a higher level could be better for describing the internals of a system To be able to reasonably document something we should try to minimize the burden of documentation Comment your code name your variables structure your code so that its easy to read selfdocumenting code Document your code as you are writing it No silver bullets Claim no single improvement in technology management would cause the software development productivity simplicity or reliability to increase by an order of magnitude The making of a great designer"},{"title":"\"When genius failed\"","href":"/notes/when-genius-failed","content":" What is Long Term Capital Management LTCM LTCM is a hedge fund founded by the bond arbitrage group from Salomon plus Nobel laureate economists Merton and Scholes from Harvard They operated from 1994 to 1998 culminating in 1 turning into over 4 for each dollar invested in 1993 and a 45 billion total capital and losing it all each 1 worth 33 cents in 5 weeks in 1998 The Fed organized a bailout operation to ingest capital to LTCM from major Wall Street banks out of fear of LTCM collapsing triggering a series of crises What does LTCM trade How do they make money LTCM specialized in spread trades usually betting the spreads to converge a field of expertise inherited from their days at Salomon whose arbitrage unit was its main source of income Spread trade is identifying usually a pair of instruments with one or both instruments thought as mispriced due to short term market inefficiency or psychology Eg two 10yr US treasury bond issues with a 6month gap the new issue on the run after its issuance usually has a lower yield and higher price compared with the old issue off the run due to the new issue more actively traded more liquid than the old issue although the chance of US govt defaulting within this 6month period is practically 0 meaning over time the two bonds arent that different and the yields and prices should converge thus they long the cheaper one and short the more expensive one and no matter if the prices of both go up or down as long as spread narrows LTCM makes money This trade albeit fairly safe only pays pennies since the spreads usually arent that large to start with To make a fair profit compared to its capital LTCM borrows a lot to put in these trades building a leverage of 301 for each dollar of their capital the money they raised from investors they borrow 30 dollars from banks at a really low sometimes none haircut to invest in their trades LTCM was able to do so in a bullish market where banks were actively looking for parties to invest money in and were afraid of losing LTCMs business to another Similarly such opportunities can be found in mortgage PO principal only IO interest only strips if interest rate borrowing cost goes down people tend to refinance their mortgages and pay back early thus IO yields will go down and PO yields go up treasury bond yields go up Making these proxies to bet on interest rate movement Another example being the yield difference between Italian and German bonds Italian bonds typically considered higher credit risk yields higher than German bonds Yet with EU and their unification of currencies on the horizon this spread should narrow LTCMs phenomenal gains were from such trades achieved with very high leverage Such opportunities are found and analyzed via models assuming markets are will become efficient over time market prices are continuous volatility is an innate constant of a particular instrument and volatility of today tomorrows can be predicted with those of the past For LTCM partners themselves they invest their money in the fund and charge a ridiculous fee to their investors They were able to entice investors with such fees due to the high profile fund managers raising 2 billion in 1993 The details of their trades are not disclosed to investors As others gradually catch up and utilize similar methods to identify mispriced opportunities LTCM resorts to more risky trades in equity volalitity trading merger arbitrage and directional bets And usually such are done via derivative contracts with major banks The fund had 60K such contracts and is deeply entangled with financial institutions worldwide by the time of collapse Such contracts made over the counter do not show up in their statements sheets Usually to keep their trades secret they pick different counterparties for the same trade Eg to do a spread trade on Italian and German bonds Goldman may see one leg and JP Morgan another making it difficult also for banks to identify their risk and exposure via LTCM As for the more risky trades equity vol bets on markets becoming more efficient and in a turbulent time volatility should go down to a more stable level The higher the volatility the higher the price of equity options youd pay more to be able to buy something at a fixed price in a more turbulent environment Thus LTCM shorted equity options again with a huge leverage Merger arbitrage is a bet that an annouced merge will work out causing the stock price of either party to move towards an expected level LTCM does not have expertise or information advantage in this area Directional bets are plain gambles without hedging eg buying high yield junk bonds emerging markets etc hoping they wont default How did it all go wrong so quickly The triggering event was Russian govt bond defaulting in 1998 before that crisis in Asian currencies and shortly afterwards turbulence in South America LTCM had Russian bonds directional bets but that by itself could not have brought down the fund The real problem is its effect on investor psychology nuclear powers dont default or do they Investors flocked to more secure instruments US treasury bonds and quickly dragging down the prices of everything else Treasury yields dropped to historical low and high credit risk instruments yields grew extraordinarily high The spreads widened for a prolonged period of time In the long run LTCMs trade would have made money but the companys capital has to be able to survive the spread widening period given its leverage ratio the pressure was big even with a small percentage and LTCM could not afford to be wrong for long For that the company is forced to sell some of its positions but there are no buyers no liquidity as everyone only trades treasury Spread kept widening for 5 weeks and LTCM lost most of its capital and it was either a sell bail out or a bankruptcy Govt agencies the regulators should not meddle in the lives and deaths of hedge funds prevention as opposed to intervention however in the case of LTCM to prevent a further disaster the Fed did intervene by having major Wall Street banks form a consortium put money in LTCM and take over A predecessor event that hurt LTCMs capital was that they returned some outside investors money against those investors will in late 1997 when the fund was doing extremely well reasoning being the investing opportunities arent as many The fund wishes to maintain its level of leverage so it returned some capital and the founders are the ones getting hit hardest when the ensuing crisis came in the losing streak leverage soared to 1001 How about diversifying their portfolio LTCM is diversified bonds equities derivatives geolocations currencies etc In form but not in essence in different geolocations currencies they invest in junk bonds or make the same bet that spreads should converge market should head towards the direction of more liquidity and higher efficiency When the global market doesnt head in that direction They lose How likely is this to happen on a global scale Perhaps more likely than LTCMs models foretold What about risk management meetings LTCM has every week The partners do assess their exposures very often yet there is no independent body dedicated to risk management It always ends up being the star traders getting their way putting bets on areas LTCM has little advantage in or placing their trust in models whose assumptions might not hold in a crisis Moral of the story Understanding the models what assumptions do they make are they reasonable assumptions would they be able to account for a real crisis in which investors are no longer rational Understand their limitations Liquidity and leverage a high leverage means you cant afford to be wrong continuously for long This plus a market whose liquidity can be questionable in times of a crisis is like playing Russian roulette with yourself Greed and hubris being invincible in one asset class does not guarantee youll be invincible in the next one not even your next investment in the same asset class Do due diligence assess the risks and make sure this process isnt just formality"},{"title":"\"支教往事\"","href":"/posts/2011-july-in-gepai","content":"201915 458 5 7 confidencecourage 7 20117B 20088 5 7 "},{"title":"\"2018\"","href":"/posts/2018","content":"December 23 Hudson River Trading Library Floor 58 4 World Trade Center 2018 has been an eventful year Leaving much to be pondered and appreciated Three visits to Chicago As we wandered by the snowcovered shores of Lake Michigan in January I wouldnt have known two more trips to the same city awaits me a mere few months away Clouded by a sense of crisis and self doubts there was a month when I said yes to every recruiter who reached out 7 interviews were scheduled 5 of which onsite And by the grace of Fortuna 5 offers ensued Some from companies that I knew little about but gradually came to appreciate IMC Akuna in Chicago and Zoox in Silicon Valley Others perhaps beyond my dreams Google and Hudson River Trading In the last of which I currently sit on the last Sunday night before Christmas I rarely thought myself a strong candidate having less than 300 leetcode questions under my belt Paseo de la Reforma It now seems laughable how I frantically checked for updates from Google on the way back from Teotihuacan to Ciudad de Mexico Pacing restlessly up and down Paseo de la Reforma looking for a quiet spot to answer the call and desparately trying to take in all the details knowing that I have made the cut The trip left fond memories My foolhardy attempt of pointing a phone at the stormy skies to capture the weapon of Zeus and enjoying a hearty meal under the calm morning sun on the following day Osiris Two weeks later I cleared an entire day of interviews in Four World Trade Center In Room Osiris overlooking the Statue of Liberty where in between the rounds I had the leisure of taking in panoramic views of the Hudson River glistening in dazzling summer sun Why they named the room after the Egyptian deity was beyond me but a fitting name nonetheless him of rebirth and of Nile inundation hence of life and harvest Arguably I find it hard to judge if Im worthy of such Prudence often advised against judging especially when not knowing the complete picture or being able to assess the variance in the most minute details Worthy or not I am grateful for everything I was gifted with and remained willing to commit my passion and efforts into what is deemed right May our path stays guided by prudence and such willingness last 22 Vain and jealous as it may sound the 22 was my last straw"},{"title":"\"On habits\"","href":"/posts/2019-on-habits","content":" Bookkeeping Often enough we found ourselves not being able to pick up notes or research results done longer than a few months ago To ease future lookup and improve future efficiency we should follow certain bookkeeping conventions Work logs research results profiling outcome historical binaries should go into designated location on a shared drive or public git with personal projects with folder names indicating subject and style of work done each folder should have a readmemd recording what this is and why what we tried the current status results as well as comments on anything in the folder that is not immediately obvious These should include failed experiments where we should also record why the idea is given up what the conclusions are and current plan is It seems good to also have one centralized and shared place to record overall status tracking and projects we wanted to take on for atwork and personal ones The scale and time horizon of these nowadays appear big enough to justify another and hopefully last formal bookkeeping attempt Coding design habits A common theme we are seeing more often with personal work is having to rework things previously done procrastinating too long in getting simple features in eg meaningful logging using designated modules division of components meaningful argparse or opting for brute force approach as opposed to setting tools up for longer term eg consistent autoformatting Software architecture recommendation we received was to generally prioritize going well over going fast and my past behavior also seemed to lean generally towards the latter Then as an overall principle in personal projects I should consider spending more time on design think and research more before you start writing knock out problems we consistently get confused about follow known best practices and if something takes too much effort to follow invest into ways of automation This echoes back to an earlier realization regarding what to do when encountering unfamiliar vocabulary in English literature Finding a balance seems nontrivial yet the tendency of glossing over seemed to have harmed overall efficiency and quality in recent observations Knowledge compaction consolidation It seems almost cliche that we often see ourselves repeatedly making similar realizations Completely forgetting and relearning something from scratch is fine and often times would come with new and improved insight but having to relearn too often or having knowledge retained in memory for too short appeared to have hurt our overall effectiveness This appears to be a compounded effect of the following subpar bookkeeping hoping to be better addressed in item1 not understanding the subject matter well enough in the first place hoping to be better balanced in item2 not having an established compaction schedule Historically we had issuetracker implemented but the habit was never formed and the strict curve in the first iteration appeared too much overhead to actually implement Compaction often seems less cool than picking up new things which makes it more critical for us to establish well thoughtout procedures to carry them out Personal finance If ones life would be long enough for law of large numbers to kick in presumably following an established hopefully backed investment strategy would pay off This is something we should get into habit of doing for longer term our financial well being Until dollar cost averaging on a total stock index has been found to be ineffective should we just follow this strategy Health Ergonomics exercise cleanliness tidying up your room good sleeping habits all matter Think in the longer term invest in the future Daytoday Empirical observation would suggest most biggest factors in loss of efficiency are usually distractions and illness Distraction comes in the form of not wanting to start and often correlated to not having rested well enough Itd be useful to identify instances of each and regulate our behavior in reasonable ways"},{"title":"\"Addressing self\"","href":"/posts/addressing-self","content":"Nov 10 2018 Preface It always surprises me how futile the attempts were to refrain from looking back especially on a fine Saturday morning in office overlooking the Central Park and having the entire floor to myself Often times the thoughts revolve around how much has changed and the naivete of my past and current self that the regular feeling of deja vu started to convince me I was trapped in a labyrinth making little progress in terms of real realizations Perhaps such is the process of making realizations however we dont enter the labyrinth emptyhanded We dive in this time equipped with a ball of thread like what Theseus was given in an attempt to jot down the fine details of our thoughts through which we derive the core values and principles External conditions Recent events reinforced the realization that much of my fate is not in my own hands For the first time in my last five years as an immigrant to this country I saw how limited my freedom is in pursuing what I deemed worthy due to the restrictions in switching employers and traveling abroad This may be a moment when I should swallow my pride fear and anxiety and accept it for what it is as the serenity prayer would preach On one hand to live and learn is but the best option we have and on the other hand there is something to be salvaged out of every misfortune and it is exactly through such we evaluate ourselves steer our goals and harden our resolve In these endeavors know that we do not bow to the whims of external conditions I hold firm the belief that ones true excellence is decoupled from external conditions The goal is to master my own fate The way there is through the pursuit of knowledge And in the pursuit I aim to stay true to the virtues below and to weather the storms with faith and temperance Pursuit of knowledge We often times pondered what it means to really know something and among all the arts in this world what would be worth knowing The majority of my life was spent as a student yet among the things I was taught I cannot say I learnt most of them One can read a book audit a course labor in pratices yet all of these are ways of obtaining knowledge mostly irrelevant with evaluating whether our goal of learning something is achieved I often times see these feats from those considered masters of their arts To be able to explain it to a community both from a high level and indepth in order to cater to different interest and skill levels of the audience To be able to connect the dots when something seemingly irrelevant to the untrained eyes is brought up To be able to apply what they know to produce something tangible Perhaps this is better phrased as to internalize something and to grasp the underlying principles or confluences by the analogy of some crossdisciplinary scholars Our pursuit of knowledge is then characterized by the same feats Let us not rest in the comforts of having read something but one step further of pondering discussing applying and teaching what we have read as true tests of our internalization of such knowledge In the field of engineering this can come in the forms of understanding the design alternatives building something with it having seminal discussions with likeminded people and many others For all of which we should seek and create opportunities if we find them lacking When reflecting daily what we have achieved make sure to apply the same criteria of internalization Openmindedness The topic of what is worth internalizing is a subjective and mercurial one I appreciate the beauty in most trades be it various fields of liberal arts sciences and engineering skills in particular sports interpersonal relationships and many other techniques Such diversity is what makes here an incredible place for the curious whose efforts are fueled by the freedom to pursue these It is my goal to number among the curious To be ready and excited waking up each morn thinking about the new knowledge we are going to internalize and the new challenges we are going to tackle To take in think and act with an open mind and be open and flexible to change without losing sight of what we set out to achieve or failing to adhere to our principles To reflect strengthen what we did well and readily acknowledge where I was wrong and to not judge others for our differences in the perception of what is worth internalizing Compassion gratefulness and humility I was gifted with a loving and supportive family We were not affluent in our wealth but never lacked the means to afford food lodging or education Throughout my life I was fortunate enough to learn from many whom I admire and appreciate Such cannot be said for many others not for those from a poor rural area where I taught in 2011 and not even for my parents who grew up in turbulent times It is not due to their lack of endeavors the cases usually suggest quite the opposite but simply a factor of luck To such fortune and those who participated in weaving it we show gratitude And to cherish these that we arguably arent worthy of we work and learn hard and pay it forward Do not ridicule or easily discredit others it could be just a difference in habits belief temperament and priorities Most in life are burdened with their own struggles be it career health relationships and many others Do not think that the troubles you face are the biggest adversity or decide what few accomplishments you have deserve the attention and praise of all or boast what little you own and realize Listen attentively show compassion be supportive learn from them and heartily congratulate them for their achievements Greed vanity and ignorance often cloud our judgment do not fall victim to the temptations of such Courage determination and discipline Readily jump into actions Perseverance rituals habits The wisest men follow their own directions Fragility of life To clear the unworthy out of your mind and to learn like you will not be able to tomorrow Opportunities choices and tradeoff The beauty of computer science"},{"title":"\"Bloomberg 2017 - 2018\"","href":"/posts/bloomberg-2017-2018","content":"11232018 The coming week marks the end of my first job as a fulltime software engineer working with Bloomberg LP In the last year and a half I was gifted with the opportunity to work in an incredible environment with many talented people for which I remain grateful I was fortunate enough to have forged a bond with a few among them hopefully one that will last beyond my now limited days in the company The decision Before transitioning from a student researcher to a professional engineer I did not picture myself looking for new opportunities or quitting a job in the first two years I knew there would be problems and chores that I dislike but as engineers we are hired to deal with them and in doing so we develop our core skill set namely that of problem solving It may sound hypocritical to claim that I still hold such beliefs despite deciding to leave instead of seeing it through all within two years There are however a plethora of factors behind each decision and I hope to justify mine from these ones below Growth As a junior engineer growing my skill set is the topmost concern There is a tactical and a strategic perspective to most of our problems The tactician in us identifies the best approach to the battle at hand while the strategist looks ahead and behind carefully planning the next campaign making sure its fought at the right time and place so that we could reap the heftiest benefits The strategist surveyed the battleground and became doubtful of the outlooks In seeking growth we strive for the areas we want to grow in to line up with our efforts at work Yet most of the time we found the work so focused in the mundane that we doubt if anything we gain is of strategic value ie transferrable skills Coupling that with legacy frameworks inherited from a long past by software engineering standards the outlook dims further Legacy systems arent necessarily evil and everything has its lifetime We were rightfully taught to be skeptical about the new and shiny technologies and to focus on grasping the fundamentals A system infested with questionable practices outofdate design decisions and friction in iterability may make a great proving ground for a skilled veteran and Id be delusional to consider myself one such Its comforting that the company is actively looking to address many of these but it takes time to overcome such inertia Seeking change Being curious is mentioned as a highly desirable trait in software engineers by many sources Seeing the woes of our development today sometimes I cant help but wonder how others do it differently What could be better than to experience the difference firsthand Without seeing the contrast its difficult to appreciate what we enjoy here Over the years I saw myself becoming accustomed to bad practices and getting satisfied from just getting by This would be the exact state we fear settling for the suboptimal knowing only the superficial essentially becoming stale The environment assimilates us it lures us to its comforts dulls our senses and weakens our resolve I am aware the young are often criticized for their lack of perseverance in pursuing one goal for a prolonged period of time I myself may very well be subject to this blame On the other hand we live but once the twentyfifth year of our lives perhaps the best period to expand our horizons and embrace change Going forward It pains to state that from my undersampled observation quite a few who left are considered the best and brightest among us I admire the dedication and loyalty as well as the talent of those who stayed Nor dare I number myself among the ranks of the best and brightest Now that we are given an opportunity to start fresh at a place well regarded by some I hope we make sure to treasure such A few areas we hope to do better in Proactivity Deliver Critical path and prioritization Advocate for yourself sell and speaking up"},{"title":"\"Recurring themes in software engineering\"","href":"/posts/engineering","content":"2019 marks the end of a decade since I wrote my first line of code and two years and a half since I became a professional software engineer Along the process some recurring themes are observed lessons learnt realizations made and this summary intends to be a discourse on such on a methodological level ie a collection of commonalities The hope is that when faced with an engineering problem one can see the alternatives easier make better judgment calls and think architecturally when needed Tradeoffs There is no magic It is all tradeoffs It is so between timespace in an algorithm biasvariance in statistics and noisereduction vs detailssharpening when post processing a photo As is with most choices a small company vs a big one moving fast vs architecting something well having an abstraction that covers all vs individual solutions tailored to each Anything reasonably sophisticated would likely involve making nontrivial tradeoffs and arguably this decisionmaking element ie designing or strategizing is where the most dynamics and excitement lie Know the approach identify its tradeoffs map them to first principles and be able to draw upon them when designing your own systems Prioritization is a prime example of tradeoff A practical system cannot be perfect in architecture Looking at MapReduce in 2019 perhaps few would think a fixed framework of mappers and reducers with a sorted shuffle step forced in between and materialization of transient states is anywhere near general enough for distributed processing workflow Yet it worked well and was general enough for a time for particular workload like building search indexes and PageRank Know your use case and its bottlenecks and make tradeoffs from there Simplicity A complex system that works is invariably found to have evolved from a simple system that works The inverse proposition also appears to be true a complex system designed from scratch never works and cannot be made to work Simplicity comes from knowing what things our system needs to do and not being overambitious do one thing and do it well single responsibility plan for some changes but not all of them Unix programs principle are good embodiments of such as is a RISC instruction set GFS uses one inmemory master for directory storage and failover is tackled by a different component Simple to reason about and simple to operate Why make something more complicated than it needs to be There may be a valid reason for that to account for some possibility of future changes be it features scales or operability Yet such concerns can lead to premature optimization Some woes observed from the past include We thought we need multithreading because we didnt understand concurrency well enough We thought we need a highavailability distributed NoSQL database because it is in fashion We thought we need blockchain because we never thought whether our system model need to be trustless There is a fine line here How can a system be simple if the business problem it models itself is complex to start with We break it down into pipelines and compositions of components and letting each achieve one and only one goal We then cheat by hiding the complexities behind abstractions of each Abstraction generalization and specialization Abstractions make complicated things seem simple Coding in assembly vs C or Python is one example and having five layers in TCPIPs stack is another Abstractions take freedom away from the user you are tied to the particular ways the abstraction approaches a problem and as a result performance is often sacrificed specific optimizations for some users particular cases cannot be made and special features may become awkward to accommodate Performance is overrated anyway Shame on me for saying that as an engineer tasked to build highfrequency trading platforms Though performance concerns do seem the prime reason leading to premature optimization ie the likes of I thought this would run faster in hypothetical scenarios ABC Make datadriven decisions and know that for most of our cases something should be as fast they as they need to be and anything faster than that might be making sacrifices elsewhere maintainability operability evolvability cognitive overhead etc Think twice if those are worthy sacrifices Yet often enough they are worthy sacrifices and specialization comes in As the domain evolves two uses cases thought to share the same logic start to diverge the abstraction becomes over ambitious or slow Holes are punched encapsulation violated patches that hurt maintainability committed and the time has come for specializations to be introduced and abstractions refactored These are parts of the natural life cycle of software and do not necessarily indicate a bad design architecture It is at times like these our barriers are best tested Dependencies and coupling Interface segregation Dependency inversion Plan for change Domain evolves hence the software must When choosing how to store your data did you consider how easy itd be for the schema to evolve When deploying your system did you consider how easy itd be to upgrade When modularizing your code did you consider what are the likely ways for it to evolve and how well each are insulated from changes in the other on the likely paths of evolution Are the ones likely to change at the same time for the same reason near each other Are the above considerations even important to your problem Do not consciously leave traps for the future person nor should you prepare for armageddon in your code Whats the criteria Think hard and experience will tell What experience brings When someone amazes us with their ability to think outside the box and to come up with innovative solutions it often times is their box being unfamiliar to us Before we learn to think outside a box learn to think inside one In some sense experience may seem like the ability to simulate the situation accurately without needing to have actually experimented or gauging the counterfactual Our repertoire of knowledge and toolkit is that box Knowledge comes in many levels Knowing certain API and what happens underneath the hood give you an edge Knowing how to troubleshoot an inhouse system gives you more Knowing their whys and why nots is even better and Being able to create new ones combining first principle and triedandtrues of others makes you an architect There will always be the young ambitious and eager person with just too much time on your team who will outdo you in the first few levels of knowing Yet the last ones come with experience the tradeoffs are firmly implanted through your training that you will deduce sometimes without having to experiment what will work and what will not Think hard at the boxes weve built and at the ones we are given dissect them take them apart and rebuild them in different ways and from such gain experience Devils in the detail With experience we make empirical claims Know that any such can be dangerous especially in that they often times come with assumptions we didnt fully realize ie the details When not sure about something experiment When sure about something it does not hurt to also experiment Do the mental exercise but always be prepared to get your hands dirty and write the code Undeterminism One level of indirection Lixia a prominent researcher in Internet architecture and my advisor once brought up the idea that one level of indirection is a powerful tool in engineering Over time we saw many falling into this pattern DNS offers a level of indirection for addressing such that application level need not hard code IP addresses which would be bound to a specific physical interface Normalization denormalization in database schema design is a level of indirection when storing enumerables eg neighborhood in someones profile do we store them as plain text in the user table or an ID that can be cross referenced with a dedicated mapping to plain text A message broker is a level of indirection to handle producers coming and going have them both talk to the broker as opposed to directly to each other A pointer reference is one level of indirection with these the underlying object can now be mutated inplace or shared by many There is however no universal answer to the question should we or should we not apply a level of indirection Common sacrifices we make include efficiency due to needing to follow an extra link for gains such as decoupling different layers unifying different representations and not needing to have changes propagate through the codebase Things to consider include if this should be modeled as a multilayer situation if there is one source of truth that everything else adheres to etc Anecdotally another thing she brought up as an important tool in our engineering kit is randomization example being TCPs random initial sequence number for crash recovery Im sure venturing more unto the undeterministic will yield more realizations for such Prototype and iteration MVPs and convincing Beyond coding Testability Maintenance Operational Documentation Communication Work ethic Advocation Seeing it through Routine and habits"},{"title":"\"July 2019\"","href":"/posts/july-2019","content":"July 28 2019 Hudson River Trading Library Floor 58 4 World Trade Center This week saw three core developers leaving the company due to performance reasons I do not know the detailed circumstances of each yet it always seems demoralizing to see folks being asked to leave be it a recent hire of two seasons or a seasoned hand of several years Back at school or in my yearandhalf at Bloomberg the thought of being forced onto something seemed so distinct that only now has it begun to dawn on me what that means for someone international and perhaps worse for someone who has not experienced a serious downturn This gets me pondering about my own work If nothing else being the latest member on a large team with degrading performance is worrying especially when my work is more individual and replaceable What is really the role of an engineer What are our core values What are the tradeoffs Which is the critical path from here for myself my work and the team and the company Engineers are problem solvers first and foremost What problem to solve Prioritize the one blocking the critical path One can bitch about code style violations poor integration testing nonexistent continuous integration all day but is that the weakest link Spotless code that does the wrong thing is still worse than a piece of crap which gets the job done When we complain about such perchance it is the one thing we are good at not the weakest link in our chain In other words we are twisting the problem to fit our strength not adapting to solve the problem Similarly one can boast skills in distributed system machine learning software architecture or simply writing good code but if they do not get you any closer to a solution what use is there of your training in such say profound art of dragonslaying Hence we should first realize the problem on the critical path the one that blocks progress Whose critical path Unfortunately in my line of work at its current stage that of my employers I am paid as an individual contributor to make the companys problems go away Great if they line up with my interest and priorities if not in the long run we should reconcile the two either leave for something different or bend either towards a blend of boths A mismatch here would be a common cause for stagnation in ones growth The reality in locating the weakest link however is often times murky as what one faces is always unique A true visionary may know the problem better than the client he serves with tact and resolve he would guide others into his view Yet being too stubborn in his pursuit he could become trapped in fixing any problem on a solution he knows well To navigate such ground the most useful seems to be to observe communicate and reflect Gather the facts apply our expertise and make a judgment call Outside work the same would apply only that we ourselves would be in control of the critical path what do we want to pursue where do we want to improve and why The balancing act The solution almost always is a compromise a tradeoff between multiple aspects or conflicting goals It is so between time vs space in an algorithm bias vs variance in statistics and noise reduction vs details when post processing a photo As is with most choices a small company vs a big one moving fast vs architecting something well having an abstraction that covers all vs individual solutions tailored to each Or personal traits being cautious or daring lenient or aggressive reaching out for help or diving deep oneself Know the problem know your tradeoff make a judgment call and know what you are sacrificing Know that the problem and our experience both evolve Evaluate your situation and reflect know when to dive deep and exploit also when to take a step back go wide and explore Values In finding the right balance we saw common traits shared by good problem solvers Face your worst fears Seek and embrance change Try Take risks No excuses Persistence Take a break Daily quarterly yearly Clear your mind Appreciate the problem appreciate others Learn and grow HRT has a large pool of talented folks It is perhaps time to acknowledge no matter how hard I try I will not be as good as someone else at things I care about Yet someone better in strategy may be worse in tactics another better in both may be worse in execution Find the right blend for yourself and the ground for you to shine Ebb and flow No matter how hard we try we fall into ruts Know where we have come from to gauge where we want to be Looking back the flows include first year of undergrad when we did well in school and spent the summer in memorable activities summer of 2013 in the US and the following year at remap during which we dived hard to understand NDN from the 2nd quarter at grad school till the first half year at Bloomberg when we established good rituals and laid the foundation for the next steps the last quarter at Bloomberg till the first quarter at HRT when we saw our skills sharpened acompanied by good fortunes in seeking a change The ebbs followed each flow where sophomore and junior year during which we saw excessive amount of time spent on games subsequent years at remap when learning slowed down to a slog and our vision of future became clouded after first performance evaluation at Bloomberg when our effort seemed misguided and the growth aspect seemed hindered after the initial thrill at HRT expires when job security started to become a concern Disatisfaction and lack of confidence in our skills started to rise Some among pessimism jealousy cynicism procrastination and a disinterest in everything typically accompany an ebb whereas confidence creativity and a stream of projects and hobbies usually come with each flow Inspecting our recent predominant mindset gives a good clue of the state we are in Understand its part of the natural cycle but seek to overcome it Reflect on the problems our approach and tradeoffs adhere to our guiding values Appreciate learn and grow Wake up with one problem we want to work on one ritual we want to strengthen today Go to bed satisfied knowing weve made good progress"},{"title":"\"Memory and bigtable\"","href":"/posts/memory-and-bigtable","content":"October 28 2019 Trump Plaza Jersey City A recent conversation drew me to realize the surprising similarity between the way our memory works and that of a log structured merge tree In my pseudoscientific language idea credit Sophia our shortterm memory would be like a memtable longterm memory would be like an SSTable sleep would be the process of committing a memtable to an ondisk SSTable it would also be the process where compaction happens our memory is keyed on some vague and flexible index and some operations seem incredibly faulty in that indexes clash and the values become intertwined in unpredictable ways the query process would be similar in that memtable is queried first and the latest SSTable the next latest etc memories like memtables and SSTables are appendonly in that we cannot unsee something already seen They are also immutable in regular flows not including the random faults of the mind once committed we may even also keep a Bloomfilter like structure as an optimization to the query process to be able to quickly answer something with that does not ring a bell at all the notes we keep are like logs in such a system to recover from eg a crash before commit from memtable wed need additional information like our physical notes other peoples memory of the same event can also serve as backup mechanism redundancy there definitely is also a layer of LRU cache that is associated to our shortterm memory too Like keeping hot content at the tip of tongue whats a good way of formalizing this Takeaways Sleep is important Consciously schedule improve review may help the compaction process We can probably create some interactive experience to model this process and bring in the idea of reference counting where something is forever lost in our memory history when nothing remembers it Or maybe this would be an excellent demonstration of a person inexperienced in one field trying to project his misguided thoughts onto another field where he thought hed be more knowledgeable in what bias is this called"},{"title":"\"Random realizations, quotes from conversations with friends\"","href":"/posts/random-realizations","content":"Although we are young and the cost of making suboptimal choices is relatively small but with each choice there still is a cost Now that I think of it miss the chance to go to Tsinghua CMU Google once we probably miss it for life The impact magnifies over time leading one down different paths in lifemaybe diverging farther from the goal we set out to achieve if there ever were one At some point one settles down becomes satisfied and loses the mettle When that day comes and I think back I just want to reassure myself I did everything I could and worked hard for everything I believed right At a game as well as at other things a big difference is whether there is a purpose in mind Today Ive a goal to improve my skills What do I want to learn and internalize today How do I achieve that Did I manage to achieve it There were times in games when I thought why dont you noobs get good then I realized its just different life priorities Some play to have fun others play to win Im in the middle of figuring out why I play and along that process we found a few things that might make a difference Now is just as good a time as any to do those After a while the absurd becomes the norm hourlong build times nonexistent coding convention hideous undefined behaviors floating around dependency loops and the like The silent desperation takes over and one starts to think it must be the same everywhere else and not worth the trouble to seek changes This too unfortunately is what comes with experience Zoom in zoom out dont lose sight of what we set out to achieve and embrace changes On how we do things and more importantly on how I perceive them and take them on With travel comes change habits uprooted and reestablished We may fear losing the key initiatives inspirations insights we just had or simply fear the unknown It is at such moments we should reiterate the core values we believed in what we set out to achieve stay confident in who we are and seek opportunities within the changes The difference often comes from that final push one that goes just beyond the critical mass and tips the balance in your favor One can give up thinking the tasks all but impossible or complain about the inability of others Yet know that more often than not failure comes from ones own resolve falling short or ones inexperience in seeing the merits and contributions of others Much like the timevalue of money using which we discount future cashflows to derive a present value there is a timevalue of knowledge skills achievements metaphysically all assets as well Different branches of knowledge have instrinsically different discount curves mapped to a multidimensional space of evaluation criteria The key would then lie in identifying the critical path and giving it all you got now You should know when to stop And when you are about to stop push one step further You never know if just one step would make all the difference if you dont take it The biggest asset of a company or perhaps any human construct is its people Make sure they are treated as such and make sure they are given the right environment and resources to realize their potentials Looking at my documents since 2013 there are at least three dozen named notes thoughts realizations plans and the like Most of them are scratches mind dumps and early studies that never came to fruition They struck me with the realization that almost none of my ideas now is completely new They all trace back to somewhere although the source may seem long forgotten Work on your bookkeeping taxonomy and consolidate often Learn like you will not be able to tomorrow That may actually happen Do your best to learn from them rather than be intimidated by them Reach out Ask Praise Appreciate Voice your thoughts Right wrong good bad its all a matter of perspective Keep an open mind Know and appreciate that your perspective may not be one shared by others Question and improve that of your own realize and appreciate those of others Maybe its this particular feeling of having ones mind inspired that drew me close as it longed for food for thought It almost reminds me of an old friend and the way she used to kindle ideas from these ashes Come to think of it perhaps I was naive and biased while she was right all along In hindsight perhaps her decisions were for the best and delivered in a very tactful manner while my thoughts and feelings misguided what I dont do well in convey your thoughts in a confident and clear way Some fear seems to have been holding us back I would think Im not a cynic not an inconsiderate person not overly vain and not terrible at certain things are those merely blissful illusions that the mind chose to weave for me When to settle for a compromise and when to aim only for the very best how we balance Learn something new different every 30 60 90 360 days Improve in certain ways Change forming habits a conscious influence and a dedicated effort This mind seems to have lost some of its most treasured liveliness creativity imagination daringness humor could one blame it on the burden of real life Did things really take a downturn since this job What exactly is this burden Immigration Work Future Desire for a breakthrough Routine Is this something we should actively work to thwart Our perspectives are often shaped by the limited knowledge and experience we have We take what we are used to for granted and any development longer than the lifespan or the radius of a human being can be hard for the particular being to comprehend Realize this and keep an open mind Sapiens Languages First try to make progress then seek breakthroughs Are you running away from your problems What better time than now to face and overcome them Much like a civlization an individual goes through periods of rise and fall ebb and flow Some started out brilliant at school early on in their career but later see their talents and potentials buried in the daily routine shackled by tradition Keep a mindset of change and growth and enjoy the process Probablistic statistical The unstructured life is not worth living for some Things we are systematically bad at De javu and is math useful is math olympiad useful Is skepticism trying to take the easy way out"},{"title":"\"Thanksgiving 2018\"","href":"/posts/thanksgiving-2018","content":"11202018 It used to bewilder me why in China we dont seem to have a holiday dedicated to appreciating the God and those around us Perhaps we never celebrated a harvest or the arrival of new immigrants like the first Puritans did 400 years ago One so vital upon which the very survival of the colonies is contingent Ive not been one to express gratitude verbally or much to my distress not many other positive feelings or compliments either for that matter Yet I do find myself constantly reflecting on the people and events of the past Usually no matter if those brought joy anger or pain at the moment I experienced them by the time of the reflection I feel gratitude towards the much treasured memories we shared or the bitter lessons they taught It is almost once again this time of the year and in typing these down I would hope to make up for my lack of words No particular order would do these justice perhaps the cliche alphabetical one is but the best I can do To Adam and Jochen for their excellent advice on career and on what I could have done better To Buyu for putting up with my nonchalance and ambiguity which I am now ashamed of To Chongshan Dongjie Zhongxia and Jian for all the fun in our first reunion in four years And dragging me to a net cafe not something Im particularly proud of but another first nonetheless To Fei Li and Sergii for their willingness to help and share their knowledge and the time they generously invested in me To Janani for always being encouraging and comforting to talk to Often times I dont see myself share this much with friends A change for the better I suppose To Jiajun and Josh for sharing their knowledge and motivating each other on the similar path we undertook To Jian Ken and others of Jump for the many Friday nights we spent discussing papers technologies of mutual interest Perhaps one of those was my finest moment this year To Joseph David Erin Kevin Kai Kim and many others at HRT for the fun interviews and events Also for inspiring me to keep improving to remain humble and not get satisfied with my few achievements To Joseph for being an outstanding conversationist and lunch pal To Nikhil and Shawn for their encouragement and support on my decision to leave earlier A shame the way it turned out To Ning for the gifts and few interactions in a foreign office In the past year those days if not now were perhaps the ones during which I was most troubled To Nzo for always being there to bounce ideas with Even those evil unethical and silly ones Perhaps not many are lucky enough to share such an acquaintance To Ponjo for being a genuine friend someone to share happiness experience or simply programming techniques with To Sam Eduardo Rayna Jason and others who bombarded LinkedIn Inbox for reaching out and reminding me how much more there is to explore out there To Setty for the day hikes we went on the trips to Chicago and CDMX the arguments we had and the many sentiments we shared From my disappointment in Februrary the anxiety in April and May the exhilaration in July till September and the deep concerns now To Sophia for helping me realize how much I should treasure the opportunities now and wanting to work harder to be worthy of what luck has graciously gifted me with To Yao for keeping my ego in check at work To those I met in my travels this and the last year for making me feel at home in foreign lands To my parents always And to the many unnamed for your kindness patience support or simply your smile at a stranger in distress I was treated by this world with kindness for that I constantly remind myself to pay it forward I find myself again at a crossroads perhaps making questionable decisions Even if the path may turn out to be a dead end work hard learn stay humble and appreciate"},{"title":"\"The first principles\"","href":"/posts/the-first-principles","content":"There seems layers of principles behind many arts in life The Platonic school would differentiate facts and the truth where a fact can be a drawn triangle is rectilinear and truth being any triangle demonstrating a2 b2 c2 is rectilinear This resonates with our observations on generalization and specialization The Aristotelian program its four modalities the final cause and rationality defined by being able to perceive generality also suggests there is what is it for in every art and technique ie the first principle Memory of the specialized fades but a grasp on the first principle generalized from the individual instances usually holds firm The first principle of photography revolves around the strength or peculiarity of the subject The elements and their arrangement your lighting depth of field and shutter speed as well as editing decisions all serve to bring out the strengths of the subject to guide the audience to appreciate such as you had and to not distract From this principle specialized guidelines become intuitive Eg casting hard lighting to bring out a threedimensional shape applying monochrome when colors will distract seeking leading lines and frame and posing your models back or waist in an Sshape with a gap between arms and waistline The first principle of linguistic seems to be the change Some innate complexity plus the natural dissolution shifts and variations on top of which geopolitical influence diglossia pidgins and creoles The first principle of software architecture seems the design of reasonable abstractions and the tradeoff that ensues Single responsibility openclosed Liskov substitution interface segregation and dependency inversion all describe something about the ideal abstractions or the technique to maximize its benefits on your architecture The first principle of algorithmic optimization revolves asymptotic complexity and memory access pattern Taking one step back the first principle of performance optimization could be algorithmic or systematic Ultimately the first principle of problem solving seems the dynamic to identify and overcome obstacles on the evolve critical path With the why the end goal in mind tracing back to identify what needs to be done and coming back with how When generalizing to the first principle especially the principles of highlevel abstract topics like problem solving the claim almost seems apparent and of not much practical implication This perhaps echos with Platos observation in Meno that truth is not something taught rather something we inherently know but sometimes dormant and awakened via dialectical discourse The realization seems one such process of the truth or the first principle being awakened How are principles formulated Does dialectical methods apply to natural sciences Does it not carry practical implications What about the principle of financial investment Designing a distributed system Living a healthy and productive life Interacting with people Effective communication Music composition Poetry Decision making Is the principle different from the key features to uphold such that the art is declared to be performed in an agreeable way Or is the principle a key universal observation about an objective phenomenon from which effects of such phenomenon or those like it can be deduced Are these different Does the difference imply anything"},{"title":"\"Turkey 2019\"","href":"/posts/turkey-2019","content":" Islam A better place Hi phesiz ki Antalya dnyann en gzel yeridir Without a doubt Antalya is the most beautiful place in the world Having spent an entire day traversing from citys numerous beaches to vast Anatolian hills I can see Atatrks exclamation being quite reasonable Seeing locals young and old swimming in tranquil waters of the Mediterranean and basking on a rocky shore on a warm September day was not something I had expected when planning the trip nor was the delicate old city or the panoramic escalators leading to its natural harbor Yet the city did leave a brush of shadows upon my mind Coming back from a night photography session at the Duden falls my host Sahar Sara asked me to guess where she was from I would not have suspected her not being a native having experienced how diverse Turkish people can look When she brought up Iran something about her looks did click elegant and shapely very much like the few Persian ladies I met back in New York What do you think its like to migrate from 40 degrees hell to 20 degrees I hope to see you again in a better place someday she replied I thought to myself perhaps a better place I know not nor do I find it likely that our paths would ever cross again I offered to help in any way I can she smiled gently and asked only for my best wishes With that we parted and my last stop awaits Hers as well as those of many others would be an ordeal I cannot presume to comprehend I ask of myself to cherish what I was given to work hard to be worthy of it to listen and not judge to treat the world with kindness and help those in need Atatrk Varner"}]