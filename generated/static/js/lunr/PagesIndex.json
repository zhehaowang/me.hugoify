[{"title":"Zhehao Wang","href":"/about","content":"Hi there My name is Zhehao My fulltime job is to develop software that trade stocks in a company based in New York City Ive a passion for everything engineering and love to ponder about knowledge conduct cultures and life in general duh I recently took an interest in photography and traveling like everyone else does I spent my spare time learning about philosophy and linguistics lately My resolution for 2020 is to get enough sleep"},{"title":"Gallery","href":"/gallery/gallery","content":""},{"title":"\"Classical mythology\"","href":"/notes/classical-mythology","content":"Hesiod Theogony Work and Days Chaos Gaia Tartarus Eros Erebus Hemera Nyx Uranus Gaia Cronus Rhea Zeus Poseidon Hades Hestia Hera and Demeter Hestia is rarely mentioned in literature almost negligible in mythology but presumably very important in religion Zeus Hera Ares whom everyone hates Enyo Hebe Eileithyia childbirth Eris discord Hephaestus Hera by herself Zeus Metis wisdom Athena from Zeuss head Zeus swallows Metis who gave birth to Athena inside Zeus Metiss son is prophecied to overthrow Zeus One of the rare cases where gods mortals cheat fate Greeks believed thoughts wisdom comes from the torso and Metis gave counsel to Zeus from inside his belly In mens meeting with gods at Mecone to decide sacrifice Prometheus tried to trick Zeus into picking the pile of bones covered by meat and fat as sacrifice and leaving the pile of meat covered by bones and unedible parts to human Zeus knew and decided to punish Prometheus by punishing men and took fire from them Prometheus later stole fire and gave it back to men for which act he was chained to a pillar and have his liver grown everyday to be eaten by an eagle Zeus punished men too for Prometheus stealing fire by giving men the first woman named Pandora in Work and Days Heracles rescued Prometheus from the chain Greek gods have little care for human beings While these are from Theogony many anecdotes stories we hear today are actually from Ovids metamorphoses eg Apollo and the nymph Daphne the story of laurel Phaethon son of Apollo asking to ride the chariot of the sun for a day and Narcissus falling in love in his own reflection in water Demeter Persephone the Eleusinian Mysteries seasons and Greek view of life and death Persephones abduction by Hades and splitting her time in the underworld vs mount Olympus resulted in the seasons where one third of the year crops dont grow As told by the Homeric hymn on Demeter Homeric in time and style but not by Homer during Demeters search for Persephone she visited Eleusis disguised as a crone and babysitted the queens son whom she intended to bring to immortality by annoiting and throwing into the fire The plan was foiled by the queen who happened upon the ritual angering Demeter and causing the death of the prince The Eleusinian Mysteries were a cult ritual dedicated to Demeter in which only the initiated were told of the rituals Our view of what the ritual is about preserved from those of the potentially biased early Christian authors Presumably symbolisms of life and death While the Greek view of afterlife is in general pretty grim most men became shadows images in the grim world of tartarus except extraordinary souls or terrible sinners eg tantalus who intended to trick the gods into eating flesh by serving his own son whose shoulder only Demeter ate mistakenly resulting in an ivory shoulder when the gods bring the son back the Eleusinian Mysteries might have offered a brighter view Note that the general grim view differs from traditional belief of Christianity or Islam in that the latter believes the afterlife the eternal is the important part and the deeds of this life would affect that of the afterlife while the former places its emphasis on this life and rarely consequences of ones deeds in this life Orphism is another cult dealing with life and death The teaching of the mortal Orpheus who surprisingly some sources claim is the child of Apollo and a muse while in the underworld trying to bring back his dead wife Eurydice Persuaded by his superb music Hades and Persephone gave their consent on the condition that Orpheus does not look back when leading Eurydice out of the underworld which he failed to do Reincarnation might have been a central concept in Orphism which took a view like that of Buddhism that reincarnation cycleaftercycle is something one wants to escape from and one does it by saying certain things to certain gods in the underworld Apollo and Artemis Children of Zeus and Leto who is of little mythological importance otherwise Apollo him of medicine and plague youth and sudden death struck by the arrow of Apollo all the creative arts administers over the Muses rationality prophesy delegated by Zeus and later on associated with the Sun Depicted as a male in 20s with bow and quiver showing that of typical youthful manly beauty Pythia prietess of the Oracle of Delphi delivers the words of Apollo to those seeking prophesy which are most of the time ambiguous in nature eg Croesus of Lydia was told if he attacks the Persians he would destroy a great empire that of his own Motto at the Oracle of Delphi know yourself nothing in excess Know yourself in the sense of knowing your limitations and not offend the god Queen of Thebes suggesting she should be worshipped for having 14 children while Leto only has two and all her children were struck dead by Apollo and Artemis Artemis the huntress goddess of animals the untamed women before marriage and the young Hestia Artemis and Athena are chaste goddesses of Greek myths in Artemiss case its potentially related with her representation of the untamed sex is viewed as domination and one Greek way of saying wife came from the word tamed Her association with hunting is also related with protection of the young which hunters often times do for sustainability Most famous place of worship in Turkey temple at Ephesus Hermes and Dionysus Hermes God of exchange of commerce trading thieves beggars boundaries the messenger and deliverers of souls to the underworld Hermes invented the lyre from tortoise shells and traded it with Apollo for a cattle he stole from Apollo in the first place The pillar herma is associated with Hermes herma represents boundaries those of a market eg and usually depicts a bearded god with an erect phallus The pillar is sacred and their mass destruction before Alcibiadess expedition to Syracuse was a serious matter that caused his investigation Dionysus of madness irrationality frenzy growth of wild plants and wine Male worshippers Satyrs human with beast features such as years horse tails and later on goat below the waist always in a frenzy female worshipers maenad human women who has the strength to tear wild animals apart Nietzsche find greek culture often times representing the contrast of Apollonian reason and Dionysian madness Dionysus is often associated with tragic and comedian traditions of Athens perhaps due to their origins in rituals and actors impersonating someone else bears some similarity to Dionysian irrationality Euripidess Bacchae describes birth of Dionysus Zeus in disguise bedded mortal woman Semele whose nanny Hera disguised as out of jealousy and suggested Semele verify her partner is indeed a god not a pretender by asking him to swear on the river styx to fulfill one wish of Semeles which Zeus did Semele asks him to reveal his true glory which incinerated her Zeus took her embryo Dionysus and nurtured him in his thigh from where Dionysus is born He is thus sometimes referred to as the twice born The Theban siblings of Semele refused to acknowledge Dionysuss divine status for this slight Dionysus drove the women of Thebes mad and tore apart the siblings Aphrodite of sexual passion Quite a different concept as modern perception of love in its being transient Origined from Uranuss genitals in Theogony or child of Zeus and Dione in Iliad Aeneas son of Anchises and Aphrodite Zeus giving Aphrodite a taste of her own medicine Gods men and animals are said to be unable to resist Aphrodites power except the three chaste goddesses Homeric hymn to Aphrodite tells the story of Anchises and Aphrodite who lied to the former of being a mortal woman Aeneas being the hero of Aeneid and claimed ancestor of Romans No good ever comes of the union between men and goddesses Examples including Adonis killed for his relationship with Aphrodite and Tithonus and Eos the goddess of dawn Zeus agreed in Eoss plead to make Tithonus immortal but she forgot to ask for youth Tithonus ages but never dies and eventually became just a babbling voice that Eos shut away Sexual relationship between an older man and an adolescent young man is often seen as ok in greek myths but not other forms of homosexuality Maybe it does not make sense to question the Greeks whether they believe in their gods who are personifications of natural forces To ask if an ancient Greek believes in Aphrodite would be like asking if he acknowledges the presence of sexual passion Belief in gods could be very much a monotheistic thing and the perspective is usually quite different if someone outside a cultural group were to inspect that groups myths Minoan and Mycenaean civilzations Minoan civilization prospered on Crete in about 2000BC its literature linear A and linear B were left the latter was not yet deciphered Mycenaean civilization prospered about 1600 to 1100BC after which a Greek dark age followed Mycenaean seemed to have lost their ability to create intricate artwork and magnificent cities Trojan war and the heroic age was about this time Homers work whose description of ancient Greek cities were verified by modern day archaeologists to match excavated Mycenaean cities may very well be oral traditions that traces back to historic events during that time The Five Ages Works and Days described the human race going through the golden age rein of Cronus no strife or turmoil long lives silver age Zeus Long childhood 100 years lifespan destroyed by Zeus for impiety bronze age Created by Zeus from ash tree spear shaft War like and violent ended with Deucalions flood heroic age an improvement compared with bronze age heroes and the Trojan War and iron age Hesiods time theres no help against evil and eventually the grim vision that babies will be born greyhaired due to worries Story of Theseus Unifier of Attica The test and quest pattern Dual fathership Aegis king of Athens and Poseidon mated with the same woman on the same night Aegis told Theseuss mother about a pair of sandals and a sword under a boulder and that if she bores a son when the son is old enough to lift the boulder collect the items and head to Athens On his way he slew Procrustes who insists travelers stay in his bed and that they must fit too tall lop off their legs too short stretch them Aegiss then wife Medea tried to poison Theseus but the plan was foiled by Theseus drawing his sword to cut the meat and Aegis recognizing the sword common theme reunion and the last minute Medea escaped Theseus then volunteered as tribute to Minos to feed the minotaur whom he slew with the help of princess Ariadne common theme help of a young woman The latter provided him with a ball of threads the clue of Ariadne to guide him out of the labyrinth Theseus took her with him back on the ship to Athens but apparently forgot about her on an island in the way Dionysus later rescued Ariadne married her and made her a goddess This is quite unusual and prompts assumptions that Ariadne was a Cretan goddess Theseus also forgot about raising white sails as promised causing Aegiss suicide After becoming king he tried to marry three times First being abducting the young Helen then journeying with Pirithous both conspired to marry daughters of Zeus Theseus picked Helen and Pirithous picked Persephone to the underworld to abduct Persephone but tricked by Hades and got stuck on stone chairs only to be rescued later by Heracles common theme in heros justification running into Heracles Pirithous was left in the underworld and Theseus returned to Athens to find Helen rescued by his brothers Theseus then abducted Hippolyta queen of the Amazons common theme in heroes encounter who died giving birth to Hippolytus Theseus defeated Amazons trying to rescue their queen In Euripidess play Hippolytus Theseus then marries Phaedra sister of Ariadne and daughter of Minos Phaedra wanted Hippolytus was refused due to the latters oath of chastity and hanged herself leaving a letter accusing Hippolytus of raping her Theseus exiled Hippolytus and the latter died after which Theseus learns of Phaedras lies The timeline of Theseuss story seems hopelessly confused Minos lived three generations before the Trojan War Heracles one generation before yet Theseus encountered both and abducted Helen Presumably Theseus held such importance in Athens that they needed to connect him with every major plot in classical myths Yet his origins may correspond with historical events archaeologists uncovered a ritual sport handling bulls and Athens very well could be paying tribute to Minoan civilization in Theseuss time Minotaur is the offspring of Minoss wife queen of Crete with a bull Minos promised to offer the bull to Poseidon but instead offered a lesser animal Poseidon enlisted the aid of Aphrodite to make the queen desire the bull and the best artisan in Crete crafted her a disguise as a cow to mate with the bull and conceived Minotaur The labyrinth in which Minotaurs locked is designed by the same artisan Theseuss story represents that of a rite of adulthood of a male Heracles Pan Hellenic hero and god Son of Zeus and a human mother Alcmene a descendant of Perseus Life of hardship because of jealousy of Hera Birth Descendant of Perseus Zeuss decree to be king of Mycenae trick of Hera strangling the snakes A man of excessive appetite sexuality impregnated 50 daughters of a king in one night many later Greek kings traces back to this eg king of Sparta and rage Depicted naked with his lion skin and club Killing his wife Megara and his children in his rage Told by the Pythia to accomplish 12 labors in service to his cousin the king of Mycenae to redeem himself and become immortal The first 6 in Peloponnese related with animals Nemean lion nineheaded hydra bringing back Artemiss hind Erymanthian Boar Augean stables Stymphalian Birds The next 3 farther away maneating mares of Thrace bull of Crete Hippolytas girdle belt The last 3 in the far west associated with death and immortality driving back cattle of monster Geryon golden apples of the goddesses daughters of the night guarded by a dragon bringing back Cerberos the guarddog of the underworld Marries Deianira afterwards killed Nessus a centaur who attempted to rape Deianira with arrow poisoned by the blood of the hydra Centaur persuades Deianira to keep his blood as a love potion when Heracles loses interest in her someday in the future Deianira did and soaked a cloak in the centaurs blood Heracles did lose interest in her and Deianira brought out the cloak to Heracles which he wore which tore his skins and exposed his bones In great agony he threw himself upon a funeral pire and died His psyche then joined and Olympians and he married Hebe as a god A man of great contrasts in his tragic life and ascension as a god his great courage and rage etc Trojan War is the most important episode in classical mythology and marks the end of heroic age We have heroic accounts of sons of those who fought in Trojan War but not grandchildren the connections heroes had with gods had ended and the iron age had begun Greeks believed the fall of Troy happened in 1184BC at the same time the Mycenaean civilization waned Over time the myths developed into a complex and timelineconfused system Including Wedding of Thetis a river goddess whose son is prophesied to be greater than the father for which Zeus married her off to a human Eris of discord not being invited and feeling offended tossed the golden apple for which Hera Athena and Aphrodite vied and Zeus left Paris to judge The family of Agamemnon and Menelaus Atreus and the formers sacrifice of his daughter The sack of Troy the fate of Ajax Cassandra Priam Agamemnon Menelaus and Troy after the war Homer did not mention these but mentioned Aeneas and his prophesied escaped from Troy which Virgil took note and claimed he as an ancestor of Romans Tragic house of Atreus One of kinslaying and incest Tantalus Pelops served by his father to the gods and revived tricks fatherinlaw in chariot race to win his daughters hand in marriage causing the death of the fatherinlaw in question Atreus and Thyestes wife of the former betrayed him for the latter latter cheated in deciding kingship of Mycenae between the two former took vengeance by exiling him and later inviting him back to a feast during which he was served his children Agamemnon sacrificed his own daughter Menelaus and Aegisthus fathered by Thyestes on his own daughter kills Menelaus was killed by Oreste Moral sins are passed on from fathers to sons Story of Oreste is best told by Aeschyluss Oresteia the only full surviving tragic trilogy of ancient Greece The third act is the judgement of Oreste for killing his mother who murdered his father with Apollo defending Furies persecuting 12 citizens of Athens in jury and Athena as judge Athena casted the vital vote to acquit Oreste Oresteia in particular is written in 5th Century BC during Athenss transition from being ruled by the court of Areopagus to classical democracy The play might have wanted to remind the people the importance that the court still serves in judicial system as opposed to city politics Format of Greek tragedy is usually a tragic trilogy followed by a comedy where the three episodes of the trilogy are connected via same events characters or the same theme Performed during the festival of Dionysus The creativity freedom an authors given may be very different from those of todays playwrights the theme story is somewhat fixed and well known but an author can add or twist particular details to serve his dramaturgy purposes Eg in Oresteia to justify matricide Clytemnestra as opposed to Aegisthus is described as the killer of Agamemnon Commonly depicted tragedy involves a moral dillema that a character faces eg Oreste needs to avenge his father and he also must not hurt his mother but his mother murdered his father Or Agamemnon needs to lead his army across the sea to do Menelaus and his house justice but to do that he must sacrifice his daughter to appease Artemis Trying to evade fate but only falling in its traps instead was another common theme Eg Oedipus fated to kill his father and marry his mother Modern interpretation tends to focus on the struggle between fate and free will in Oedipuss story or Freuds farfetched theory of the same name though the concept of free will might have been foreign to ancient Greeks whose language did not have a word for it Amazons are a recurring encounter for heroes and the counter usually ends with heroes taming them eg Heracles Theseus putting women in their proper place as the patriarchal Greek society sees it Medea is not an Amazon but another one of the dangerous immoral yet attractive foreign woman figure described as a witch from Colchis married first to Jason tore her brother to bits to slow down his fathers pursuit of them for the stolen golden fleece and later poisons Jasons and her own sons to hurt him then to Aegis Intergenerational tension and anxiety about women sexuality are recurring themes in classical myths in which males of the heavily patriarchal society is anxious about having heirs that are their own Ancient Greeks might have believed that babies are not blood relatives of mothers father does the work by planting seeds in the mother like how plants were grown Romans largely took Greek myths for their own and rhetoric philosophy or culture in general Julius Caesar traces his lineage to Aeneis thus to Aphrodite Virgil would claim Aeneis is the ancestor of the Roman people without contradicting the myth that Romulus founded Rome the city 400 years after the fall of Troy Livy recounts the story of Romulus Vergil Horace Livy and Ovid are authors during Augustuss reign which saw a golden age for Roman literature Ovid of these did not receive patronage of the emperor due to his books on practical techniques of seduction being in conflict with the emperors agenda to reestablish traditional morals and religion punish extra marrital affairs and encourage fertility Shakespeares work is influenced by Ovids Metamorphosis often times quotes and allusions in the former traces to stories in Metamorphosis Ovid named his book Metamorphosis due to each story being related with turning from one form to another though many stories barely reflect that"},{"title":"\"Clean architecture\"","href":"/notes/clean-architecture","content":" Clean architecture Part 1 Goal of architecture To minimize human resources required to build and maintain the required system A good indication of a badly designed system tremendous cost of each line of code and low productivity of engineer over time And yet despite all their heroics overtime and dedication they simply arent getting much of anything done anymore All their effort has been diverted away from features and is now consumed with managing the mess Their job such as it is has changed into moving the mess from one place to the next and the next and the next so that they can add one more meager little feature And so the developers never switch modes They cant go back and clean things up because theyve got to get the next feature done and the next and the next and the next And so the mess builds and productivity continues its asymptotic approach toward zero The only way to go fast is to go well The misconception of going faster by spending less time in design The bigger lie that developers buy into is the notion that writing messy code makes them go fast in the short term and just slows them down in the long term Developers who accept this lie exhibit the hares overconfidence in their ability to switch modes from making messes to cleaning up messes sometime in the future but they also make a simple error of fact The fact is that making messes is always slower than staying clean no matter which time scale you are using And making matters worse Their overconfidence will drive the redesign into the same mess as the original project Two values of a program behavior and architecture Behavior programmers are hired to make machines behave in a way that makes or saves money for the stakeholders To fulfill its purpose software must be softthat is it must be easy to change When the stakeholders change their minds about a feature that change should be simple and easy to make The difficulty in making such a change should be proportional only to the scope of the change and not to the shape of the change From the stakeholders point of view they are simply providing a stream of changes of roughly similar scope From the developers point of view the stakeholders are giving them a stream of jigsaw puzzle pieces that they must fit into a puzzle of everincreasing complexity Each new request is harder to fit than the last because the shape of the system does not match the shape of the request The problem of course is the architecture of the system The more this architecture prefers one shape over another the more likely new features will be harder and harder to fit into that structure Therefore architectures should be as shape agnostic are practical Argument architecture is the more important value Consider the two extremes something that works but cannot be changed is useless facing changing requirements while something that doesnt work but can be easily changed can be changed You may not find this argument convincing After all theres no such thing as a program that is impossible to change However there are systems that are practically impossible to change because the cost of change exceeds the benefit of change Many systems reach that point in some of their features or configurations I have two kinds of problems the urgent and the important The urgent are not important and the important are never urgent Eisenhower The dilemma for software developers is that business managers are not equipped to evaluate the importance of architecture Thats what software developers were hired to do Therefore it is the responsibility of the software development team to assert the importance of architecture over the urgency of features Fulfilling this responsibility means wading into a fightor perhaps a better word is struggle Frankly thats always the way these things are done The development team has to struggle for what they believe to be best for the company and so do the management team and the marketing team and the sales team and the operations team Its always a struggle Just remember If architecture comes last then the system will become ever more costly to develop and eventually change will become practically impossible for part or all of the system If that is allowed to happen it means the software development team did not fight hard enough for what they knew was necessary Part 2 Paradigms of programming Structured programming impose discipline on direct transfer of control OOP impose discipline on indirect transfer of control Functional impose discipline upon assignment Each of these paradigms takes something away from us Note how well the three aligns with the big concerns of architecture function separation of components data management Structured programming Goto not in the context of if while makes a program hard to decompose into smaller units making proof by divide and conquer hard Structured programming allows modules to be recursively decomposed into provable units Falsifiable testable Tests can prove a program incorrect but not correct Software is like scientific laws we show correctness by failing to prove incorrectness despite our best efforts OOP Whats really the point Encapsulation Cs forward declaration where data members of a struct need not be declared in header can achieve hiding members and implementation from clients C breaks this kind of encapsulation Inheritance In C if one struct A is a pure superset of another B A can masquerade as B Cstyle cast of A to B Such trickery is how C implements single inheritance as well Polymorphism C can achieve polymorphism as well Eg the definition of STDIN how does a console getchar call know which device is STDIN Unix require that every IO device driver provide five standard functions with same function signatures open close read write seek FILE is a struct with members that are pointers to the above function signatures STDIN can then be pointer to FILE who was populated with consoles implementation of the above five functions This approach is not that different from Cs vtable polymorphism While polymorphism is achievable in C C makes it more convenient and safer This plugin architecture of device drivers makes programs device independent Dependency inversion the source code dependency of an inheritance relationship points in the opposite direction compared to the flow of control where a highlevel function needs to know the source of a lowerlevel function in order to call it This means any source code dependency no matter where it is can be inverted And one is not constrained to organizing dependencies aligned to the flow of control With the dependencies organized via an interface and plugged in one also achieves independent deployability and independent developability Having compile dependency being opposite to runtime dependency is dependency inversion What is OO to an architect OO is the ability through the use of polymorphism to gain absolute control over every source code dependency in the system It allows the architect to create a plugin architecture in which modules that contain highlevel policies are independent of modules that contain lowlevel details The lowlevel details are relegated to plugin modules that can be deployed and developed independently from the modules that contain highlevel policies Functional programming Immutability Variables in functional languages do not vary Why would immutability be a concern race conditions deadlocks concurrent update problems are all due to mutable variables Mutability in Clojure functional languages like Clojure can allow mutable variables but only mutated under very strict conditions that are enforced by swap which uses a traditional compare and swap algorithm Thus a wellstructured program can be modeled in a functional language by segregating its variables into mutable and immutable ones And itd be wise to push as much as possible into immutable area Event sourcing The limits of storage and processing power have been rapidly receding from view Nowadays it is common for processors to execute billions of instructions per second and to have billions of bytes of RAM The more memory we have and the faster our machines are the less we need mutable state As a simple example imagine a banking application that maintains the account balances of its customers It mutates those balances when deposit and withdrawal transactions are executed Now imagine that instead of storing the account balances we store only the transactions Whenever anyone wants to know the balance of an account we simply add up all the transactions for that account from the beginning of time This scheme requires no mutable variables If this still sounds absurd it might help if you remembered that this is precisely the way your source code control system works BigTable those that are based on log structured merge trees Change data capture and building derived views from log Kafka and logbased message brokers are all ideas along this line Part 3 The principle of SOLID for module mid level software design component in BDE terms Goal tolerate change easy to understand create basis of components that can be used in many software systems Not confined in OOP general enough for any paradigm that groups functions and data into general classes Single responsibility principle so that each module has only one reason to change being responsible only to one actor group of stakeholders Cohesion SRP Symptoms eg putting functions that support different parties in one class Resolution eg putting them in different classes and if required have them share the same underlying data Now more classes needs to be instantiated and kept track of Consider Facade pattern if needed Similarly common closure principle and at architecture level axis of change Open closed principle software system should allow change of its behavior by adding new code rather than changing existing code A software artifact should be open for extension but closed for modification Or the behavior of a software artifact ought to be extendible without having to modify that artifact Architecture separates functionality into a hierarchy of components no bidirectional dependencies crossing package group boundaries Higherlevel components are protected from the changes made to lowerlevel components depend on an interface not an implementation dependency inversion Liskov substituion principle build software from interchangeable components and have them conform to a contract so that they are substitutable We want this substitution property if for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T input type contravariant Remind yourself of this especially when you start to see a lot of special case handling in the code It might be that we have the wrong generalization Can Colombian quasiIL Chinese dimsum Italian tax rate be justified Interface segregation principle avoid depending on things you dont use This includes transitive dependencies consider using an interface for information hiding in that case In general it is harmful to depend on modules that contain more than you need Achieving flexibility in dynamically typed languages like Python may be different in this sense from say C Consider the current sfgs architecture Or by DI the same principle should hold highlevel policy should not depend on lowlevel details Dependency inversion principle highlevel policy should not depend on lowlevel details should be the other way round The most flexible systems are those in which source code dependencies refer only to abstractions not to concretions Reduce the volatility of interfaces Dont refer to volatile concrete classes The relationship between this and the usage of abstract factory Dont derive from volatile concrete classes Dont override concrete functions Dont mention the name of anything concrete and volatile Main will often times be a concrete component that relies on concrete components as ultimately one needs to specify which concrete implementation should be used Rethinking our question on TCPClient in client side business objects Say main creates a client side business logic object One should consider using abstract factory to abstract away a transport mechanism from client side business logic object assume this object needs to instantiate thus one of the differences between taking in an interface and an abstract factory other consideration being fully isolated unit tests acquire which might make transport layer client not such a great example a transport mechanism which should be transport mechanism independent So it looks like when you instantiate the client side business object in main give it a could be global concrete factory Think about architectural boundaries and dependency inheritance lines that cross them more in dependency rule Part 4 package group rules package a unit of release that retains the ability to be independently deployable and developable Early days of programming and nonrelocatable code linking loader external reference U symbols and external definitions definitions exposed to external callers T symbols Then linking and loading were split linking is a one time job after compilation and loading happens at runtime claim shared object is reexploring the idea of a linker loader Package cohesion which components go into which packages REP reuse release equivalence principle The granule of reuse is the granule of release the package must have some overarching theme or purpose that components in it all share and components grouped into a package should be releasable together weak rule CCP Gather into packages those components that change for the same reason and at the same times Separate into different packages those components that change at different times and for different reasons Its similar with Single Responsibility Principle in that the latter discusses what to put in one component and this discusses what to put in a package CRP common reuse principle Dont force users of a package to depend on things they dont need Classes and modules that tend to be reused together belong in the same package We want to make sure that the classes we put into a package are inseparable it is not possible for a client to depend on some and not on others Its similar with Interface Segregation Principle package level vs component level The first two rules are inclusive and third is exclusive and an architect strikes the right balance and expect the balance to change over time Package coupling the dependency between packages ADP acyclic dependencies principle Break down dependency cycles so that when one package changes the affected parties are only its clients and their change should not propagate back to this changed package and the release of each packge can go from bottom to up Cycles essentially make the packages one large piece and difficult to isolate packages and unit test Common ways to break down cycles include separating a smaller piece out that both packages depend on and applying DIP The logical design of the system can go topdown with whom the packages dependency design grows and evolves The weekly build similar as ADP also introduced to combat multiple contributors to the same project changing things others depend on and project being unable to build four days independent dev and one day integration not exactly robo cycle SDP stable dependency principle Depend in the direction of stability There are packages that are developed knowing theyd be volatile and subject to change Any volatile package should not be depended by a package that is difficult to change Stability does not necessarily mean frequent changes but rather the effort required to change the package size clarity complexity amount of clients eg as denoted by NumberOfDependencies NumberOfDependencies NumberOfClients and the lower the more stable the package is Typically abstract interfaces in statically typed languages should be very stable SAP stable abstraction principle A package should be as abstract as it is stable Stable abstract or consists of interfaces unstable concrete implementation This echos with OCP where stable interfaces are closed for modification but open to extention SAP SDP DIP for packages If something is widely depended on it better be abstract and do not change often With packages there can be a shade a grey partially abstract and stable a measure of abstractness can be number of interfaces divided by total number of classes in package Consider a plane of Stableness Abstractness per calculations above Packages grouping around 0 0 likely introduce pain since these are concrete and stable Some packages do fall into this zone eg a DB schema or a concrete utility such as String class which is concrete and highly depended on Packages grouping around 1 1 are likely useless abstractions that few uses The most desirable position for components is at the two ends of the diagonal aka the main sequence Thus a design can be analyzed for the overall distance to the main sequence The quantization of this metric though is arbitrary and imperfect Part 5 architecture Architecture What is a software architect He is a programmer first and foremost The purpose of an architecture is to facilitate development deployment operation and maintenance of the software system contained within The strategy behind this facilitation is to leave as many options open as possible for as long as possible Note that it is not architectures primary focus to make sure the system works correctly sure it helps but many existing systems work fine but have poor architecture for these systems the pain is in development maintenance and deployment Different team structures imply different architectural decisions On the one hand a small team of five developers can quite effectively work together to develop a monolithic system without welldefined components or interfaces In fact such a team would likely find the strictures of an architecture something of an impediment during the early days of development This is likely the reason why so many systems lack good architecture They were begun with none because the team was small and did not want the impediment of a superstructure On the other hand a system being developed by five different teams each of which includes seven developers cannot make progress unless the system is divided into welldefined components with reliably stable interfaces If no other factors are considered the architecture of that system will likely evolve into five componentsone for each team Such a componentperteam architecture is not likely to be the best architecture for deployment operation and maintenance of the system Nevertheless it is the architecture that a group of teams will gravitate toward if they are driven solely by development schedule Deployable to be effective a software system must be deployable ideally with one single action One potential downside of micro services For example in the early development of a system the developers may decide to use a microservice architecture They may find that this approach makes the system very easy to develop since the component boundaries are very firm and the interfaces relatively stable However when it comes time to deploy the system they may discover that the number of microservices has become daunting configuring the connections between them and the timing of their initiation may also turn out to be a huge source of errors Had the architects considered deployment issues early on they might have decided on fewer services a hybrid of services and inprocess components and a more integrated means of managing the interconnections Operation architecture tends to be a less dramatic effect as almost any operational difficulties can be resolved by throwing more hardware at the system Yet architecture should reveal operation elevate use cases features and the required behaviors of the system to first class entities that are visible landmarks for the developers Maintenance of all the aspects of a software system maintenance is the most costly whose primary costs come from spelunking cost of digging through existing software to decide the best place and strategy to add a new feature or repair a defect and risk changing existing behavior and introducing bugs Keep options open recall the two values of a software with the greater value being structure as thats what makes software soft it does so by leaving choices open Choices that are left open are details that dont matter All software systems can be decomposed into two major elements policy and details Policy embodies business rules and procedures which is where the true value of the system lives Details are things that are necessary to enable humans other systems and programmeres to communicate with the policy but that do not impacting the policy at all IO devices DBs web systems communication protocols etc The goal of the architect is to create a shape for the system that recognizes policy as the most essential element of the system while making the details irrelevant to that policy This allows decisions about those details to be delayed and deferred And the longer you wait to make those decisions the more information you have to make them properly and this leaves you open to trying different experiments flavors of details What if the decisions have already been made by someone else What if your company has made a commitment to a certain database or a certain web server or a certain framework A good architect pretends that the decision has not been made and shapes the system such that those decisions can still be deferred or changed for as long as possible A good architect maximizes the number of decisions not made Device independence detail vs policy we learnt from the early days Good architects carefully separate details from policy and then decouple the policy from the details so thoroughly that the policy has no knowledge of the details and does not depend on the details in any way Good architects design the policy so that decisions about the details can be delayed and deferred for as long as possible Independence A good architecture must support use cases in making them obvious and operation maintenance development deployment A good architecture makes the system easy to change in all the ways that it must change by leaving options open Conways law Any organization that designs a system will produce a design whose structure is a copy of the organizations communication structure A good architecture does not rely on dozens of little configuration scripts and property file tweaks It does not require manual creation of directories or files that must be arranged just so A good architecture helps the system to be immediately deployable after build In the real world the difficulty is that most of the time we dont know what all the use cases are nor do we know the operational constraints the team structure or the deployment requirements Worse even if we did know them they will inevitably change as the system moves through its life cycle Meanwhile some principles of architecture are relatively inexpensive to implement and can help balance those concerns even when you dont have a clear picture of the targets you have to hit Those principles help us partition our systems into wellisolated components that allow us to leave as many options open as possible for as long as possible Decoupling layers Consider the use cases an architect may not know all of them but knowing the intent of the system an architect can use Single Responsiblity Principle and Open Closed Principle to gather things that change for the same reason Some obviousness exists in identifying these things UI business rules and DB query language schema accessor etc usually change for different reasons thus keeping them independently evolvable is a good practice Business rules may serve different goals eg input validation and calculation of interest on an account These rules will change at different times for different reasons so they should be separated and kept independently changeable Decoupling use cases Use cases can be grouped as well based on when and why they change thus making use cases a natural way to divide the system While the components UI business rules DB are horizontal layers use cases are vertical slices through the horizontal layers Thus we divide the system both horizontally and vertically Advantage of decoupling use cases dividing vertically being each use case uses a different aspect of UI business rule DB and adding new cases wont affect existing ones The division by functions YASN SWPM OVME CDSW etc is an example of this vertical decoupling We do find awkwardness in this decoupling This could be a case where the general guideline applies but making the right decision is case by case and requires knowledge and observation of the system Decoupling mode eg into services The decoupling we did for layers use cases also helps with operations different parts of the system can run on different machines cater to different performance throughput requirement scale differently To take advantage of the operational benefit the decoupling must have the appropriate mode eg not depending on addressing in the same machine Many architects call such components services and architecture based on services is a serviceoriented architecture Modes can be decoupled at Source level changing one module does not force the recompilation of one that is independent from it Binary deployment level changing one module like changing a shared object do not force others to rebuilt and redeployed Execution unit service level reduce dependency between components down to the level of data structures And each execution unit is independent from others SoA may not be the best architecture for everything The best changes as the project evolves Although the general solution is SoA it may be expensive development time and system resources and encourage coarsegrained decoupling Rather a good architect leaves the options open for as long as possible with decoupling mode being one of those options and protects majority of the code from potential changes foresees these changes and appropriately facilitates them Independent developability The horizontal and vertical decoupling helps with team organization and focus Independent deployability If done right the decoupling should help each component deploy and roll out separately Duplication Architect often fall into a trap that hinges on their fear of duplication Duplication is generally a bad thing in software But there are different kinds of duplication two seemingly duplicated code blocks may evolve along different paths and for different reasons When you separate vertically you will be tempted to think two use cases are similar because of similar algorithm UI screens or DB accesses Be careful Make sure the duplication is real before committing into coupling them Similarly when separate horizontally you may notice the data structure of a particular database record is very similar to the data structure of a screen view You may be tempted to simply pass the DB record to the UI rather than to create a view model that looks the same and copies the elements across Be careful this duplication is most likely fake better keep them decoupled The discussion with Fei about DPQA using an individual DBRecord class client structure or generated message type may reflect consideration for this coupling Boundaries drawing lines Separate software elements from one another with those side A knowing little about side B side B may know about side A Some are drawn early on some later Those early on are drawn for the purpose of deferring decisions avoid having decisions about details pollute business logic Coupling because of premature decisions saps a systems maintainability A good system avoids premature decisions made on details such as frameworks databases web servers util libraries dependency injection and defer them to the latest possible without significant impact what might go wrong in the design of an SoA they are not panacea Which lines to draw between things that matter and things that dont Contrary to common misconception usually business rule doesnt need to know database schema thus the layer of abstract database accessor which fortunately is in dpqa design Eg business rule and accessor interface belong to one element accessor impl belongs to another Direction of lines crossing the line we draw business rules element doesnt know about DB impl DB impl knows about business rules as db cant exist without business rules query languages in accessor impl reflects business logic not vice versa business rule only needs to know about an accessor interface what underlying storage is does not matter Similarly IO GUI is irrelevant to business rules With this we form a plugin architecture DB GUI are plugins to business rules This deeply asymmetric relationship is what we want in our systems for certain modules business rules to be immune to others DB GUI Touching back on earlier content elements on different sides of the line change at different times for different reasons Single Responsibility Principle tells us where to draw the boundaries Boundary anatomy Architecture of a system is defined by a set of components with boundaries separating them At runtime a boundary crossing is a function on one side calling one on the other side Trick to creating appropriate boundary crossing is to manage source code dependencies Without OO or an equivalent form of polymorphism architects must fall back on the dangerous practice of using pointers to functions to achieve the appropriate decoupling Reminder for the argument that high level business logic should not depend on low level mechanism should be the other way round Threads are not architectural boundaries but rather a way to organize the schedule and order of execution They may be contained in one component or span across multiple A local process is a much stronger physical boundary and a service even stronger For local processes and services the same argument of high level services should not contain specific knowledge of low level services but rather low level services can serve as plugins to high level services applies Policy and level Software systems are statements of policy that which transforms inputs to outputs Policies further break down into smaller ones eg business rules for calcs input validation output formatting etc Part of the art of developing an architecture is carefully separating policies from one another grouping them based on the ways they change and into separate components which are formed into an acyclic graph Meaning policies too should be grouped by Single Responsibility Principle and Common Closure Principle In a good architecture the direction of those dependencies is based on the level of the components that they connect In every case lowlevel components are designed so that they depend on highlevel components Level a strict definition of level is the distance from the inputs and outputs The farther a policy is from both the inputs and the outputs of the system the higher its level The policies that manage input and output are the lowestlevel policies in the system The data flows and the source code dependencies do not always point in the same direction This again is part of the art of software architecture We want source code dependencies to be decoupled from data flow and coupled to level Higherlevel policiesthose that are farthest from the inputs and outputstend to change less frequently and for more important reasons than lowerlevel policies Lowerlevel policiesthose that are closest to the inputs and outputstend to change frequently and with more urgency but for less important reasons Lower level policies should be plugins to higher level policies Think about the sapiwrapper service we recently looked at retrieving different data from a number of services carry out some business logic and serve the requests to it In this case the services sapiwrapper asks for data should be plugins to sapiwrapper service imagine the data source could be a BAS client or file system etc with sapiwrapper services own business logic being higher level Then if we think about plugging in guts to a business logic module metrics being low level should probably also be not a direct dependency of the higher level business logic module But instead we have a metrics interface Business rules If we are to divide our application into business rules and plugins we need to understand what business rules really are Critical business rules those that will save make business money executed on a computer or not Critical business rules usually require critical business data to work with We model critical business rules and critical business data with an Entity an object a module etc and this entity should exist unsullied with concerns about databases UI or thirdparty software Not all business rules are entities they can be use cases a description of how the automated system is used applicationspecific business rules one that wouldnt exist on paper Use cases contain rules that specify when critical business rules within the entities are invoked they control the dance of entities while entities should have no knowledge of them Entities are high level use cases are low level in the sense that use cases are application specific rules while entities are general enough for any applications Use case does not describe say a concrete user interface or a data ingestion method which are at an even lower level use case takes in input structure and gives output structure From what transport mechanism those come is lowlevel detail You might be tempted to contain references to entity objects inside requestresponse model because they share so much data Avoid this temptation as they change for different purposes at different times Business rules are the family jewels that should remain pristine unsullied by baser concerns such as UI or DB who should be plugged in as lesser concerns Screaming architecture Software architectures are structures that support the use cases of the system Just as the plans for a house or a library scream about the use cases of those buildings so should the architecture of a software application scream about the use cases of the application as opposed to a particular framework When using a framework think about hwo you can preserve the use case emphasis of your architecture Develop a strategy that prevents the framework from taking over the architecture If you have kept your frameworks at arms length and your architecture is all about the use cases then you should be able to unit test each particular use case without having frameworks say BAS ComDB2 in place The clean architecture An architecture should be independent of frameworks UI DB external agencies and testable Figure 221 is a good illustration in a concentric circle entities being the inner most wrapped by use cases then by interface adapters and finally by frameworks Given this circle we have the Dependency Rule source code dependency should only point inwards no changes in the outer circle should cause changes in the highlevel business object Note how the flow of control is the reverse of source code dependency business object calls something from the outer circle via an interface flow of control but in this callingaconcreteimplementationviaaninterface pattern why do we say source code dependency is inverted Maybe although the DB accessor does not include the business object header but the queries it makes reflect the usage pattern mandated by the business object An answer could be that the interface is considered part of the inner circles package and a concrete implementation includes this interface definition such that it can inherit from this interface And in terms of what goes in this interface take DB as an example recall that we dont allow SQL in use case layer So the particular query inputoutput would be hidden behind an interface and implemented by the concrete accessor as opposed to accessor exposing a SQL execution interface Isolated simple data structures are passed across boundaries we dont want data structures to have any kind of dependencies that violate the Dependency Rule eg passing a DB row structure thats particular to a DB Presenters and humble objects The humble object pattern was introduced as a way to help unit testers to separate behaviors that are hard to test from those that are easy to test split the behaviors into two classes the humble one containing all the hard to test behavoirs eg where an element is in GUI the view and the other one containing the easy to test behaviors the presenter that formats the data it gets and makes them screendisplay ready Focus on testing the presenter and keep the view humble does nothing more than load data from View Model into the screen Another example would be a DB accessor the concrete component that wraps around a particular DB query is humble and the interactors using such accessors via an interface are not they encapsulate particular business rules and perform things based on the output Focus the test on the interactor Then speaking of real examples is iribsz sft mapper humble I believe yes one way to think about it is this module packs a plain old data to be carried across boundaries the mapper itself matters less than the interactor that works with the data not unlike given a View Model load data to the screen If this holds does it justify not unit testing iribsz sft mapper Partial boundary A full boundary may be expensive boundary interfaces input output data structures and hard to maintain A purely anticipatory design may violate YAGNI you arnet going to need it in which case architects sometimes propose a partial boundary in which all the components may be in place but sitting in one group or a dependency inversion interface is in place or like a Facade pattern where even a dependency inversion is not enforced Layers and boundaries Hunt the Wumpus game example support multiple languages Different input interfaces Underlying data storage Multiplayer Player status management as a microservice The point is however simple a program architects need to identify boundaries when they are needed and if we dont add them now how much effort would it be to add them later Over engineering is often much worse than under engineering but on the other hand when you discover you do need boundaries whereas they dont already exist its often risky and costly to add them As an architect you make educated guesses and keep a watchful eye and revisit your decisions as the project progresses The main component In every system there is at least one component that creates coordinates and oversees others We call this component Main Main is the ultimate detail the lowestlevel entry Nothing depends on Main Main create higher level objects and hand over control to them Think of Main as a plugin to the application who sets up the initial conditions and configs gathers outside resources then hand over control to higher level policy of the application A different Main configuration could be plugged in for each scenario Services great and small SOAs Are services always architecturally significant Not necessarily the architecture of the system is defined by boundaries that separate highlevel policy from lowlevel detail Services themselves are a form and they are architecturally significant when they represent architectural boundaries Do they offer better decoupling Not necessarily They can still be coupled by shared resources on the network or data they share Eg similar to function signature changes plumbing a new field in a series of microservices needs service schema changes Do they offer independent development and deployment True to some extent but often times we see operations still needing coordination Architecture is not defined by the physical mechanisms by which elements communicate and execute Cross cutting concerns a system built with functional decomposition are very vulnerable to new features that cut across all functional behaviors SOA or not eg Uber to deliver cats Considering which taxi suppliers drivers are in passenger allergies etc The answer is to think in terms of functional components a taxi supplier component service or not could offer an interface where the concrete human cat rides behavior can derive from In which case the boundary may run through services dividing them into components To summarize useful as they are SOAs are not panaceas and to account for cross cutting changes the internals of a service or a functional component may need to be designed with Dependency Rule in mind and allow pluggable concrete implementations Test boundaries Tests of different flavors unit integration etc are part of the system They are very detailed and concrete nothing depends on the them and they follow the Dependency Rule pointing inwards Fragile test problem changing common system components breaks tons of tests Tests that are not well integrated into the design of the system tend to be fragile and they make the system rigid and difficult to change Design for testability tests and others should not depend on volatile things Eg test the business logic without depending on UI Structural coupling questionable if a test suite has a test class for each production class and a test method for each production method tests have to be changed as well for each production interface change The proposed solution is a test API that allows changes in production which dont affect the tests I find this questionable and should be analyzed case by case in unit tests all public interfaces should be tested At unit test level having test API probably means having methods to force the object into a certain state This would be useful but does not allow not having a test method corresponding with each method in the public interface what the authors claim to be structural coupling Clean embedded architecture Software does not wear out unlike hardware However software can be destroyed from within by unmanaged dependencies on firmware and hardware What really is firmware Code that lives on ROM Maybe another way of seeing it is code that is heavily dependent on hardware In a sense we write firmware too if our software is coupled with particular hardware details Dont write firmware if you want your code to have a long useful life First make it work Then make it right Then make it fast The hardware is a detail Split your software from your firmware and make those that dont need to depend on hardware hardwareindependent Software should allow off target testing Introduce a Hardware Abstraction Layer between firmware and software if needed and HAL exposes applicationsemantic interfaces Similarly you should insulate your software from OS dependencies An OSAL should be introduced Similarly for us pricing code should be testable off target as well Dont repeat yourself Part 6 details The database is a detail The particulars of a database is of little architectural importance while the data model is How would one define a data model Business entity should not know the detail of the underlying data stored in a relational way represented as tabular rows of records etc Why are database systems so prevalent They work with disks file system does and offers a document abstraction databases are content based and provide a natural and convenient way to find records based on their content Relational database used to be a buzzword nowadays are SoAs such words as well The web is a detail The endless pendulum clientside execution serverside execution superhosts and dumb terminals or vice versa cloud or edge centralizing distributing the spiraling up These oscillations will continue for some time to come The story of the UI of company Q The upshot is simple the GUI is a detail and the web is a GUI as an architect keep such stuff away from your business logic Alternatively does it make sense to provide abstractions over different technologies like the web and the desktop or over RabbitMQ and Kafka The argument can be made that a GUI like the web is so unique and rich that it is absurd to pursue a deviceindependent architecture When you think about the intricacies of JavaScript validation or draganddrop AJAX calls or any of the plethora of other widgets and gadgets you can put on a web page its easy to argue that device independence is impractical To some extent this is true The interaction between the application and the GUI is chatty in ways that are quite specific to the kind of GUI you have The dance between a browser and a web application is different from the dance between a desktop GUI and its application Trying to abstract out that dance the way devices are abstracted out of UNIX seems unlikely to be possible The argument is that its still possible to provide such abstractions especially isolating business logic from the underlying details This kind of abstraction is not easy and it will likely take several iterations to get just right But it is possible And since the world is full of marketing geniuses its not hard to make the case that its often very necessary Frameworks are details Frameworks are not architectures Framework authors do not know your problems although your problems may overlap The framework author is asking you to couple with their framework your commitment to the framework is huge while the framework makes no commitment to you Risks with a framework the framework may help you with early features but your product may outgrow the framework the framework may evolve in a direction you dont want a better framework may come along the way The solution is that you can use a framework but dont marry it dont let it in the inner circle If the framework wants you to derive your business objects from its base classes say no Derive proxies instead and keep those proxies in components that are plugins to your business rules Use case Video serving website roles and use cases division of components The missing advice Your best design intentions can be destroyed in a flash if you dont consider the intricacies of the implementation strategy Think about how to map your desired design on to code structures how to organize that code and which decoupling modes to apply during runtime and compiletime Leave options open where applicable but be pragmatic and take into consideration the size of your team their skill level and the complexity of the solution in conjunction with your time and budgetary constraints Also think about using your compiler to help you enforce your chosen architectural style and watch out for coupling in other areas such as data models The devil is in the implementation details Afterword The evolution from Big architectures and engineering practices to Agile Now youve seen how its possible to write code that delivers value today without blocking future value tomorrow the onus is on you to put in the practice so you can apply these principles to your own code Like riding a bicycle you cant master software design just by reading about it To get the best from a book like this you need to get practical Analyze your code and look for the kinds of problems Bob highlights then practice refactoring the code to fix these problems If youre new to the refactoring discipline then this will be a doubly valuable experience"},{"title":"\"Data intensive application\"","href":"/notes/data-intensive-application","content":" Designing Data Intensive Applications Chap 1 Reliability scalability and maintainability Data intensive vs compute intensive Reliability Fault and failure Hardware error usually uncorrelated and handled by redundancy Software error usually happens when certain assumptions about the system dont hold any more Correlated and cascading and harder to debug Human error Scalability Defining load The Twitter example to handle large fanout large read workload problem Relational query approach Fanout queue approach Measuring performance Percentiles Load parameters Head of line blocking Tail latency amplification Conventional wisdom for scaling databases or stateful data systems is to scale up grow tall as opposed to scale out grow wide until you hit a limit Think ComDB2 Elasticity Maintainability Operability simplicity extendibility agility of a system Complexity as native to the use case required functionality or accidental debt and hacks accumulated over time fitting a circle into a square etc Good abstractions albeit hard to come by help bring down accidental complexity Nonfunctional requirements Chap 2 Data model and query languages Relational data model SQL Data is organized by relation into tables of rows unordered collection of tuples Comes from business data processing business transactions and batch processing Back in the days network model and hierarchical model eg xml DB were the main alternatives NoSQL describes a variety of nonrelational approach to data modeling Driven by these concerns Scalability very large datasets very high write throughput Open free software over commercial software Specialized queries not well supported in SQL Restrictiveness of a relational schema need more dynamic and expressiveness In the near future well see a polyglot persistance relational approach coexists with NoSQL Objectrelational mismatch matching SQL tables rows columns and OOP objects takes effort ObjectRelational Mapping tools are introduced to address this If we are to model onetomany with SQL the options are separate table with foreign key latest SQL has structured data type support eg xml json This comes with indexing query support inside This is almost like a blend between documentbased and relational encode as json xml and store encoded Wont be able to index query in this case The resume use case selfcontained document one person to many jobs One json object a row makes sense documentoriented DB like MongoDB comes in handy This features a lack of schema flexibility and better locality normalization in DB refers deduplicating humanmeaningful information using one level of indirection eg the neighborhood a person lives in should not be represented with a raw string but rather an enum with another map from enum to neighborhood name string This brings the problem of modeling manytomany where documentoriented storage can get awkward wed need applevel joins on several documents network model uses a treelike structure with multiple parents and software traverses the structure following along access paths like traversing a linked list This makes it difficult to change data model and the software the relational model addresses this with separate tables and supporting arbitrary select and join conditions Query optimization is abstracted away from applevel code In dealing with manytomany relationship relational and document databases arent fundamentally different a unique id to identify the external record called a foreign key and a document reference respectively Relational vs document databases today Schema flexibility better performance due to locality closer to the data structure of the application for document databases Better support for join manytoone and manytomany for relationship databases Relational can employ the technique of shredding splitting a documentlike structure into multiple tables linked by a foreign key to achieve a documentlike structure which is cumbersome to schema and complicates application code Document cannot refer directly to a nested item within a document instead you say the 2nd item in the list of positions for user 256 like an access path To model manytomany in document you can do denormalization but the application code need to do additional work to keep denormalized data consistent Schemaonread the reading application interprets expects certain schema like runtime type checking and schemaonwrite the database enforcing some schema upon writing data like static type checking in a compiled language In general there is no right answer in terms of which is preferable The difference is particularly noticeable when performing a schema change Changing application code or migrating DB Managing locality When you need large parts of data in the same document Or in relational model Spanners interleaved table rows within a parent table Similarly columnfamily in Bigtable Cassandra HBase data model When the relational model is introduced SQL a declarative way to query the data came with it whereas others of its time queried databases primarily imperatively The declarative approach follows relational algebra notions more closely while the imperative way is more like telling the computer step by step what to do Higherlevel abstractions are often preferable as they are easier more expressive and the application query is decoupled from underlying database performance improvement Higher level query languages like SQL can be implemented as a pipeline of MapReduce operations and there is nothing that constraints a SQL query to run on a single machine Graphlike data models social graph web graph road networks etc where nodes are homogeneous Eg Facebooks social graph can have nodes as people locations events etc Property graphs Each node has a uniqueid incomingvertices outgoingvertices and a collection of keyvalue pairs Each vertice has a uniqueid headvertex tailvertex label and a collection of keyvalue pairs Can store flexible information and good for extension Described in a relational schema it looks like the following CREATE TABLE vertices vertexid integer PRIMARY KEY properties json CREATE TABLE edges edgeid integer PRIMARY KEY tailvertex integer REFERENCES vertices vertexid headvertex integer REFERENCES vertices vertexid label text properties json CREATE INDEX edgetails ON edges tailvertex CREATE INDEX edgeheads ON edges headvertex any can connect with any with the indexes we can efficiently find head and tail edges of a given vertex thus traversing the graph we then insert entries like in Cypher graph query language CREATE USAlocation nameUnited States typecountry Europelocation nameEurope typecontinent Francelocation nameFrance typecountry Lucyperson nameLucy France WITHIN Europe Lucy BORNIN USA Lucy LIVEIN France and the query of all people emigrated to Europe from the US looks like MATCH person BORNIN WITHIN0 uslocation nameUnited States person LIVEIN WITHIN0 eulocation nameEurope RETURN personname find any vertex call it person that meets both conditions has an outgoing BORNIN edge to some vertex from there you can follow any number of WITHIN edges until reaching a node of type location whose name property is United States Similar for the Europe analysis the query optimizer then chooses the optimal way to execute this query from all persons or the two regions eg depending on where you have index Although its possible to put a graph database in a relational database supporting queries in such can be difficult as the number of joins is not known beforehand SQL supports WITH RECURSIVE but is very clumsy Triplestore graph model all information stored in threepart statement subject predicate object where an object can be a value or another node In case of value this means a property in case of another node this means an edge TurtleN3 France a location name France type country Lucy a person name Lucy livein France this can alternatively be expressed in XML Semantic web independent from triplestore proposes a Resource Description Framework under which websites publish data in a consistent format thus allowing different websites to be automatically combined into a web of data like a graph database of the entire web SPARQL query language operates on tripstores using the RDF data model Cyphers pattern matching above is borrowed from SPARQL difference being triplestore does not differentiate properties and edges thus both can be queried using the same syntax Are graph databases the second coming of network model CODASYL No CODASYL schema specifies what record types can be nested within which other record types Graph databases allow any vertex to connect to any other In CODASYL the only way to reach a node is via an access path graph database allows query by index on uniqueid as well CODASYL children of a record are an ordered set graph databases have no such constraint on nodes and edges In CODASYL all queries are imperative and easily broken by change in schema Cypher or SPARQL queries are highlevel Datalog Foundation for later query languages Similar data model as triple store generalized to predicatesubject object We define rules that depend on other rules or themselves Datalog Prolog withinrecursiveLocation Name nameLocation Name withinrecursiveLocation Name withinLocation via withinrecursivevia Name migratedName BornIn LivingIn nameperson Name borninperson bornloc withinrecursivebornloc BornIn liveinperson liveloc withinrecursiveliveloc LivingIn migratedWho United States Europe The Datalog approach is powerful in that rules can be combined and reused in different queries Summary Data models Hierarchical historically relational came along to tackle manytomany relation Recent applications whose data model does not fit either NoSQL mostly diverged in document and graph databases These dont usually enforce a schema on data they write but has assumptions about data when reading Each model comes with their own sets of query languages SQL MapReduce Cypher Datalog MongoDBs pipeline etc Relational document and graph are all widely used today and one model can be emulated using another though the result is often awkward Some ongoing research about data models try to find suitable ones for sequence similarity searches genome data PBs amount of data particle physics and fulltext search indexes Chap 3 Storage and retrieval Database is all about storage and query Logbased storage engines an appendonly sequence of records and pageoriented storage engines An index is an additional structure that is derived from the primary data Adding and removing indexes does not affect the contents of the database only performance of queries Any kind of index usually slows down writes because the index also needs to be updated onwrite Thus databases typically let the application developer choose what to index on since they know the access pattern of the database best Logstructured indexes Inmemory hash index A simple approach could be a logstructured storage where each record is and a hash index is stored in main memory Bitcask is one example does this and is well suited for cases where key cardinality is low fits in memory but updated often hence requiring fast writes To avoid only growing disk usage we can chunk the log into segments of a certain size and when closing off on writing one segment we perform compaction where we only keep the most recent update to each key We can also merge several segments together when performing compaction While compaction is being performed we can continue to serve read and write requests using the old segment files compaction never modifies an existing segment files the result is written to a new segment files After compaction is done we switch read and write requests to using the newly produced segment file and delete the old segments With a hash index each segment now needs its own hashmap index in memory and when looking for a key we first search in the hashmap of the most recent segment and if not found we go to the hashmap of the next most recent segment We usually store the log in binary format append a special tombstone to indicate deletion of a record To recover from a crash one could read all the segments to repopulate the hashmap index which is slow Bitcask stores a snapshot of each segments hashmap on disk which can be loaded into memory for faster recovery To handle partially written records eg crash while writing Bitcask uses a checksum For concurrency control we can allow only one writer thread to append to the current segment Multiple threads can read at the same time Appendonly log may seem wasteful but as opposed to updating the record appendonly uses sequential write operations which are generally much faster than random writes especially on magnetic spinning disk hard drives Concurrency and crash recovery are also made much simpler if segment files are appendonly or immutable Merging old segments avoids the problem of data files getting fragmented over time Hashbased index should fit in memory you could maintain a hashmap on disk but ondisk hashmap query requires a lot of random access IO is expensive to grow when it becomes full and hash collisions require fiddly logic Hashbased index is also efficient when doing range queries SSTables and LSM tree Previously our stored records key value pairs appear in the sequence they are written SSTable Sorted String Table require that the sequence of key value pairs is sorted by key SSTables has several advantages over log segments with hash indexes Merging segments is simple and efficient even for file bigger than the available memory starting at the head of all segments each time copy over the lowest orderedkey over to the new file This produces a merged segment file also sorted by key If the same key appears in multiple input segments we only need to keep the value from the most recent segment To find a particular key we dont need to keep an index of all the keys in memory Instead you can have an inmemory sparse index to tell you the offsets of some keys and you can scan a range of two offsets to find if the key you are looking for is in Blocks of entries between every two sparse indices can be compressed which reduces IO bandwidth Constructing and maintaining SSTables We maintain an inmemory stored structure memtable say a redblack tree and write requests gets added to this tree When tree gets big enough we flush it into disk While this tree is being flushed we can keep writing to a new memtable When querying we first look inside the memtable then the most recent ondisk segment then the next recent etc From time to time run merge and compaction to combine segment files and to discard overwritten or deleted values To recover from memtable crashes as we write to memtable we also write to a log This log doesnt need to be sorted by key as all it serves is crash recovery and can be discarded when a memtable gets flushed to disk LevelDB and RocksDB use the algorithm described above Similar storage engines are used in Cassandra and HBase both were inspired by Bigtable paper which introduced the terms SSTable and memtable Lucene indexing engine for fulltext search used by Elasticsearch and Solr uses a similar method for storing its term dictionary This merging and compacting indexing structure originally builds upon Log Structured Merge trees which was built upon earlier work on logstructured file systems Optimization Query can be slow first memtable then segmentbysegment lookups We can add a bloom filter to definitively tell if a key does not exist Sizetiered and leveltiered compaction RocksDB and LevelDB use leveltier where key range is split into smaller SSTables and older data is moved into separate levels HBase uses sizetier where newer and smaller SSTables get merged into older and larger ones Cassandra supports both The basic idea of LSMtrees keeping a cascade of SSTables that are merged in the background is simple and effective It scales well when data is much bigger than memory supports range query well and because all disk writes are sequential the LSMtree can support remarkably high write throughput Btree indexes The most widely used indexing structure is Btree They remain the standard implementation in almost all relational databases and many nonrelational databases use them too Similar as SSTables Btree also sorts by key but thats where the similarity ends Logstructured indexes break the database down into variablesize segments and always write a segment sequentially while Btrees break the database down to fixedsize blocks or pages traditionally 4KB in size and read write one page at a time This corresponds closely to the underlying hardware One page is designated as root of the tree root points to child pages where each child is responsible for a continuous range of keys A leaf page can either contain the key value inline or contains references to pages where the values can be found Typically branching factor of a Btree is several hundred To update an existing value for an existing key find the leaf page change the page and write the page back to disk Adding a new key may split an existing page into two The split algorithm keeps the Btree balanced Most databases can fit into a Btree that is three or four levels deep 4KB pages 4 levels branching factor of 500 can store up to 256TB The basic write operation of a Btree is to overwrite a page on disk with new data and it is assumed that the overwrite does not change the location of the page ie all references to the page remain intact when the page is overwritten This is in stark contrast with LSMtrees where files are appendonly and deleted but never modified inplace A write causing a split will cause two children pages and parent page to be overwritten This is a dangerous operation as a crash after some pages have been written leaves you with a corrupted index In order to make Btree resilient to crashes it includes a writeahead log an appendonly file to which every Btree modification must be written before it can be applied to the pages of the trees itself This log is used to restore Btree to a consistent state after crash Concurrency control is also more complicated a reader may see a tree in an inconsistent state without concurrency control Btrees typically use latches lightweight locks Logstructured approaches are simpler in this regard as all merging are done in the background without interfering with incoming queries and atomically swap old segments for new segments from time to time Btree optimizations have been introduced over the years eg copyonwrite pages where a modified page is written to a different location and a new version of the parent pages in the tree is created pointing at the new location Abbreviating keys in interior pages Optimized storage layout such that leaf pages appear in sequential order on disk Adding additional pointers such as left and right siblings Fractal trees borrow some logstructured ideas to reduce disk seeks Comparison Btree and LSMtree As a rule of thumb LSMtrees are typically faster in write whereas Btrees are thought to be faster in reads LSMtrees are thought as slow in this regard as potentially several SSTables at different stages of compression has to be checked However benchmarks are generally inconclusive and sensitive to the details of workload Write amplification one write to the database would result in multiple writes to the disk over te course of the datas lifetime Eg compaction in LSM trees Btrees overwriting an entire page even if only a few bytes changed This is a concern for SSD as blocks can only be overwritten a limited number of times before wearing out In highwriteworkload scenarios bottleneck might be the rate at which the database can write to disk in which case write amplification directly affects performance LSMtrees can typically sustain higher write workload than Btrees because of lower write amplification depends on workload and storage engine configuration and also writing compact SSTable structures sequentially as opposed to overwriting several pages in the tree This is particularly important for magnetic hard drives where random writes are far slower than sequential writes LSMtrees typically have lower storage overhead Btree leaves some disk space unused due to fragmentation eg when splitting LSM trees are not page based and periodically compact SSTables to remove fragmentation giving them typically lower storage overhead Some SSDs internally use a logstructured algorithm to turn random writes into sequential writes hence nullifying the downsides of Btrees random writes but LSMs typically lower write amplification and reduced fragmentation still matters for throughput One downside of LSM tree is expensive compactions affecting the performance of readwrites as a user request may need to wait for disk to finish a compaction The impact on throughput and response is usually small but at higher percentiles the response time of LSMtrees can be less predictable than that of Btrees Also as compaction threads and the logging flushing memtable to disk thread shares the write bandwith the larger the database gets the more write bandwidth compaction threads might use If compaction cannot keep up with new writes tbe number of unmerged segments grow until you run out of disk space reads would become slow as well This is a situation you want to monitor Btrees also have a key at one specific location only while LSM trees can have the same key stored in multiple places The former also made offering strong transactional semantics easier as what they can do is to lock the key range and attach those locks directly to the tree Btrees are very ingrained in the database architecture of todays and arent going away any time soon LSMtrees are getting popular but you should assess given your workload to decide which is more suitable Other indexes Seconary indexes The above discusses key value indexes like a primary key in the relational model unique identifier Secondary indexes are very common not unique and both LSMtrees and Btrees can be used as secondary indexes When storing records for each key we can store the value itself or a reference to somewhere else known as a heap file which uses no particular order can be appendonly or keep track of deleted entries and overwrite them The heap file approach is common to deduplicate the actual data When not changing the key overwriting with a smaller value is easy but overwriting with a larger value will cause the value to be relocated thus needing to update all references as well or leave a forwarding pointer in its old heap location Sometimes the extra hop from index to heap file is too much of a performance penalty for reads so it can be desirable to store the indexed row directly within an index This is called a clustered index which the primary key of a table in MySQLs InnoDB storage engine is always a clustered index and secondary indexes refer to the primary key rather than a heap file location A compromise between a clustered index and a nonclustered storing only references to the data within the index is a covering index where some of a tables columns are stored with the index allowing some queries to be answered by using the index alone Covering and clustered index can speed up reads but introduce extra storage and overhead to writes Transactional guarantee also becomes harder because of duplication Multicolumn indexes Multicolumn index can be a concatenated index where its concatenating a few keys and it would allow querying by a number of prefix keys of the concatenated key Eg to support 2D geospatial data search one option is to translate the 2 dimensions into a single number using a spacefilling curve then use a Btree or more commonly specialized indexes such as Rtrees are used Another case is when needing to support filtering by multiple columns at the same time Fulltext searches and fuzzy indexes Fuzzy search within a certain edit distance ignore the grammatical variations of the searched keyword Lucene supports such and internally it uses an SSTablelike structure where the inmemory index is a finite state automaton over the characters in the keys similar to a trie This automaton can then be transformed to a Levenshtein automaton which supports efficient search for words within a given edit distance Keeping everything in memory Compared with memory disks are awkward to deal with you have to lay out data carefully if you want good performance on reads and writes With memory getting cheaper keeping entire databases in memory becomes possible memcached inmemory caching provides a solution when data durability is not required When durability is required an inmemory database can write a log to disk writing peiodic snapshots to disk or replicating the inmemory state to other machines Despite the disk interaction its still considered an inmemory database because disk is only used as an appendonly log for durability and reads are served entirely from memory Writing to disk also has operational advantages where its easier to backed up inspected and analyzed by external utilities Redis provides weak durability by writing to disk asynchronously Counterintuitively the performance advantage of inmemory databases is not due to the fact they dont need to read from disks Even a diskbased storage engine may never need to read from disk if you have enough memory as the OS caches recently used disk blocks anyway Rather they can be faster as they avoid the overheads of encoding inmemory data structures in a form that can be written to disk Inmemory databases also provides data models that are difficult to implement with diskbased indexes eg Redis offers a DBlike interface to various data structures such as priority queues and sets Keeping all data in memory makes its implementation comparatively simple Inmemory databases can store data larger than memory without bringing back the overheads of using a disk the anticaching approach works by evicting the least recently used data to disk when there is not enough memory and loading it back in when accessed This is similar to what OS does with swaps and virtual memory but with more granularity eg individual records as opposed to memory pages hence more efficient than what the OS does This approach still requires the entire index to fit in memory Transaction processing or analytics The word transaction traces back to databases early days for recording money changing hands now it refers to a group of reads and writes that form a logical unit A transaction neednt necessarily have ACID atomicity consistency isolation and durability properties transaction processing just means allowing clients to make lowlatency reads and writes as opposed to batch processing jobs which run only periodically Over time we see two major query patterns for databases look up a small number of records by some key using an index Records are then inserted or updated based on the users input These application are usually interactive and became known as online transaction processing OLTP scan over a huge number of records reading only a few columns per record and calculates aggregate statistics sum avg etc rather than returning the raw data to user These are known as online analytics processing OLAP Relational DBs started out working fine for both OLTP and OLAP over time some companies switched over to data warehouse for their OLAP workload In some setups the OLTP systems being latency sensitive and mission critical does not serve analytics requests a readonly copy of data extracted from the OLTP system transformed into an analysis friendly schema and cleaned up is loaded into the data warehouse This process of loading OLTP data into OLAP warehouse is known as ExtractTransformLoad ETL Having a separate OLAP system allows optimization specific to its query pattern The data model of a data warehouse is most commonly relational as SQL is generally a good fit for analytics queries MS SQL Server supports transaction processing and data warehousing in the same product however they are becoming increasingly two separate storage and query engines served through the same SQL interface Open source SQLonHadoop projects fall into the same data warehouse category Apache Hive Spark SQL Cloudera Impala Facebook Presto Apache Tajo and Apache Drill Some of them are based on Google Dremel Data warehouses dont see a variety of data models as transaction processing DBs do Most follow a starschema where a giant central table facts table records events and foreignkey references to other tables dimension tables to eg normalize dimension tables would record the who what where how and why of an event This is like a star where facts table sits in the center and connects to peripheral dimension tables Snowflake schema is a variation of the star where dimensions are further broken down into subdimensions They are more normalized but harder to work with Columnoriented storage If you have trillion of rows in your facts table storing and querying them efficiently becomes a problem Fact tables are always over 100 columns wide but one query rarely accesses more than 4 or 5 of them at the same time OLTP databases including documentbased ones usually organize one row document as a contiguous sequence of bytes columnoriented storage instead stores all the values from each column together Eg each column of a facts table gets stored in its own file such that when only accessing a few columns in a query we only read those files as opposed to reading all the columns then filter Columnoriented storage layout requires each column file containing rows in the same order Columnoriented storage often offers great opportunities for compression bitmap encoding is often used The cardinality of the set of distinct values in a column is often small compared with the number of rows We then use a bitmap to represent the set of distinct values in a column eg 200 countries in the world 25B to store and each distinct column value X would then correspond with a series of bits where we have 1 bit for each column and wed have 1 if that column is X and 0 if not For each column value X wed then end up with a series of bits that are predominantly 0 and we can then apply runlength encoding 100 zeroes followed by 2 ones to further compress This storage schema also makes filtering by a few column values easier eg we apply a bitwiseor over all the series of bits of those column values and return the selected columns Cassandra and HBase offer columnfamilies which they inherited from BigTable Those would still be rowbased storage as within each column family they store all columns from a row together along with the row key and they dont use column compression Hence the BigTable model is still mostly roworiented Columnoriented layouts also are good for making efficient uses of CPU cycles CPU loads one L1cacheful of compressed column data does some bitwise andor without function calls iterate through the data in a tight loop on the compressed data directly singleinstructionmultidataSIMD instructions on modern CPUs This leverages vectorized processing In a column store it might be easiest to store data in the order they come in as then insertion becomes simple append its also possible to impose an order as in an SSTable note that fields of the same record needs to remain in the same kth record in every column data file Sorting would also help with compression especially for the first sort key Vertica sorts a columnoriented storage in several ways the data needs to have multiple copies anyway so why not sort them in different orders to answer different kinds of queries Writing to columnoriented storage can be tricky as insertion in the middle requires rewriting all column data files LSMtrees dont have this constraint column or row based when enough writes have accumulated they are merged with the column files on disk and written to new files Data cubes and materialized views Not every data warehouse is columnar if queries often involve count sum avg etc we could cache some of the counts or sums that queries use most often A view is often defined as the resulting table of some query A materialized view is an actual copy of the query result written to disk a denormalized copy of the data matching some conditions When the underlying data changes the materialized view needs to be updated as well They make writes more expensive which is why materialized view is not often seen in OLTP databases A materialized data cube is a denormalized and highdimensional materialized view with some aggregation statistics such that particular queries on those statistics are faster Summary How databases handle storage and retrieval internally OLTP transaction processing workload request volume is large each touches few records usually via some key index expect lowlatency in response Disk seek time is often the bottleneck here Storage LSMtrees appendonly immutable data files SSTables merge systematically turn randomaccess writes to sequenetial writes enabling higher write throughput Btrees overwrite fixed sized pages inplace Indexing multiindex inmemory database OLAP analytics workload data warehouse lower volume queries needing to read a large number of records Disk bandwidth is bottleneck Columnoriented storage is increasingly popular for this Indexing are less relevant instead compression becomes important Chap 4 Encoding and Evolution Application change Evolvability is important Server software usually goes through staged rollout client software upgrade are at the mercy of clients Coexistence of old and new code makes backward and forward compatibility important new being able to work with old old being able to work with new Forward compatibility is usually trickier as it requires old code to ignore additions new code did Programs work with inmemory data structures as well as serialized encoded data when needing to transfer over the network maybe with the exception of memory mapped files Languagespecific serialization formats javaioSerializable pickle etc These are usually easy to code but they may be specific to that language in order to restore data in the same object types the decoder needs to able to instantiate arbitrary classes which is a source of security problems versioning is often an after thought for these utilities efficiency is often an after thought These encodings can be very bloated CPUinefficient to use For these reasons its generally a bad idea to use the languages builtin serialization library for anything other than very transient purposes JSON XML and binary variants JSON XML CSV are all textual formats somewhat humanreadable and widely supported encodings Some subtle problems besides superficial syntactic issues ambiguity around encoding of numbers XML and CSV cannot differentiate a string of numbers and a number without referencing external schema JSON can but does not distinguish floats and integers and doesnt specify a floating point precision An integer larger than 253 cannot be exactly represented by IEEE 754 double precision float Consequently JSON returned by Twitters API includes tweet IDs twice once as JSON number and once as decimal string to work around the fact that number this large may not be correctly parsed by JS applications JSON and XML have good support for unicode character strings but not binary strings People get around this limitation by using base64 to encode the binary data and this increases size JSON and XML both have optional schema support XML schema is widely used JSON not as much In cases where a schema is not used the decoding application potentially needs to hardcode the appropriate encoding decoding logic CSV does not have schema each column is up for interpretation Its also vague eg what happens if a value contains a comma Escaping has been formally specified but all parsers support them correctly Despite these flaws these encodings will likely remain popular as data interchange formats Binary encoding has been developed for JSON and XML since they dont prescribe a schema they still need to include all the field names within encoded data Thrift and protobuf Apache Thrift and protobuf are binary encoding libraries that are based on the same principle Both require a schema in an interface definition language IDL and come with a code generation tool that takes the schema and produces code in various languages that implement it One big difference with binary encoding of JSONXML is field names are not present in encoded data instead field tags numbers are used like normalization with a schema definition Protobuf binary encoding uses variable length integers and encodes very similarly to Thrift CompactProtocol Thrift also has a BinaryProtocol which does not use variable length integer Encoding is TLV Handling schema evolution Add new fields to the schema provided that you give each field a new tag number Forward compatibility old code not recognizing the tag number can just ignore it Backward compatibility new code can still read old messages since tag number doesnt change the only detail is that when adding a new field you cannot mark it required they must be optional or have a default value Removing is like adding with backward and forward compatibility concerns reversed you cannot remove a field that is required and you can never use the same tag again Changing data types of fields may be possible data can lose precision or become truncated One peculiarity in Protobuf there is no explicit array but instead a repeated marker a third option along with required and optional meaning something can appear 0 to N times exactly 1 time or 0 or 1 times Avro Avro is another encoding format different from Protobuf and Thrift it is developed since Thrift was deemed not a good fit for Hadoops use cases Avro has an IDL to describe schema The peculiarity is in Avro having no field tags or type indication in the encoded data a string or an integer is a length prefix data UTF8 or variable length integer encoding Being able to decode relies on going through the fields in the order that they appear in the schema meaning the decoder can work only if using the exact same schema as encoder In order to support schema evolution the writers schema and readers schema dont have to be the same When decoding Avro resolves difference by looking at writers schema and readers schema sidebyside and translating the writers schema into readers schema Field reordering can be reconciled fields in writers schema but not readers will be ignored by decoder and field in readers but not writers will be filled with default values in readers schema To maintain and backwards and forwards compatibility you can then only add or remove fields with a default value Avro doesnt have optional required marker as protobuf and thrift do it has default values and union types instead where allowing a field to be null requires including null in a union type In the context of Hadoop Avro is often used for storing a large file of millions of records all encoded with the same schema Hence the overhead of including that schema with the file is not huge In a database where records are written at different times with different schema Avro keeps a versioned schema in a schema database When sending data over the network Avro RPC protocol negotiates a shared schema between two parties Why might this be preferable to Protobuf and Thrifts schema Not having tag numbers makes Avro friendlier to dynamically generated schemas Eg in a case where you want to encode a table in a relational database exporting it to Avro Protobuf Thrift and then the table schema changes when exporting the newer version Protobuf Thrift versions have to be careful about field tag while Avro has no such concern Code generation is often useful for statically typed languages where the generated code allows type checking while in dynamically typed language code generation is often frowned upon Protobuf Thrift and Avro schemas are simpler than XMLJSON schemas as the latter support more detailed validation rule like regexp integer ranger etc Pro of schemas These encodings are based on the idea introduced in ASN1 used to define various network protocols and its binary encoding DER is still used to encode SSL certificates X509 Most relational database vendors also have their own proprietary binary encoding for their query protocol over the network the database vendors usually then provides a driver using the ODBC or JDBC APIs that decodes data over the network Binary encodings based on a schemas are viable compared to textual formats like JSONXML in particular binary encoding is more compact omitting field names schema is a good form of documentation keeping a database of schemas allows checking backward and forward compatible changes and in statically typed languages generating code from schema allows compile time type checking Modes of dataflow Usually data flows via databases service calls asynchronous message passing Via databases Be mindful of forward and backward compatibility forward compatibility and when writer of an old version reads data written by a new version writes on top of that and being able to save that data back Data outlives code upgrading server software is fast but data written long ago will still be there Most databases avoid migration if possible due to its being expensive Archiving snapshotting data usually uses the latest schema and Avro object container files are a good fit or in columnoriented analytical format Via services Server defines an API clients request data from it The API is called a service The approach is usually to decompose a larger application into smaller services by area of functionality called a serviceoriented architecture microservices architecture Key design goal being to make the application easy to change and maintain by making services independently deployable and evolvable A web service is where HTTP is used as the underlying protocol of talking to the service REST and SOAP are two popular approaches to web services They are diametrically opposed in terms of philosophy REST is not a protocol but a design philosophy that builds upon the principles of HTTP where it uses simple data formats URLs for identifying resources HTTP features for cache control authentication and content type negotiation An API designed with such in mind is called RESTful SOAP is an XMLbased protocol for making network API requests Although most commonly used over HTTP it aims to be independent from HTTP and avoids using most HTTP features API of a SOAP web service is described using Web Services Description Language WSDL an XMLbased language WSDL is not designed to be humanreadable users of SOAP rely heavily on tool support and code generation Web services are the latest incarnation of a long line of technologies for making API requests over a network based off the ideas of RPC whuch tries to make a request to a remote service look the same as calling a function or method in your programming language This may be fundamentally flawed in the sense that a local function call is predictable and either succeeds or fails depending only on the parameters that are under your control A network request is unpredictable a local function call either returns a result or throws an exception or never returns a network request can have more outcome like returning without a result due to a timeout retrying a call that didnt respond in time may actually cause the action to happen twice in the remote in which case idempotence needs to be built in Local calls dont have this problem local function call times are more predictable while network delays can vary wildly a local function call can take in references efficiently network call requires all actual data to be sent over the network the client and service may be implemented in different languages so RPC framework has to translate datatypes from one language into another not a problem for local function calls REST does not try to hide the fact that it goes through network RPC libraries can be built using REST With these flaws RPC isnt going away in the short term gRPC is an RPC implementation using protobuf Thrift and Avro come with RPC support The new generation of RPC framework is more explicit about a remote request being different from a local call eg they may use futures promises to encapsulate asynchronous actions that may fail gRPC also supports streams with a call consisting of more than one request and response Dataflow through services can be easier to involve than dataflow via database you can assume servers are always upgraded first and clients after them Hence responses need to be forward compatible and requests need to be backward compatible Forward and backward compatibility properties of an RPC scheme are usually inherited from the encoding format they use Thrift gRPC Avro RPC SOAPs XML schemas RESTful API usually uses JSON without a formally specified schema where adding optional request param and adding to response are usually changes that maintain compatibility Messagepassing dataflow Asynchronous messagepassing systems are somewhere between RPC and databases A clients request message is delivered to another process by going through an intermediary called a message broker message queue or messageoriented middleware Compared with RPC a message broker can act as a buffer if the recipient is unavailable or overloaded thus improving reliability can automatically redeliver messages to a process that has crashed preventing messages being lost avoids the sender needing to know the IP address and port number of the recipient allows one message to be sent to multiple recipients logically decouples the sender from the recipient However a difference compared with RPC is that messagepassing is usually oneway a sender does not expect to receive a reply A reply is possible via a different channel and this communication pattern is asynchronous in that the sender doesnt wait for the message to be delivered but simply sends it and then forgets it Recently RabbitMQ Apache Kafka etc have become popular message brokers In general message brokers are used as follows one process sends a message to a named queue or topic and the broker ensures the message is delivered to one or more consumers of or subscribers to the topic There can be many producers and many consumers on the same topic Message brokers typically dont enforce any particular data model and you can use encoding format If the encoding is backward and forward compatible you have the flexibility to change producers and consumers independently and deploy them in any order The actor model is a programming model for concurrency in a single process Rather than dealing directly with threads race conditions locking etc logic is encapsulated in actors Each actor has some local states and communicates with other actors by sending and receiving asynchronous messages Message delivery is not guaranteed Distributed actor model uses this programming model on different nodes as there is less of a fundamental mismatch between local and remote communication when using the actor model This model essentially integrates a message broker and the actor programming model Some distributed actor model frameworks are Akka Javas builtin serialization Orleans Erlang OTP Summary Encoding their efficiency and evolvability impact Programming specificspecific encodings Textual formats like JSONXMLCSV Optional schema Can be vague about types Binary schemadriven formats like Thrift Protobuf Avro Modes of dataflow Via databases Writer process encodes reader process decodes Via RPC and RESTful APIs SOAP Client encodes request decodes response and the opposite for server Via asynchronous message passing using message brokers or actors Backwardforward compatibility and rolling upgrade are achievable with a bit care Deployment should be incremental and frequent Part II Distributed data Part I deals with a single node Part II deals with multiple where we may gain in scalability fault tolerance high availability and latency Problem with scaling vertically communication via shared memory using a more powerful machine is cost grows nonlinearly Fault tolerance and latency are also issues with a single powerful node Another approach is a shared disk architecture but contention and overhead of locking limit its scalability Share nothing architecture horizontally scaling are not necessarily the best solution for everything while having advantages in cost of scaling high availability and low latency distributed to near where clients are they usually add complexity for applications and sometimes limits the expressiveness of data model Two common ways of distributing data across nodes Replication Same copy of data in different locations Provides redundancy and helps improve performance Partitioning sharding Split a big dataset into smaller ones to be stored separately Chap 5 Replication Reasons for replication keep your data geographically close to your users allow the system to continue working even part of it have failed to scale out the number of machines that can serve read queries thus increase read throughput This chapter assumes your dataset is small enough to live in one machine The complexity in replication lies in handling changes to replicated data Algorithms for replicating changes across nodes singleleader multileader and leaderless Leaders and followers To ensure change gets to all replicas the most common solution is leaderbased replication active passive master slave replication One of the replicas is the leader master primary all write requests must go through the leader which first writes the new data to its local storage Whenever leader writes to its local storage it sends the change to all of its followers read replicas slaves secondaries hot standbys as a part of replication log or change stream Each follower updates accordingly by applying the changes in the same order as they were processed by the leader a client read can be handled by the leader or any of the followers This mode of replication is builtin for many relational DBs MySQL PostgreSQL etc and some nonrelational DBs MongoDB Espresso etc This is not limited to distributed DBs Kafka and RabbitMQs high availability queues also use it Synchronous and asynchronous replication Synchronous the leader wait for a follower to confirm it received the write before reporting success to the user and before making the write visible to other clients Advantage the follower is guaranteed to have an uptodate copy of the data that is consistent with leader If leader suddenly fails data is still available on the follower Disadvantage the write cannot be processed if the follower does not respond and the leader has to block all writes Its impractical for all followers to be synchronous Asynchronous leader does not wait for follower response before telling the user Usually if you enable synchronous replication on database its semisynchronous where one of the followers receive synchronous updates and all others async If the synchronous followers becomes slow one of the async followers is made sync This guarantees uptodate data on at least two nodes In practice leaderbased replication is often configured to be full async where a write even if confirmed by the leader are not guaranteed to be durable Setting up new followers To spin up a new follower full copy usually doesnt work as data is being written as we copy while locking the DB means lowering availability Usually we take a snapshot of the leaders DB at some point copy this snapshot over and the new follower then requests all the changes that have happened since the snapshots taken until it fully catches up This requires the snapshot being assoicated with an exact location in leaders replication log known as log sequence number binlog coordinates Handling node outages Followerfailure catchup each follower keeps a log of data changes it has received from leader When a follower crashes it picks up the last processed transaction from this log and request all the data changes since that Leaderfailure failover one of the followers needs to be promoted leader and clients need to be reconfigured to send their writes to the new leader and other followers need to start consuming data changes from the new leader Determining leader failure usually relies on heartbeat Electing a new leader is a consensus problem Usually the replica with most uptodate data changes from the leader is chosen to minimize data loss Reconfiguring the system to follow the new leader and if old leader comes back ensure it becomes a follower to the new leader Failover is fraught with things that can go wrong With asynchronous replication there can be writes from the leader before it failed that are not in the new leader We could discard those unreplicated writes but this would violate clients durability assumption Discarding writes is especially dangerous if other storage systems outside of the DB need to be coordinated with database content In some fault scenarios it could happen that two nodes believe they are the leader As a safety catch some systems shut one down in this case these need to be carefully designed as well so as to not shut both down deciding the right heartbeat timeout for a leader to be considered dead Implementation of replication logs Statementbased replication In a relational system each INSERT UPDATE or DELETE is forwarded to followers This has problems with calling nondeterministic function like NOW and RAND if statements depend on existing data in the column they must be executed in exactly the same order on each replica statements with side effects triggers stored procedures userdefined functions may result in different side effects on different replica unless side effects are deterministic there are workarounds but other replication methods are usually preferred Writeahead log shipping in LSM trees this log is the main place for storage in Btrees this is the writeahead log for restoring to a consistent state after a crash we can use the same log for replication leader appends to its own log and also sends it across the network to followers main disadvantage is log describes data on a very low level like which bytes were changed in which disk blocks this makes replication closely coupled to the storage engine Its typically not possible to run different versions of the database software on the leader and followers this advantage has a big operational impact upgrade requires downtime If this allows followers to run a newer version than the leader then zero downtime upgrade can be achieved by upgrading some followers then perform a failover Logical rowbased replication uses a different log format for replication than the log of the storage engine The formers a logical log while the latters a physical log logical log for a relational DB is usually a sequence of records describing writes to tables at the granularity of a row insert contains new values of all columns delete cotntains the primary key or if no primary key old values of all columns update contains the primary key or enough info to uniquely identify a row and the new values of all columns a transaction that modifies several rows generates several such records followed by a record to indicate a commit MySQL binlog uses such this higherlevel log allows leader and follower to run different storage engines this log is easier for external applications to parse eg transcribing data to a warehouse or for building caches custom indexes This is called change data capture Triggerbased replication the above are replications implemented by the database system Sometimes you want more flexibility as an application to eg replicate a subset of the data Then replication then may be moved up to application level or use triggers and stored procedures available in many relational DBs which lets you register custom application code executed automatically when data changes This usually has more overhead is more errorprone but more flexible Problems with replication lag Replication helps with availability read throughput and latency In a system with few writes and lots of reads one leader and many followers may seem ideal though this system can only replicate asynchronously meaning a client reading from an asynchronous follower may see outofdate data reading at the same time from leader and a follower may give different results transiently Without writes eventually they should converge and this is known as eventual consistency where eventual is vague and can be arbitrarily long Read your own writes Readyourwriteconsistency readafterwriteconsistency is a guarantee that user will always see any updates they submitted themselves To implement readafterwriteconsistency we could when reading something that the user might have modified read it from leader otherwise read from follower This requires your system to be able to identify what the user can modify eg profile of that user on a social media page or track the time of last update and for a certain time based on replication lag after the last update make all reads from the leader or client can remember the timestamp of its most recent write then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp and switch to a different replica or wait if that replica does not have this timestamp This can be a logical timestamp that indicates the ordering of writes or system clock where clock synchronization becomes critical Another complication arises when the same user is accessing your service from multiple devices Timestamp remembering becomes more difficult this metadata needs to be centralized and known across devices If your replicas are distributed across different data centers the users multiple devices may connect to different data centers If your approach requires reading from the leader you may first need to route requests from all of a users devices to the same data center Monotonic reads Another anomaly with asynchronous replication is user can see things moving back in time Monotonic reads is a guarantee this does not happen a guarantee stronger than eventual and weaker than strong consistency One way to achieve this is make the user read always from the same replica eg using a hash of the UserID rather than randomly However this needs to handle rerouting when that replica fails Consistent prefix reads A third anomaly with asynchronous replication is violation of causality Consistent prefix reads is a guarantee this does not happen which says if a sequence of writes happens in a certain order then anyone reading those writes will see them appear in the same order This is a particular problem in sharded databases as if the database always applies writes in the same order reads always read a consistent prefix and this cannot happen But in many distributed databases different partitions operate independently so there is no global ordering of writes One solution is to make sure any writes causally related to each other are written to the same partition but in some applications that cannot be done efficiently When working with an eventually consistent system its important to consider what if replication lag gets long and what kind of consistency guarantee you need to provide to users Application code can manually perform some operations on the leader to provide a stronger guarantee than the underlying DB but this is error prone and complex and itd be ideal if application doesnt need to worry about consistency and can trust their DB to do the right thing This is why transactions exist they provide stronger guarantees from the database such that applications can be simpler When moving away from a single node to multi node many dropped transactions claiming they are too expensive or hurts availability too much and eventual consistency is the only guarantee This is true to some extent but an oversimplified statement Multileader replication A multileader configuration mastermaster activeactive replication allows more than one node to accept writes Each leader in this setup simultaneously acts as a follower to other leaders It rarely makes sense to use a multileader setup in a single datacenter as the benefits rarely outweighs the added complexity With multiple data centers this can serve as an alternative to having all your writes go through a leader in one data center Perceived performance may be better in this case as user writes can be handled by the data center closest to them and the interdatacenter network delay is hidden from them Having all writes go through one node may defeat the purpose of multidata center This can tolerate data center outage This is more tolerant of network problems as interdata center connections usually go through the Internet and is much more error prone than local network within a data center a temporary network outage does not prevent writes from being made Some databases support multileader configuration by default but it often implemented by external tools over commercials DBs Multileader comes with a big downside of the same data can be concurrently modified in two different data centers and those writes have to have their conflicts resolved Multileader can be quite dangerous due to configuration pitfalls and surprising interaction with other DB features such as autoincrementing keys triggers and integrity constraints Clients with offline operation support is essentially a multileader replication as when offline the clients local DB acts as a leader that can accept writes Realtime collaborative editing such as Google Doc poses a similar problem where the user edits can be written to the local web browser acting as a leader You could make a user wait until another has finished editing holding a lock which would be similar to singleleader replications with transactions on the leader For faster collaboration youll want to make the unit of change very small and avoid locking which brings the challenges of multileader replication including requiring conflict resolution Handling write conflicts Imagine two users modifying the same thing at the same time in a singleleader system the second write ordering is deterministic can be either blocked or rejected while the first is ongoing You could make the replication synchronous to handle this but this breaks the main advantage of multileader replication allowing each replica to accept writes independently Conflict avoidance Best way to deal with conflicts is to avoid them if the application can ensure all writes for a particular record go through the same leader the social media profile example then conflicts cannot occur In some cases you have to change the designated leader for a record due to datacenter failure or user having moved in which case concurrent writes on different leaders needs to be dealt with again Converging towards a consistent state In a multileader setup if each leader were to just apply writes in the order they receive them eventual consistency cannot be guaranteed in that writes can have different orders getting to different leaders Well want a convergent way which can be achieved with give each write a unique ID random number timestamp UUID hash etc and pick the write with the highest ID as the winner If timestamp is used this is known as last write wins a popular approach but prone to data loss give each replica a unique ID and let writes originated at a highernumbered replica always take precedence This also implies data loss somehow merge them eg order them alphabetically then concatenate record the conflict in an explicit data structure that preserves all information and write application code that resolves the conflict at some later time perhaps by prompting the user Custom conflict resolution logic Conflict resolution in multileader systems are often applicationspecific and these systems would let application supply their custom logic for conflict resolution which gets executed either onwrite or onread In latters case all conflicting versions are written and given to the reader the next time they are read the reader resolves automatically manually and the result gets written back Note that conflict resolution usually applies at the level of an individual row or document not for an entire transaction A transaction with several writes usually have each of these writes considered separately for conflict resolution Conflict resolution is errorprone Amazon is often cited as having surprising effects due to this This inspired research in CRDT conflictfree replicated datatypes a family of data structures for sets maps ordered lists counters etc that can be updated concurrently by multiple users and automatically resolve conflicts in reasonable ways Mergeable persistent data structure tracks history explicitly similarly to git and uses a threeway merge function whereas CRDT uses two way merge Operational transformation is the algorithm behind Google docs designed particularly for concurrent editing of an ordered list of items such as the list of characters that constitute a document Multileader replication topologies With more than two leaders various replication topologies are possible circular star alltoall etc In circular and star a replication passes through several nodes before reaching all replicas Nodes forward data and to prevent infinite replication loops each node is given a unique identifier and in the replication log each write is tagged with the identifiers of all nodes it passed through and a node wont apply changes that are already tagged with its own tag Node failure in a circular or star topology interrupting replication flow is also a bigger issue than in a more densely connected topology Alltoall topologies may have issues with some replication messages overtaking others like causality being violated because different paths gets causally related messages over at different times Simply adding a timestamp wont fix it as timestamp cannot be assumed to be in sync all the time To order these correctly versionvector can be used Many multileader replication arent implemented carefully and its worth checking your DBs docs and test to ensure it actually provides the guarantees you believe it to have Leaderless replication Dynamo is an example of this where a client can write to any nodes Riak Cassandra are opensource leaderless replication systems inspired by Dynamo There is no concept of failover in a leaderless replication system The client writes to multiple replicas and as long as enough number of replicas returned success the write is considered successful by the client The client also reads from multiple replicas and in parallel and version numbers are used to decide which value is newer When an unavailable node comes back online to get uptodate data to it we could use read repair where a client makes a read from several nodes in parallel and detect stale response in which case they write the newer value back to that relica This works well for values that are frequently read Many data stores also have an antientropy process that constantly looks for differences in data between replicas and copies over missing data from one replica to another This may have signifcant delays Quorums for reading and writing If there are n replicas every write must be confirmed by w to be considered successful and we must query at least r nodes for each read As long as w r n we expect to get uptodate value when reading because at least one we read from must be up to date Reads and writes that obey this are called quorum reads and writes A typical setup is to let n be an odd number and r w be n2 rounded up When fewer nodes returned success the read or write operation returns error Lowering w r below n will make you likely to read stale values but allows higher availability and lower latency Although quorums appear to guarantee that a read returns the latest written value in practice its not so simple due to edge cases Dynamosstyle databases are generally optimized for use cases that can tolerate eventual consistency and r w shouldnt be taken as guarantees In particular you usually dont get readyourwrites monotonicreads consistentprefixreads as they require transactions or consensus Monitoring staleness For leaderbased replication because leaders and followers apply the write in the same order you can typically monitor the amount of replication lag In leaderless replication there is no fixed order in which writes are applied making monitoring more difficult Eventual consistency is a deliberately vague guarantee but for operability its important to be able to quantify eventual Sloppy quorums and hinted handoff Leaderless databases with appropriately configured quoroms can tolerate failures without failover as well as slowness since as long as w r nodes returned the operation succeeds This makes them appealing for highavailability lowlatency workload and one that can occasionally tolerate stale reads This scheme as described is not tolerant to network partition during which its likely fewer than w or r reachable nodes remain and a quorum cannot be reached Designers face a tradeoff in this case block off all reads writes or let writes proceed anyway to nodes that are reachable but arent among the n nodes on which they value usually lives The latter is sloppy quorum reads and writes still require r and w successful responses but those may include nodes that arent among the designated n home nodes for a value Once the partition is fixed any writes that one node temporarily accepted on behalf of another are sent to the appropriate home nodes this is called hinted handoff Sloppy quorum is particularly useful for increasing write availability the database can accept writes as long as any w nodes are available This also means even when w r n you cannot be sure to read the latest value for a key as the value may have been temporarily written to some nodes outside of n Hence sloppy quorum isnt a quorum but a durability assurance Sloppy quorum are optional in all common Dynamo implementations Riak Cassandra Voldemort Multidata center operation Leaderless replication is also useful for multidatacenter operation since it is designed to handle conflicting concurrent writes network interruptions and latency spikes Cassandra usually have the number of replicas n in all data centers each write is sent to all replica but only w needs to acknowledge and these are usually all from local data centers so the client is unaffected by interdata center links Detecting concurrent writes Dynamostyle DBs allows several clients to write to the same key and conflicts can occur during writes read repair or hinted handoff If each node simply overwrote the value for a key whenever it received a write request from a client we could end up in an eventually inconsistent state The replicas should converge towards some value and if you as the application developer want to avoid losing data you usually need to know a lot about the internals of your databases conflict handling Last write wins Concurrent writes dont have a natural ordering so we force arbitrary orders on them eg attach a timestamp to each write and pick the biggest timestamp as the most recent This conflict resolution LWW is the only support resolution in Cassandra LWW achieves eventual convergence but at the cost of durability It may even drop writes that are not concurrent due to time drift If losing data is unacceptable LWW is a poor mechanism for conflict resolution the only safe way to use LWW is to ensure a key is written only once and immutable thereafter Eg a recommended way of using Cassandra is to use a UUID as the key thus giving each write operation a unique key Happensbefore relationship and concurrency How to decide if two writes are concurrent an operation A happens before another operation B if B knows about A or depends on A or builds upon A in some way Whether one operation happens before another is the key to defining what concurrency means Two can be said to be concurrent if neither knows about the other Note that concurrent here does not necessarily mean happening at the same physical time as with clocks in distributed system its usually quite difficult to tell two things happening at the same time From a physics perspective if information cannot travel faster than the speed of light then two events sufficiently far away from each other cannot possibly affect each other if the time delta of them happening is lower than the time light would take to propagate from one location to another Capturing happensbefore Imagine a single server with multiple writers Server maintains a version number for every key increments the version every time that key is written and stores the new version number along with the value written When a client reads a key the server returns all values that have not been overwritten as well as the lastest version number A client must read a key before writing When a client writes a key it must include the version number from the previous read as an indication of what Ive already seen and it must merge together all the values it received in the prior read When the server receives a write with a particular version number it can overwrite all values with that version number or below since it knows that they have been merged into the new value but it must keep all values with a higher version number because those values are concurrent with the incoming write Essentially when a write includes the version number from a prior read that tells us which previous state the write is based on Merging concurrently written values The above algorithm ensures nothing is silently dropped but it unfortunately requires the clients to do extra work to merge the concurrently written values siblings before writing another Merging siblings is essentially the same problem as conflict resolution in multileader replication as discussed before You could merge by taking one value based on a timestamp losing data take a union of all siblings or if you allow clients to also remove union wont do the right thing and the system must leave a marker tombstone with the appropriate version number to indicate that the item has been removed when merging siblings As merging siblings in application code is complex and errorprone some data structures try to perform this automatically eg CRDTs Version vectors Scaling the algorithm to multiple replicas we need to use a version per key as well as per replica Each replica increments its own version number when processing a write while also keeping track of version numbers it saw from other replicas This indicates what to overwrite and what to keep as siblings The collection of version numbers from all the replicas is called a version vector they are sent from the replica to the client when read and sent back to the database when a value is subsequently written Version vector allows the DB to tell which writes are causal and which are concurrent Riaks dotted version vector is probably the most used which it calls a causal context Similar to the singlereplica example the application may need to merge siblings Summary Replication can serve these goals high availability disconnected operations lower latency scale better Three major approaches to replication singleleader write all goes to leader read can be served from any Reads might be stale Easy to understand and implement multileader write sent to one of several leaders leaders propagate changes to each other and followers More robust but harder to reason and provides only weak consistency guarantees leaderless read and write both from several nodes to detect and correct nodes with stale data The effects of replication lag and different consistency models eventual readyourwrite readmonotonic consistentprefixreads In multileader and leaderless system ways to reconcile write conflicts LWW versionvectors based merge Chap 6 Partitioning A partition in this chapter is called a shard in MongoDB Elasticsearch SolrCloud a region in HBase a tablet in Bigtable a vnode in Cassandra and Riak and a vBucket in Couchbase In effect each partition is a small database of its own although the database may support operations that touch multiple partitions at the same time The main reason for wanting partitioning is scalability Large complex queries can potentially be parallelized across many nodes Different partitions can be placed on different nodes in a shared nothing cluster The fundamentals of partitioning apply to both kinds of workloads Partitioning and replication Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes Even though each record belongs to exactly one partition it may still be stored on multiple different nodes for fault tolerance Each partitions leader is assigned to one node and each node can be the leader for some partitions and a follower for other partitions The choice of partitioning scheme is mostly independent from the choice of replication scheme so this chapter ignores replication Partitioning of KeyValue data Goal spread the data and the query load evenly across nodes A skewed partitioning makes partitioning less effective in that some partitions serve more than others hot spot We can randomly assign which is unideal in that we dont know which node a particular queried item is on and we have to query all nodes in parallel Partitioning by key range Partitioning by a sorted key is used by Bigtable its open source equivalent HBase and earlier MongoDB With each partition we keep keys in sorted orders advantage being range scans are easy downside being certain access patterns can lead to hot spots think a process keeps writing real world clock time sensor readings to a timestamp partitioned database In this case you need something other than timestamp as the first element of the key eg the sensor name Now if you want all sensor levels over a time range multiple queries are needed Partitioning by key hash A good hash function takes skewed data and makes it uniformly distributed For partitioning purposes they need not be cryptographically strong MD5 murmur3 FowlerNollVo functions are all used Each partition would now serve a range of hashes as opposed to keys This technique is good at distributing keys fairly among partitions Boundaries are evenly spaced or chosen pseudorandomly Consistent hashing uses randomly chosen partition doundaries to avoid the need for central control or distributed consensus This approach actually doesnt work very well for databases so its rarely used in practice Using hash means the sorted order of keys is lost Range queries on primary keys are not supported by Riak Couchbase or Voldemort and in MongoDB enabling hashbased partitioning means range queries will be sent to all partitions Cassandra does a compromise between the two partitioning strategies a table can be declared with a compound primary key of several columns the first part of that key is hashed to determine the partition and the others are used as a concatenated index for sorting data in Cassandras SSTable A query can therefore specify the partitioning key and do range queries on the other columns with a fixed partitioning key This enables an elegant data model for onetomany relationships like a social media site where a user has many updates the partitioning key is the user ID and we could then query the users feed within a time range Skewed workloads and relieving hot spots Think of unusual cases of a celebrity on social media with user ID hashbased key all their query would still fall onto the same partition Todays DB typically doesnt handle this automatically application can reduce the skew if it knows one key to be very hot by appending a random number to the key 1 digit gives you 10 partitions with downsides being read now needs to do additional work to read from all partitions This also requires additional bookkeeping Partitioning and secondary indexes If data is over accessed by primary key we can determine the partition from that key The problem is more complicated with secondary indexes involved A secondary index usually doesnt identify a record uniquely but rather is a way of searching for occurrences of a particular value Secondary indexes are the bread and butter of relational DBS and they are common in document databases too Many keyvalue stores such as HBase and Voldemort have completely avoided them some started adding them and they are the raison detre of search servers such as Solr and Elasticsearch The problem is they dont map neatly to partitions and two main approaches are documentbased partitioning and termbased partitioning Documentbased partitioning Each partition maintains its own secondary indexes covering only documents in that partition ie a local index eg map each secondary keys value to the primary keys of records living in this partition So querying by secondary index would mean sending a query to all partitions and combine the results you get back This approach to querying a partitioned database is sometimes known as scattergather and it can make read queries on secondary indexes quite expensive Despite this it is widely used in Cassandra MongoDB Riak Elasticsearch and SolrCloud Termbased partitioning Rather than having local indexes as above we can a global index that covers all data in all partitions and partition that global index to different nodes by key range or hash The term fulltext indexes we are looking for decides which partition the global index is on we read the index from that partition and figure out the primary keys of records we need to query Reads now only needs to request from partitions containing the term it wants as well as the global index but writes are slower as a writing to a single document may now affect multiple partitions of the index every term in the document might be on a different partition This would also require a distributed transaction across all partitions affected by a write to keep data and indexes on different partitions in sync which many dont support In practice updates to a global secondary indexes are often asynchronous as in Dynamo Rebalancing partitions Data size change machine failure etc all calls for moving data from one node to another a process called rebalancing Requirements for rebalancing load should be shared fairly between nodes while rebalancing the DB should continue accepting reads and writes no more data than necessary should be moved when rebalancing Hash mod n This makes rebalancing expensive in terms of data that has to be moved around hence earlier the hashbased partition lets each node store a hash range Fixed number of partitions Create much more partitions than there are nodes assign multiple partitions to the same node If a new node is added the node can steal a few partitions from every existing node until partitions are fairly distributed again The number of partitions or the mapping from keys to partitions dont change the only thing that changes is the mapping of partitions to nodes While a rebalancing is ongoing the old node that this partitions is on continues serving read and write requests This approach is used by Riak Elasticsearch etc For simplicity some of these DBs dont implement splitting a partition so the number configured initially is the max number of partitions you are going to have which should be larger than your number of nodes You should then choose a number high enough to account for future growth but not too high as each partition has management overhead Dynamic partitioning For keyrange partitioning DBs a fixed number of partitions with fixed boundaries can be very inconvenient For this reason HBase creates partitions dynamically it splits and merges in process similar to Btree nodes After a split one half is transferred to another nodes and in case of HBase the transfer happens through HDFS the underlying distributed file system Advantage is the number of partitions adapts to the volume of data To not start out from one single partition empty DB HBase and MongoDB allow an initial set of partitions to be configured on an empty DB This requires you to know what the key distribution is going to look like Dynamic partitioning can be applied for key range partitioned data as well as hashpartitioned data Partitioning proportionally to nodes With dynamic partitioning the number of partitions is proportional to the size of the dataset as split and merge keep the size of each partition between some fixed min and max With fixed number of partitions the size of partitions is proportional to the size of the dataset In both these cases the number of partitions is independent of the number of nodes Cassandra makes the number of partitions proportional to the number of nodes ie to have a fixed number of partitions per node In this case the size of each partition grows proportionally to the dataset size while the number of nodes remains unchanged but when increasing the number of nodes each partition becomes smaller Since a larger data volume generally requires a larger number of nodes to store this also keeps the size of each partition fairly stable When a new node joins the cluster it randomly chooses a fixed number of existing partitions to split and takes ownership of one half of each of those split partitions while leaving the other half in place This introduces unfair splits but averaged over a large number of partitions the new node ends up taking a fair share of load Picking partition boundaries randomly requires hashbased partitioning so the boundaries can be picked from the range of numbers produced by the hash function Manual or automatic rebalancing Fully automated rebalancing can be convenient due to less operational work but can be unpredictable in that rebalancing is expensive and if not done carefully this can overload the network and harm the performance while rebalancing is in progress This could create cascading failure when used in combination with automatic failure detection Detect an overloaded node to be slow decides to move data away from it and further overloading that node Request routing How does a node know which partition to request from This is an instance of a more general problem called service discovery Several highlevel approaches Allow clients to contact any node via a roundrobin load balancer eg if that node does not own the partition it forwards the request to the appropriate node receives a reply and passes that on Send requests from clients to a routing tier first which determines the node to handle the request and forwards it The routing tier acts as a partitionaware load balancer Require clients be aware of the partitioning and the assignment of partitioning to nodes a client can connect directly to the appropriate node without any intermediary In all cases the key problem is how does the routing decision component learn about changes in partition assignment Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata Each node registers itself with ZooKeeper which maintains the authoritative mapping of partitions to nodes The routing component subscribes to ZooKeeper and gets a notification when a change in partition happens HBase SolrCloud and Kafka use ZooKeeper to track partition assignment MongoDB uses similar architecture but relies on its own config server implementation Cassandra and Riak take a different approach which uses a gossip protocol to disseminate any changes in cluster state Requests can be sent to any node and that node forwards them to the appropriate node for the requested partition This model puts more complexity in the database nodes but avoids the dependency on an external coordination service such as ZooKeeper Couchbase does not rebalance automatically which simplifies the design When using a routing tier or sending requests to a random node clients still need to find the IP addresses to connect to this usually isnt fast changing and DNS works just fine Parallel query execution The above focused on very simple queries that reads or writes a single key or scattergather in the case of working with documentpartitioned secondary indexes This is about the level of access supported by most NoSQL distributed data stores Massively parallel processing often used for analytical workload is much more sophisticated in the types of queries they support The query optimizer breaks a complex query into parallel stages Summary Goal of partitioning scalability spread the load evenly avoid hotspots Two main approaches Key range partitioning Efficient range queries risk of hot spots Hash partitioning Each partition owns a range of hashes Inefficient range queries more even data distribution This is often used in combination with fixed number of partitions although dynamic partitioning can also be used Or a hybrid of the two like the compound key in Cassandra To support secondary indexes Documentpartitioned indexes local indexes Secondary index stored in the same partition as the primary key and value A single partition to update on write but a read of the secondary index requires a scattergather across over all partitions Termbased indexes global indexes When write several partitions will need to be updated however a read can be served from a single partition Rebalancing strategies Routing techniques By design each partition operates mostly independently which allows a partitioned database to scale to multiple machines Chap 7 Transactions Many things can go wrong in a distributed data system failing in the middle of a write may crash at any time network can partition several clients can overwrite each others changes may read data thats partially written race conditions etc Transactions have been the mechanism of choice for simplifying these issues they simplify the programming model for applications accessing a DB Transactions group a series of reads and writes into one logical unit to be executed as one the entire batch either succeeds or fails Transactions make application not need to worry about partial failures Not every application needs transactions This chapter discusses read committed snapshot isolation and serializability These concepts apply to distributed storage as well as single node Transactions have been the main casualty of the noSQL movement There emerged a popular belief that transactions are the antihesis of scalability This is not necessarily true ACID The safety guarantees provided by transactions are described by ACID atomicity consistency isolation durability In practice one databases implementation of ACID does not equal anothers Eg isolation can be quite ambiguous Systems not meeting ACID vague are sometimes called BASE basically available soft state eventual consistency an even vaguer term Atomicity atomic operation in the context of multithreaded program means another thread cannot see the the halffinished operation of another In ACID atomicity does not have to do with multiple processes trying to access data at the same time that is isolation In ACID atomicity means if among several writes one fails at some point the database must discard or undo any writes it has made so far in that transaction Consistency is a terribly overloaded term Replica consistency refers to eventual consistency readafterwrite monotonicread consistentprefixread Consistent hashing is an approach to partitioning some systems use for rebalancing CAP theorem consistency means linearizability ACID consistency means an applicationspecific notion of database being in a goodstate ACID consistency means some invariants about your data must always be true like in an accounting system credits and debits must balance This then becomes an applicationspecific definition The application may rely on the DBs atomicity and isolation properties to achieve consistency this is not up to the DB alone and C from ACID should be tossed ACID isolation means concurrently executing transactions are isolated from each other The classic database textbooks formalize isolation as serializability meaning each transaction can pretend its the only transaction running on the DB though in reality there might be several running at the same time ACID durability is a promise that data a transaction wrote successfully will not be forgotten It usually involves writing to hard drive as well as a writeahead log for recovery and a DB reporting a transaction as successful only after successful log write Perfect durability reliability does not exist there are riskreduction techniques but take any theoretical guarantee with a grain of salt Durability historically meant writing to a disk but now has been adapted to mean replication Single and multiobject operations Atomicity and isolation describe what should happen if clients make transactions allornothing concurrent transactions shouldnt interfere with each other another transaction should not see half written results of this transaction This usually requires some way to tell which reads and writes are in the same transaction relational database uses begin transaction and commit Many nonrelational DBs dont have a way of grouping operations together even with a multiobject API in the same statement it doesnt necessarily mean that statement guarantees allornothing Atomicity and isolation apply to singleobject writes as well imagine a disk failure when halfway through writing a large object What does the storage engine guarantee in this case Itd be very confusing if no guarantees are provided so storage engines typically provide atomicity and isolation on the level of a single object on one node which can be implemented with a writeahead log atomicity and a lock on each object isolation Some databases also provide an atomic increment operation and a compareandset operation These singleobject atomicity and isolation guarantees arent the usual sense of ACID A and I they usually refer to grouping multiobject modifications into one unit of execution In many cases multiple objects need to be coordinated updates with foreign keys in relational model updating denormalized fields in document model and updating secondary indexes Implementing these without transactions is possible but error handling is made difficult A key feature of a transaction is that it can be aborted and safely retried if an error occurred Although retrying an aborted transaction is a simple and effective error handling mechanism it isnt perfect in that transaction may have actually succeeded but network failed to deliver the success message Youll be redoing the transaction in this case which would require an applicationlevel deduplication mechanism if error is due to overload retrying is going to make matters worse without reasonable exponential backoff number limits its only worth retrying after transient errors retrying after permanent error is pointless if the transaction is not side effect free the side effect could happen twice if a client fails while retrying what its trying to write is lost Weak isolation levels Race conditions come into play when one transaction reads writes data that is concurrently modified by another transaction Concurrency can be difficult to reason about and debug For this reason databases have long tried to hide concurrency issues from application developer by providing isolation Serializable isolation is a guarantee that transactions have the same effect as if run one at a time This comes at a performance price one which many DBs dont want to pay Read committed Read committed is the most basic level of transaction isolation It makes two guarantees when reading from DB you will only see data that have been committed no dirty reads when writing to the DB you will only overwrite data that has been committed no dirty writes Read committed does not prevent eg two transactions incrementing the same counter but the counter ends up being incremented only once first read second read both get the same value and did not dirty read first write incremented commit then second write incremented commit the later commit did not dirty write Most commonly databases prevent dirty writes by using rowlevel locks modifying a row document requires holding a lock on it and it must then hold the lock until the transaction is committed or aborted No dirty read can be enforced by having readers acquire the same lock while they read this affects performance And instead most databases prevent dirty reads by remembering the old committed value and the new value set by the transaction currently holding the write lock and while write transaction is ongoing it returns the old committed value to readers Snapshot isolation and repeatable reads Imagine a user transferring money between her 2 bank accounts one transaction does two writes acct1 100 1 acct2 100 2 commit another transaction does two reads of acct1 3 and acct2 4 commit with read committed isolation if we have sequence 3 1 2 4 the read is going to see acct1 without the 100 and acct2 with the 100 Such a read is unrepeatable or read skew in that doing the read transaction again after the write is committed will give the expected result Snapshot isolation is the most common solution idea being each transaction reads from a consistent snapshot of the database meaning the transaction sees all the data that was committed in the DB at the start of the transaction even if the data is subsequently changed by another transaction each transaction sees only the old data from the particular point in time This makes integrity checks possible which would otherwise be difficult with just read committed isolation Implementing snapshot isolation also uses a writer lock but read does not require locks Read is built upon a generalized approach in read committed as the DB must potentially keep several different committed versions of an object as various inprogress transactions may need to see the state of the DB at different points in time This is known as multiversion concurrency control MVCC Still readers dont block writers and writers dont block readers Each transaction is usually given a unique increasing ID and each write is tagged with transaction ID created by transaction X deleted by transaction Y An update is delete creates Deleted arent immediately gone but garbage collected later When a transaction reads the transactio IDs are used decide which objects are visible essentially a long enough history of writes to the object and find the latest committed point in history that was before your read transaction At the start of each transaction DB makes a list of all other transactions in progress at the time Any writes those transactions have made are ignored even if they become committed later Any writes made by aborted transactions are ignored Any writes made by transactions with a later transaction ID started after the current transaction are ignored All other writes are visible to the applications queries How do indexes work in a multiversion DB We could have it point to all versions of an object and require an index query to filter out versions not visible to the current transaction With a Btree implementation indexing multiple versions could look like a appendonlycopyonwrite Btree that does not overwrite pages of the tree when updated but creates a copy of each modified page Parent pages up to the root of the tree are copied to point to the new versions of child pages Pages not affected by a write need not be copied Every write then creates a new Btree root and a particular root is a consistent snapshot of the database at the point in time when it was created This approach requires a background process for compaction and garbage collection Repeatable read can be a confusing term in SQL standards Some use repeatable reads to refer serializability Preventing lost updates Dirty write is only one type of write conflicts that can happen Another is the lost update problem as illustrated in the incrementing two counters case an readupdatewrite cycle parsechangewriteback a json object two users editting the same wiki at the same time etc Atomic write operation is one solution which removes the need of readupdatewrite cycles in application code which are usually the best if your code can be expressed in such Mongo supports atomic operations for making local modifications of a json document and redis provides atomic operation to update a data structure like priority queue They are usually implemented with an exclusive lock on the object such that when read other reads are also blocked Another option is to force all atomic operations on a single thread Explicit locking is another approach if the DBs builtin atomic operations dont provide the needed functionality This works but can be hard to get right Automatically detecting lost updates as opposed to forcing serial like in previous approaches this allows parallel and tries to detect lost update and when detected forces one transmission to abort and retry This check can be performed efficiently in conjunction with snapshot isolation Some DBs instead provide a compareandset operation this avoids lost updates by allowing an update to happen only if the value has not changed since you last read it If current value does not match what you previously read this forces an abort and the readmodifywrite cycle has to be retried In a replicated scenario lost updates can happen on different nodes Locks and compareandset assume there is a single uptodate copy of the data which cannot be guaranteed in a multileader leaderless replication Instead they allow concurrent writes to create siblings and use application code or special data structures to resolve and merge them Atomic operations can work well in a replicated context especially if they are commutative they can be applied in different orders and get the same result Write skew and phantoms Imagine you have a hospital where at least one person has to be present oncall and a person can give up oncall if at least there is another oncall Now the only two persons oncall update their individual records to give up oncall at the same time check if there is another person oncall using a countselect on all if so give up its own oncall and the system could end up with 0 persons oncall This is a write skew not a dirty write or lost update since the two transactions are updating two different objects This can be thought of a generalization of the lost update problem two transactions read the same objects then update some objects different in this case same in the case of dirty writes or lost updates Automatically preventing write skew requires true serializable isolation Some DB provides constraints foreign keys constraints or restrictions on a particular value Most dont have support for constraint involving multiple objects but one may be implemented with materialized views or triggers Without true serializable isolation level your best option is to explicitly lock all the rows the transaction depends on Enforcing two users cannot claim the same username in a snapshot isolation DB has the same problem fortunately unique constraint is a simple solution here where the DB will reject the second transaction All these examples follows a similar pattern readcheck conditionwrite where the write could change the condition checked in step 2 In the first example we can lock everything read in step 1 and in the second we are checking for absence and we cant attach a lock to anything An approach called materializing conflicts would have us create locks in advance for nonexistent objects This is errorprone and leaks a concurrency mechanism into application model The effect where a write in one transaction changes the result of a search query in another transaction is called a phantom Snapshot isolation prevents phantoms in readonly queries but readwrite transactions can still have phantoms that led to write skew Serializability Looking at some application code its hard to tell if its safe to run at a particular isolation level and there are no tools to help detect race conditions The simple answer has been use serializable isolation which was usually regarded as the highest isolation level Historically on a single node serializable isolation was implemented with actual serial execution two phase locking or optimistic concurrency control techniques such as serializable snapshot isolation Actual serial execution Remove concurrency entirely Only recently have designers decided a singlethreaded loop for executing transactions was feasible Two developments caused this RAM has become cheap enough that its often feasible to keep the entire dataset in memory executions become much faster due to no disk involvement DB designers realized OLTP reads and writes are usually small By contrast analytical workloads are usually large and readonly they can run a consistent snapshot outside of the serial execution loop Redis Datomic has support for this In order to make the most of the single thread transactions need to be structured differently from their traditional form Systems with singlethreaded serial transaction processing dont allow interactive multistatement transactions instead the application must submit the entire transaction code to the DB ahead of time as a stored procedure to prevent the single thread waiting on the back and forth network transmission cost of interactive queries in the same transaction every transaction has to be small and fast Stored procedure has existed for some time in relational databases and theyve been part of SQL for long Theyve a bad reputaion for different vendors using their own languages Oracle PL SQL server TSQL Redis Lua Datomic Java code running in a DB being difficult to manage test deploy integrate with a monitor system and a DB is often much more performance sensitive than an application server and a badly written stored procedure can mean more trouble Executing all transactions serially made concurrency control simpler but limits the transaction throughput to the speed of a single CPU core on a single machine this can be a bottleneck for a high write throughput system To scale to multiple CPU cores you can partition your data such that each transaction only needs to read and write data in one partition served by one CPU core For any transaction needing to access multiple partitions the database must coordinate the transaction across all the partitions it touches and the stored procedure needs to be performed in lock step across all partitions to ensure serializability across the system Data with multiple secondary indexes is particularly difficult Two phase locking For around 30 years there was only one algorithm widely used for serializability 2PL Note that 2PL is completely different from 2 phase commit 2PC To prevent dirty writes we have when two concurrent transactions trying to write the same object the lock ensures the second writer wait till the first one has finished or aborted before it may continue Two phase lockings lock requirement is much stronger Several transactions are allowed to read the same object concurrently but as soon as anyone wants to write an object exclusive access is required If transaction A has read an object and transaction B wants to write that object B has to wait till A commits or aborts before it can continue If transaction A has written an object and transaction B wants to read that object B has to wait until A commits or aborts before it can continue In 2PL writers dont just block other writers they also block readers and viceversa This captures the key difference with snapshot isolation where readers never block writers and writers never block readers MySQL and SQL server use 2PL to implement serializable isolation level Blocking of readers and writers is implemented by having a lock on each object in the DB the lock can be either in exclusive mode or shared mode To read an object the transaction has to acquire the lock in shared mode not owned by any exclusive To write to an object the transaction has to acquire the lock in exclusive mode not owned by any exclusive or shared If a transaction first reads then writes it may upgrade its shared lock to an exclusive lock a process that works the same way as getting an exclusive lock directly After a transaction has acquired the lock it must continue to hold the lock until the end of the transaction This is where the name twophase came from first phase is when the locks are acquired second phase where all locks are released at the end of a transaction Deadlock can happen with 2PL they can also happen in lockbased read committed isolation level but much more rarely in which case the DB automatically detects and abort one of the transactions The aborted is later retried by the DB Big downside of 2PL is performance due to locking overhead and reduced concurrency To prevent phantom writes we may need predicate locks which rather than belonging to a particular row in a table it belongs to all objects that match some search conditions If a transaction wants to read write an object matching the predicate it must acquire the shared exclusive predicate lock as well Same goes for transactions trying to insert delete Predicate locks apply even to objects that dont yet exist in the database If twophase locking includes predicate locks the DB prevents all forms of write skews and other race conditions making it serializable Predicate locks dont perform well instead most DB with 2PL implement indexrange locking a simplified approximation of predicate locks they lock a bigger range of objects than necessary by locking all objects associated with an index range but they have lower overheads If there are no suitable index where a range lock can be attached then the DB can fall back on entire table locking its safe but bad for performance Serializable Snapshot Isolation Serializable isolation and good performance seem at odds with each other SSI may be able to change that 2PL is a pessimistic concurrency control mechanism based on the principle that if anything might possibly go wrong its better to wait until the situation is safe again before doing anything Serial execution in a sense is pessimistic to the extreme By contrast SSI is an optimistic concurrency control technique Optimistic in that it lets transactions continue and hope everything will turn out alright and when a transaction wants to commit the DB checks whether anything bad happened If so one has to be aborted and retried This is an old idea and performs badly if there is high contention However if there is enough spare capacity and contention is not too high optimistic might be able to outperform pessimistic ones Contention can be reduced with commutative atomic operations when it doesnt matter which one is committed first Earlier the write skew happened due to trasaction having acted on an outdated premise query result might have changed we could detect these transactions and abort them There are two cases detecting reads of a stale MVCC object version uncommitted write occurred before the read To prevent this anomaly the database needs to track when a transaction ignores another transactions writes due to MVCC visibility rules When the transaction wants to commit the database checks whether any of the ignored writes have now been committed If so the transaction must be aborted detecting writes that affect prior reads the write occurs after the read An index keeps track of ongoing transactions that have read it and when a transaction writes to the database it must look in the indexes for any other ongoing transactions that have read the affected data It notifies those transactions the data they read may not be uptodate consequently they may or may not need to be aborted and retried To decide the granularity at which transactions reads and and writes are tracked there is the tradeoff between bookkeeping overhead and aborting more transactions than necessary The big advantage over 2PL is one transaction does not need to block waiting for locks held by another transaction In particular readonly queries can run on a consistent snapshot without requiring any locks which is appealing for readheavy loads Compared to serial execution SSI is not limited to the throughput of a single CPU core Serialization conflict detection can be distributed Transactions can read and write data in multiple partitioning while ensuring serializable isolation Performance SSI is affected by the rate of aborts and SSI requires readwrite transactions to be fairly short SSI is probably less sensitive to slow transactions than 2PL or serial execution Summary Transactions are an abstraction layer that allows an application to pretend that certain concurrent problems and faults dont exist a large class of errors is reduced down to a simple transaction abort and the application just needs to retry Isolation level read committed snapshot isolation repeatable read serializable Dirty reads dirty writes guaranteed by read committed Read skew guaranteed by snapshot isolation usually implemented with MVCC Lost updates some snapshot isolation implementation prevent this others require a manual lock SELECT FOR UPDATE Write skew readcheck premisewrite only serializable isolation prevents this anomaly Phantom read one transaction reads results matching a condition another writes that affects the results of the query Snapshot isolation can prevent straightforward phantom reads but phantoms in the context of write skew needs the likes of indexrange locks Only serializable prevents all these issues when using a weaker isolation level application needs additional logic eg explicit locking to protect against these Three ways to implement serializable actual serial execution 2PL pessimistic SSI optimistic Chap 8 The Trouble with Distributed Systems This chapter turns our pessimism to maximum and assume everything that can go wrong will go wrong except Byzantine failures Faults and partial failures An individual computer with good software is usually fully deterministic either fully functional or completely broken not something in between This is a deliberate choice in the design of computers if an internal fault happens we prefer it to crash completely rather than returning a wrong results because the latter are hard to deal with it hides away the physical reality on which they are implemented Distributed systems are completely different nondeterministic partial failures are possible which makes distributed systems hard to work with Two extremes of building largescale computing systems highperformance computer with thousands of cores or cloud computing usually with multitenant datacenters commodity computers connected with IP network and elastic resource allocation The first approach deals with errors usually with regular snapshotting If a node fails the entire cluster halts and recovers from a snapshot this is more like a singlenode approach of error handling This chapter focuses on the failure handling of the second type Difference being the second type of applications is often expected to be online in that they need to be available to serve users with low latency at any time Unlike a supercomputer where each node is rather reliable the commercial hardware has much higher error rates It becomes reasonable to assume at any time in point something is always broken and the system needs to be able to tolerate failed nodes useful also for rolling upgrade restarts uninterrupted services Network is often IP and Ethernet based arranged in Clos topologies to provide high bisection bandwidth Geographically distributed deployment where interdatacenter is usually slow and unreliable We build a reasonably reliable system from unreliable components think errorcorrecting codes reliability in TCP etc In distributed systems suspicion pessimism and paranoia pay off Unreliable networks Sharenothing architecture not the only way but by far predominant Loss packet switched network Delay loss one end temporarily stop responding etc are all possible The sender cant tell whether the packet was delivered with a possibly dropped ack then yes The usual way to handle this is timeout Wait some time until you give up and try again Handling network faults doesnt necessarily mean tolerating them you could just deliver an error message but you do need to know how your software reacts to network problems and ensure they can recover It may make sense ti deliberately trigger network problems to test Chaos monkey Detecting faults Timeouts Undetected delays Many systems need to automatically detect faulty nodes Its sometimes hard to do TCP ports refusing connection can get you back a RST or FIN but the node can crash mid request If application process fails a script can detect and tell other nodes if the host OS is still working The router may give you back an ICMP destination unreachable if the node you are trying to get to is unreachable Hardware failure can also be detected at switch hardware level if you cam query the management interface of the switches If you want to be sure a request was successful you need a positive ACK from the application itself NACK is useful for speedy detection but they cannot be relied upon You can wait for timeout retry a few times and declare dead if no response There is no simple answer to how to set the timeout Premature declaration of a node being down is problematic places additional load cascading failure possibility potentially duplicated operation Imagine you have a network transmission time upperbound of d and processing time upperbound of r then 2d r seems a reasonable timeout but most systems in reality dont have these guarantees If your timeout is low it only takes a transient spike in roundtrip time to throw the system off balance Many can queue a network packet switch queueing receiver OS queueing sender queueing eg due to TCP flow control TCP packet loss can cause further delay in OS waiting for retransmission and retransmitted packet to be acknowledged UDP is a good choice in situations where delayed data is worthless eg videoconferencing and VoIP Better than a a configured constant timeouts systems can continuously measure response times and their variability jitter and automatically adjust timeouts according to observed response time distribution TCP retransmission timeouts works this way as does Phi Accrual failure detector in Cassandra Synchronous network vs asynchronous Telephone networks synchronous are circuit switched a circuit a fixed route is established between the two parties throughout the call much more reliable and does not suffer from queueing as the 16B space for the call have already been reserved in the next hop of the network during a call each side is guaranteed to be able to send exactly 16B of audio every 250ms Because the network has no queueing the maximum endtoend delay is fixed a bounded delay TCPIP network is packet switched as they are optimized for bursty traffic Audiovideo call has a stable data rate while web browsing can have a variable data rate TCPIP tries to deliver as fast as possible There has been attempts to build hybrid of packet switching and circuit switching such as Asynchronous Transfer Mode ATM network a competitor of Ethernets It implements endtoend flow control at link layer which reduces the need for queueing in the network though it can still suffer from congestion and cause delays With careful use of quality of service QoS prioritization and scheduling of packets and admission control ratelimiting senders its possible to emulate circuit switching on packet switching networks or provide statistically bounded delay More generally latency guarantees are achievable in certain environments if resources are statically partitioned dividing network link statically as described above or allocating a static number of CPU cycles to each thread These come at the cost of reducing utilization Multitenancy with dynamic resource partitioning provides better utilization making it cheaper but with the downside of variable delay Variable delays in networks are not a law of nature but simply the result of a costbenefit tradeoff Currently deployed technology does not allow us to make any guarantees about delays or reliability of the network Unreliable clocks Time is tricky business in a distributed system communication is not instantaneous each machine has its own clock usually a quatz crystal oscillator These are not perfectly accurate and each machine may have its own notion of time It is possible to sync time to some degree most commonly with NTP which allows the computer clock to be adjusted according to the time reported by a group of servers The servers in turn get their time from a more accurate time source such as a GPS receiver Monotonic clock and timeofday clock Modern computer has at least these two kinds and they serve different purposes Timeofday clock wall clock time gets you time according to some calendar clockgettimeCLOCKREALTIME call on Linux gets you number of seconds since the epoch UTC 1970 Jan 1 midnight according to Gregorian calendar not counting leap seconds a day may not have exactly 86400 seconds Timeofday clocks are usually synced with NTP some oddities include eg when a local clock is too ahead it may jump back in time to a previous point These jumps make timeofday clock unsuitable for measuring elapsed time historically they are also very coarse grained Monotonic clock is suitable for measuring time interval such as a timeout or a services response time clockgettimeCLOCKMONOTONIC on Linux is monotonic clock These clocks are guaranteed to move forward The absolute value of monotonic clock is meaningless and monotonic clocks are not synchronized across machines It also makes no sense to compare the monotonic clock time from two different computers On a multiCPU system there may be a separate timer per CPU which is not synchronized with other CPUs OS tries to compensate for this discrepancy but one should take this guarantee of monotonicity with a grain of salt NTP may adjust the frequency at which the monotonic clock moves forward by 005 if it detects that the computers local quartz is moving faster or slower than the NTP server but it cannot cause monotonic clock to jump forwards or backwards Clocks drift run faster or slower than it should depending on the temperature Google assumes a clock drift of 200 parts per million for its servers an equivalent of 6ms drift for a clock that is resynced with a server every 30s This limits the best possible accuracy you can have for wall clocks If a computers clock differs too much from an NTP server it may refuse to synchronize or the local clock will be forcibly reset consequently any applications will see a jump forward backward in time NTP synchronization can also only be as good as the network delay NTP clients are robust enough to query a number of configured servers and discard outliers Leap seconds result in a minute being 59s or 61s long which could mess up systems not designed with leap seconds in mind It is possible to achieve very good accuracy if you care about it sufficiently to invest significant resources eg mifid ii draft requires all HFT to synchronize their clocks to within 100ms of UTC to help detect market manipulation Such precision can be achieved using GPS receivers precision time protocol and careful deployment If you use software that requires synchronized clocks it is essential that you also carefully monitor the clock offsets between all the machines Any node whose clock drifts too far from the others should be declared dead and removed from the cluster Timestamp for ordering events Using synchronized wall clock to order events in distributed systems is not advisable In a multileader system drift between nodes can cause the replicas to not be eventually consistent without further intervention no matter if using leader timestamp or client timestamp if multiclients Use logical clocks for this purpose which are based on incrementing counters rather than an oscillating quartz crystal Clock reading as confidence interval Clock readings over the network compared with a server whose time this syncs to is more like a range of times within a confidence interval clockgettime return value doesnt tell you the expected error of a timestamp and you dont know the confidence interval Googles TrueTime API in spanner explicitly reports the confidence interval on the local clock It returns two values earliest latest whose width depends on how long it has been since the local quartz clock was last synced with a more accurate clock source Synchronized clocks for global snapshots transaction ID Recall that in snapshot isolation each transaction has a motonically increasing ID and if B reads a value written by A B should have a transaction ID higher than that of As Generating a monotonically increasing ID in a distributed system with lots of small rapid transactions can be challenging Synchronized clock with good enough accuracy can be used to generate this ID and Spanner implements snapshot isolation across data centers this way with time returning a confidence interval if two intervals dont overlap then we know in which order those two times are In order to ensure transaction timestamp reflects causality Spanner deliberately waits for the length of confidence interval before committing a readwrite transaction so any transaction that can read this data happens at a sufficiently later time so that their confidence intervals dont overlap Hence Spanner needs to keep the interval as small as possible to minimize wait time and for this reason Google deploys a GPS receiver or atomic clock in each data center allowing clocks to be synchronized within 7ms Process pauses Assume weve a single leader system how does a leader know its still leader and not proclaimed dead by others We can let the leader hold a lease a lock with timeout To be leader the node has to renew lease before it expires Imagine a process renews its lease 10s before expiry which should not be the wall clock time of a different node who set it but the last request processing took longer than that then by the time the request finishes this node would no longer hold the lease A pause like this can happen if markandsweep GC runs long enough virtual machine suspension and resume context switches slow disk access OS swapping inmemory and ondisk vram pages frequently thrashing a process receiving a SIGSTOP followed by a late SIGCONT A node in a distributed system must assume that its execution can be paused for a significant length of time at any time even in the middle of execution Response time guarantees limiting the impact of GC Processes can pause for unbounded time as shown before On the other hand hard real time systems have a specified deadline by which the software must respond Providing realtime guarantees in a system requires support from all levels of software stack a real time operating system RTOS that allows processes to be scheduled with a guaranteed allocation of CPU time in specified interval is needed Library functions need to document worstcase execution time Dynamic memory allocation may be restricted or disallowed algother An enormous of testing is required to guarantee the requirements being met These places a lot of constraints on programming languages libraries and tooling Realtime systems often have lower throughput as they have to prioritize timely response above all else For most serverside data systems realtime guarantee is not economical Consequently they must suffer pauses and clock instability To limit the pause from markandsweep GC an emerging idea is to treat GC pause like brief outage of the node and let other nodes handle requests If a node needs to GC soon it stops taking in requests and GCs after finishing up Some trading systems do this A variant of the idea is to only GC short lived objects cheap to GC and restart periodically Knowledge truth and lies Reasoning about distributed system can be hard as you dont know the state of other nodes for sure the only way is to ask them via a not always reliable network In a distributed system we can state the assumptions we are making about the behavior the system model and design the actual system in such a way that it meets those assumptions A node cannot necessarily trust its own judgment of a situation it may think itself alive as it can hear from other nodes but other nodes cannot hear from it and declare it dead similarly think itself a leader a holder of a lease etc Instead the truth is defined by the majority a quorum that requires a minimum number of votes Most quorums require a majority number of votes as there cannot be a differing quorum at the same time Frequently a system requires there be only one of something Eg a single leader only one node holding a lock globally unique username Say a node grabs a lease to write something then GCs itself lease expires during GC and is granted to another node the GCed node coming back may resume its write operation thinking itself still holding the lease This is problematic and happened for HBase Fencing token is a technique that can address this each time the lock server grants a lease it also returns a fencing token a number incremented by the lock service we then require every clients write request to the storage service to include the current fencing token and the storage service will reject old fencing tokens if it has already seen a newer one ZooKeeper can be used a lock service with the transaction ID or node version as monotonically increasing candidates for fencing token Byzantine faults This book assumes nodes are unreliable but honest Distributed system problems become much harder if there is a risk of nodes lying A system is Byzantine faulttolerant if it continues to operate correctly even if some of nodes are not obeying the protocol Flight control systems typically need to be Byzantine fault tolerant due to radiation corrupting physical hardware With multiple participating organization a system may need to be Byzantine fault tolerant blockchain tries to address such In most serverside data systems the cost of deploying Byzantine fault tolerant solutions make them impracticable In a clientserver architecture if we assume untrustworthy clients the servers can usually perform validation to decide whats allowed Most Byzantine fault tolerant algorithms require a super majority of more than twothirds of the nodes to be functioning correctly In scenarios if a malicious attacker can compromise software running on other nodes then Byzantine fault tolerance wont help and traditional mechanisms authentication access control encryption firewalls continue to be the main protection against attackers Weak forms of lying unreliability handling is pragmatic and doesnt require fullblown Byzantine fault tolerant solutions TCP checksums user input sanitization setting up redundant NTP servers are good examples System model and reality Algorithm correctness Safety and liveness A system model is an abstraction that describes what things an algorithm may assume formalizes the kinds of faults that we expect to happen in a system Timingassumptionswise 3 system models are in common use synchronous bounded network delay processes pauses and clock error partially synchronous most of the time bounded realistic for many systems asynchronous no timing assumptions in fact we dont even have a clock and cannot use timeouts Nodefailurewise 3 system models are in common use crashstop faults node fails in one way crashing and does not come back crashrecovery faults crash may happen at any time a node may also come back after some time Nodes are presumed to have persistent storage so precrash state can be captured but inmemory states are lost Byzantine faults nodes can do anything For modeling real systems partially synchronous model with crashrecovery is most common Correctness of an algorithm under these models are described by its properties eg sorting algorithm should have all elements sorted Fencing tokens generation should have uniqueness monotonic sequence and availability node who requests a fencing token and does not crash should eventually get a token An algorithm is correct if in some system model it always satisfies its properties in all situations that we assume may occur in that system model Two kinds of properties in the fencing token example unique and monotonic are safety and availability is liveness Safety means nothing bad happens if violated we can point at a particular point in time at which its broken and a violation cannot be undone and liveness mean something good happens eventually may not hold at some point in time but there is always hope that it may be satisfied in the future Distinguishing between safety and liveness helps with dealing with difficult system models Theoretical abstract system models are quite useful even though in practice a real system can violate the assumptions of an abstract model making empirical testing equally important Summary This chapter covers what could go wrong in a distributed system lossy network clock out of sync process pause Alternatives that guarantee these dont happen throughout the stack do exist but are usually costly Partial failure can occur is the defining characteristic of distributed systems If you can simply keep things on a single machine it is generally worth doing so However scalability fault tolerance and low latency can make distributed systems desirable Chap 9 Consistency and consensus Given the problems in distributed systems introduced in chap 8 one good way of tackling them is to find general purpose abstractions with useful guarantees implement them once and let application run under those guarantees Transaction is one such guarantee that hides underlying concurrency and crashes and provides acid to the application Consensus is another such guarantee getting nodes to agree on something Consistency models linearizability Recall linearizability atomic consistency strong consistency immediate consistency external consistency the strongest form of consistency where we make the system appear as if there is only one copy of the data and all operations on it are atomic Linearizability requires the data written by a completed write call to be immediately available for all subsequent read calls Read after write is done must then reflect the written result read concurrent with write can return the result before or after the write but once result after write is returned subsequent reads must return the result after write Linearizability vs serializability the former is a recency guarantee on reads and writes of a register one individual object so it does not prevent problems like write skew the latter is an isolation property of transactions where each transaction may read and write multiple objects It guarantees that transactions behave the same as if they had executed in some serial order It is Ok for that serial order to be different from the order in which transactions were actually run A database providing both serializability and linearizability is known as strict serializability or strong onecopy serializability 2PL and actual serial execution are typically linearizable Serializable snapshot isolation is not linearizable by design Linearizability is useful in the following scenarios locking and leader election In a single leader system one way to elect a leader is to use a lock Every node that starts up tries to acquire a lock and the one that succeeds becomes the leader No matter how this is implemented it must be linearizable all nodes must agree which node owns the lock otherwise the lock is useless Coordination service like ZooKeeper use consensus algorithms to implement linearizability in a faulttolerant way and are often used for locking and leader election implementation uniqueness guarantee The situation is similar to a lock when a user registers for a service with a unique username you can think of them acquiring a lock on their chosen username A hard uniqueness constraint typically requires linearizability Foreign key or attribute constraints can be implemented without requiring linearizability crosschannel timing dependencies Implementing linearizability To implement linearizability one way is to just have one copy of the data which is not fault tolerant We need replications and revisiting different replication mechanisms singleleader replication is linearizable if reading from leader or synchronously updated followers consensus algorithms bear a resemblance to singleleader replication They also implement linearizable storage safely multileader replication systems are generally not linearizable write conflicts resolution are typically an artifact of lacking a single copy of data leaderless replication systems like Dynamo claim strong consistency by requiring w r n This is not quite true LWW conflict resolution based on timeofday clock are not linearizable as clock timestamps cannot be guaranteed to consistent with actual event timing due to clock skews Sloppy quorum also ruins linearizability Even with strict quorum this is not necessarily true To make strict quorum linearizable a reader must perform read repair synchronously before returning results to the application and a writer must read the latest state of a quorum of nodes before sending its writes The cost of linearizability CAP theorem When partition happens pick one out of consistency linearizability or availability If your application requires linearizability and some replicas are disconnected from the other replicas due to a network problem then some replicas cannot process requests while they are disconnected they must either wait until the network problem is fixed or return error become unavailable If your application does not require linearizability then it can be written in a way that each replica can process requests independently even if it is disconnected from other replicas eg multileader In this case the application can remain available in the face of a network problem but its behavior is not linearizable All in all there is a lot of misunderstanding about CAP and many socalled highly available systems actually dont meet CAPs idiosyncratic definition of availability CAP as formally defined is of very narrow scope it only considers one consistency model linearizability and one kind of fault network partition It doesnt say anything about network delays dead nodes or other tradeoffs thus although historically influential CAP has little practical value for designing systems CAP has also been superseded by more impossibility and more precise results in distributed systems Although linearizability is a useful guarantee surprisingly few systems are linearizable RAM on a modern CPU is not in the face of multithreading race conditions unless a memory barrier or fence is used Reason for this is each CPU core having its own cache and memory access first goes to the cache then changes are asynchronously written to memory This creates multiple copies and with asynchronous updates linearizability is lost The reason to drop linearizability in this case has nothing to do with CAP but for performance The same is true for many distributed databases that dont provide linearizability they sacrifice it for performance not so much for fault tolerance Linearizability is slow Can it be made fast The answer is perhaps no Research has shown if you want linearizability response time of reads and writes is least proportional to the uncertainty of delays in the network In a network with highly variable delays the response time for linearizability is inevitably going to be high Weaker consistency systems however can be made faster Ordering guarantees The definition of linearizability behaves as if there is only a single copy of the data and every operation takes effect atomically implies that operations are executed in some welldefined order Ordering has been an important theme single leader addresses this serializability is about ensuring transactions are as if they are executed in some sequential order timestamps introduced during clock synchronization is another attempt at determining which happened first There is deep theoretical connection between ordering linearizability and consensus Ordering and causality Ordering helps preserve causality Consistency level consistent prefix reads is about causality One readwrite knowing about another is another expression of causality Read skew is a violation of causality the answer can be seen but not the question A consistent snapshot in Snapshot Isolation means consistent with causality if it contains an answer it must contain a question Causality imposes on an ordering of events cause comes before effect A system is causally consistent if it obeys the ordering imposed by causality eg snapshot isolation provides causal consistency A total order allows any two elements to be compared a causal order is not a total order integer space has a total order sets dont Sets are partially ordered by subset superset Linearizability has a total order of operations one copy all atomic Causality in distributed systems may have two operations being concurrent two events can be ordered if they are causally related otherwise they are incomparable this means causality defines a partial order Git history is very much like a graph of causal dependencies Linearizability is thus stronger than causal consistency linearizability implies causality Causal consistency is the strongest possible consistency model that does not slow down due to network delays and remains available in the face of network failure The technique for determining which operation happened before which is similar to earlier discussion in detecting concurrent writes in a leaderless datastore In order to determine causal ordering the database needs to know which version of the data was read by the application this is why earlier discussion had the version number from the prior operation being passed back to the database on a write SSI conflict detection uses a similar idea when a transaction wants to commit the DB checks whether the version of the data that it read is still up to date to this end the DB keeps track of which data has been read by which transaction Sequence number ordering If there is a logical clock that generates a unique sequence number for each operation then the sequence numbers define a total ordering In particular we can create sequence numbers in a total order that is consistent with causality In singleleader replication the replication log defines a total order of write operations that is consistent with causality The leader can generate a sequence number for each write event in the log Without a singleleader various methods are used in practice each node generates its independent set one odd another even sufficiently high resolution timeofday clock can be used preallocate a block of sequence numbers for each node These methods all have the problem of the sequence numbers they generate are not consistent with causality one node being faster than another breaks the first clock skew breaks the second and preallocating block breaks causality in that a later block is always ranked later The above are inconsistent with causality but Lamport timestamp a pair counter nodeID is Uniqueness is obvious total ordering is defined first order by counter value then node ID and the key idea is every node keeps track of the maximum counter value it has seen so far and includes that maximum on every request When a node receives a request or response with the maximum value greater than its own counter value it immediately increases its own counter to that maximum As long as the maximum counter value is carried along with every operation this scheme ensures that the ordering from the Lamport timestamps is consistent with causality because every causal dependency results in an increased timestamp Version vectors are different from Lamport timestamps version vectors can distinguish whether two operations are concurrent or causally dependent whereas Lamport timestamp always enforce a total ordering You cannot tell two operations are concurrent or causally dependent from Lamport timestamp but they are more compact than version vectors Total ordering as enforced by Lamport timestamp can be not sufficient when enforcing unique username you can use Lamport timestamp to decide which operation to register the same username came later and reject that but this happens after the fact you have to check with every other node to find out which timestamps it has generated and cannot guarantee if one node is unreachable This hurts availability The problem is total ordering emerges only after you have collected all the operations The idea of knowing when your total order is finalized is captured in the section below Total order broadcast Total order broadcast atomic broadcast is usually described as a protocol for exchanging messages between nodes where two safety properties need to be satisfied reliable delivery if a message is delivered to one node it is delivered to all nodes totally ordered delivery messages are delivered to every node in the same order ZooKeeper implements total order broadcast Total order broadcast is exactly what one needs for DB replication if every message represents a write to the DB and every replica processes the same writes in the same order then the replicas will remain consistent with each other This principle is known as state machine replication Total order broadcast can be used to implement serializable transactions if every message represents a deterministic transaction to be executed as a stored procedure ad if every node processes those messages in the same order then partitions and replicas of the DB are kept consistent with each other An important aspect of total order broadcast is that order is fixed at the time the messages are delivered a node cannot retroactively insert a message into earlier position in the order if subsequent messages have already been delivered This fact makes total order broadcast stronger than timestamp ordering Total order broadcast is also useful for implementing a lock service that provides fencing tokens request to acquire a lock is appended as a message to the log and all messages are sequentially ordered in the order they appear in the log and the sequence number can then serve as a fencing token as its monotonically increasing ZooKeeper zxid is one such sequence number Partitioned databases with a single leader per partition often maintain ordering only per partition and they cannot consistency guarantees across partitions eg consistent snapshots foreign key references Total ordering across all partitions is possible but requires additional coordination Linearizability and total order broadcast Linearizability is not quite the same as total order broadcast If you have total order broadcast you can build linearizable storage on top of it Imagine we have a total order broadcast log when writing to a key we first append the desire to write to the log and then when we commit the write we only do so if we see our attempt to write happens first since the last committed write happened in log If so we commit otherwise we abort a linearizable compareandset This ensures linearizable writes and a similar approach can be used to implement serializable multiobject transactions on top of a log This asynchronous update procedure does not guarantee linearizable reads one can read a stale value This write linearizability provides sequential consistency timeline consistency slightly weaker than linearizability To make reads linearizable we can do sequencing reads through the log by appending a message reading the log and performing actual read when the message is delivered back to you if the log allows you to fetch the position of the last log message in a linearizable way you can query that position wait for all entries up to that point to be delivered then perform the read you can make your read from a replica that is synchronously updated on writes and is thus sure to be up to date We can also build total order broadcast from linearizable storage Assume you have a lineariable register that stores an int and has an atomic incrementandget operation for every message you want to send through total order broadcast you incrementandget the linearizable integer and then attach the value you got from the register as a sequence number to the message Resend lost messages and let the recipients apply the messages consecutively by sequence number Unlike Lamport clocks the numbers you get from incrementing the linearizable register form a sequence without gaps Seeing a gap in the sequence number means the recipient need to wait and this is the key difference between total order broadcast and timestamp ordering If things never fail building a linearizable incrementandget is easy you could keep it in a variable on one node when dealing with failure in general you end up with a consensus algorithm to generate linearizable sequence number generator It can be proved that a linearizable compareandset or incrementandget register and total order broadcast are both equivalent to consensus and the solution for one can be transformed into that for another Consensus Getting nodes to agree is a subtle but important problem in leader election atomic commits etc FLP result claims no algorithm is always able to reach consensus if risking a node crash in the asynchronous system model where a deterministic algorithm cannot use clocks or timeout eg to detect crashes 2Phase Commit 2PC is a simple form of consensus which can be used for atomicity in multipleobject transactions eg when updating a secondary index In a singlenode write scenario atomicity can be achieved by first making the written data durable in a writeahead log then write the commit record Crashrecovery would consider the write committed aborted if seeing not seeing the commit record at the end In case of multinodes being involved eg termpartitioned secondary index where the secondary index can live on a different node from where the data is we cant just do write data commit record sequence for each node Note that most NoSQL DBs dont support distributed transactions but clustered relational DBs do Commits are irrevocable as implied by readcommitted consistency A node must commit when it is certain all other nodes involved in the transaction are going to commit This is where 2PC comes in Differentiate 2PC with 2PL latter is for achieving serializable isolation and former is for atomic commit to distributed nodes 2PC uses an extra component coordinator transaction manager The workflow is Write Coordinator write to individual nodes Prepare Coordinator asks each node to be ready for commit after which node promises to commit without actually committing If a node replies with yes ready to commit theres no turning back from this decision it must commit if later the Coordinator tells it to Commit Upon hearing back from all nodes that they are ready for commit the Coordinator tells all to commit If failure to commit happens at this stage the coordinator must retry until succeeds theres no turning back wait for a participant recovery if needed Two points of noreturn in the workflow a node cannot refuse to commit later on if it has replied to Coordinator that it will commit if told and once Coordinator makes the decision to commit the decision is irrevocable In case of a Coordinator crash if it happens before prepare participant can abort if it happens after participant replying yes then the participant cannot abort unilaterally it has to wait for Coordinator recovery This is why the Coordinator must write its commit abort decision to its writeahead log so that when recovering it knows what its decision was In other words commit point in 2PC comes down to a single node Coordinator deciding to commit or abort 2PC is blocking atomic commit as nodes potentially have to wait for coordinator recovery 3PC can make this process asynchronous but 3PC assumes bounded network response time and bounded node response time in general non blocking commit requires a perfect failure detector a reliable mechanism to detect crashes Distributed transaction 2PC provides an important atomicity guarantee but cause operational problems kills performance and promises more than it can deliver Distributed transactions comes in Database internal distributed transaction all nodes running the same software Heterogeneous distributed transaction nodes run different software or even some running DB others running message brokers A lot more challenging Exactly once message processing between heterogeneous systems allows systems to be integrated in powerful ways eg message from a message queue can be acknowledged as processed if and only if the database transaction for processing the message was successfully committed This can be implemented by atomically committing the message acknowledgement and the database writes in a single transaction With distributed transaction support this can be achieved when the two are not on the same machine Such a distributed transaction is only possible if all systems affected by the transaction are able to use the same atomic commit protocol XA is a standard for implementing 2PC across heterogeneous technologies A series of language API bindings interacting with a Coordinator and a Coordinator library implementation In case of a Coordinator crash participants are stuck in their transaction they cannot move on as database transaction usually take a rowlevel exclusive lock on any row they modify to prevent dirty writes and DB cannot release those locks until the transaction commits or aborts To counter potentially waiting for Coordinator forever many XA implementation allows a participant to unilaterally decide to abort which can break atomicity XA either has Coordinator being a single point of failure or Coordinator faces the same distributed transaction problem its distributed log becomes a database requiring replica consistency etc XA works across systems and is necessarily a lowest common denominator and cannot detect deadlocks or implement Serializable Snapshot Isolation Fault tolerant consensus The consensus problem is normally formalized as one or more nodes may propose values and the consensus algorithm decides on one of those values It must satisfy the properties uniform agreement no two nodes decide differently safety integrity no nodes decide twice safety validity if a node decides some value v then v was proposed by some node safety termination every node that does not crash has to eventually decide some value liveness If you dont care about fault tolerance then satisfying the first three principles is easy you can hardcode one node to be the dictator and let that node make all of the decisions Should it fail then the system is stuck fourth property is violated 2PC with its Coordinator does just the above and violated termination property Termination property formalizes the idea of fault tolerance If all nodes crash and none are running then it is not possible for an algorithm to decide anything It can be proved that any consensus algorithm requires at least a majority of nodes to be functioning correctly in order to assure termination Many consensus algorithm assume that there no Byzantine faults It is possible to make consensus robust against Byzantine faults as long as fewer than onethird of the nodes are Byzantine faulty Best known consensus algorithms are Viewstamped Replication Paxos Raft and Zab Most of these actually dont use the formal definition here of agreeing on one value while satisfying the properties instaed they decide on a sequence of values which makes them also total order broadcast algorithms Total order broadcast requires messages to be delivered exactly once in the same order to all nodes This is equivalent to performing several rounds of consensus each round nodes first propose what message they to send next and then decide on the next message to be delivered same message same order agreement no duplicate integrity message not corrupted validity messages are not lost termination Viewstamped Replication Raft MultiPaxos and Zab implement total order broadcast directly Paxos implements onevalueatatime consensus Single leader replication and consensus Isnt Chap 5s single leader replication essentially total order broadcast The answer comes down to how the leader is chosen If manually chosen then you have a total order broadcast of the notfaulttolerant way termination is violated as without manual intervention progress isnt made in case of leader failure Automatic leader election failover and promoting a new leader brings us closer to total order broadcast consensus Theres a problem however the split brain issue in which two nodes think themselves the leader at the same time We then have to have all nodes agree on who the leader is to achieve consensus and to have all nodes agree is itself a consensus problem All of the consensus protocols discussed so far internally use a leader in some form but they dont guarantee the leader being unique Instead they make a weaker guarantee the protocols define an epoch number ballot number Paxos view number Viewstamped Replication term number Raft and guarantee within each epoch the leader is unique Every time current leader is thought to be dead a vote is started to elect a new leader This election is given an incremented epoch number totally ordered and monotonically increasing If leader from the last epoch wasnt dead after all then leader with the higher epoch number prevails Before a leader is allowed to decide anything it must first check there isnt another leader with a higher epoch number To do so it must collect votes from a quorum of nodes for every decision a leader wants to make it must send the proposed value to the other nodes and wait for a quorum of nodes to respond in favor of the proposal The quorum typically consists of a majority of nodes A node votes in favor of a leaders proposal only if it is not aware of any other leader with a higher epoch Two rounds of voting choosing a leader then on its proposal Key insight is quorum for these two votes must overlap Thus if the vote on a proposal does not reveal any highernumbered epoch Difference with 2PC is that Coordinator is not previously selected and going ahead requires votes from a majority but not every node Limitations Consensus provides useful properties to distributed where everything is uncertain can be used to implement linearizable atomic operations total order broadcast in a fault tolerant way However they are not used everywhere because of the benefits coming at a cost The process by which nodes votes on proposals before they are decided is a kind of synchronous replication Consensus systems always require a majority to operate 3 to tolerate 1 failure 5 to tolerate 2 Most consensus algorithms assume a fixed set of nodes that participate in voting meaning you cant add or remove nodes in the cluster Dynamic membership extensions would allow the above but they are much less well understood Consensus systems generally rely on timeouts to detect failed nodes and can be hard to apply in environments with highly variable network delays such as geographically distributed systems Frequent reelection in such cases could cause the system to end up spending more time choosing a leader than doing useful work Sometimes consensus algorithms are particularly sensitive to network problems Raft has been shown to bounce leaders often if one particular network link is consistently unreliable Membership and coordination services ZooKeeper etcd are often described as described as distributed keyvalue stores or coordination and configuration services They offer APIs looking like reading writing value for a given key and iterating over keys As an application developer it is rare for one to directly interact with ZooKeeper instead one would rely on it via other projects HBase Kafka Hadoop YARN all rely on ZooKeeper running in the background ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory You wouldnt want to store all your applications data here and this small amount of data is replicated across all nodes using a faulttolerant total order broadcast algorithm each message broadcasted is a write to DB and applying writes in the same order provides consistency across replicas ZooKeeper is modeled after Google Chubby implementing total order broadcast and other features like Linearizable atomic operations A lock can be implemented using an atomic compareandset The consensus protocol guarantees the operation is atomic and linearizable even upon node failure A distributed lock is usually implemented as a lease which has an expiry so that its eventually released in case of client failure Total ordering of operations ZooKeeper can provide fencing token as a monotonically increasing number increases every time a lock is acquired ZooKeeper provides this by total ordering all operations and giving each a monotonically increasing transaction ID and version number Failure detection Clients maintain a longlived session on ZooKeeper servers and the client and server periodically exchange heartbeats to check the other node is still alive If heartbeats cease for a duration longer than the session timeout ZooKeeper declares the session dead Any locks held by a session can be configured to be automatically released when session times out Change notification Clients can read locks and values created by another and also watch for changes Eg it can find out when a client joins fails via notifications Out of these only linearizable atomic operations requires consensus but the rest makes ZooKeeper useful for distributed coordination One example where ZooKeeper Chubby model works well is when you want a leader elected from several instances of a process or service Useful for singleleader DB and also job scheduler etc Another example is when you have partitioned resource and need to decide which partition to assign to which node and rebalance load when new nodes join leave can be implemented with ZooKeeper atomic operations and notifications Libraries like Apache Curator provide higherlevel tools on top of ZooKeeper client API An application may grow from running on a single node to thousands of nodes Majority vote over this many would be inefficient so instead ZooKeeper is configured to run on a fixed number of nodes 3 5 Normally the kind of data managed by ZooKeeper is quite slow changing on the timescale of minutes hours Tools like Apache Bookkeeper can be used for faster changing state of the application Another use case is service discovery find the IP address to talk to for a service It is less clear whether service discovery actually requires consensus DNS is the traditional way of looking up IP address for a service name it uses multiple layers of caching and DNS reads are absolutely not linearizable Leader election does require consensus Some consensus systems support readonly caching replicas to log the decision of consensus but not actively participate in voting to help other nodes ZooKeeper etc can be seen as part of research into membership services deciding which nodes are currently active With unbounded network delay its not possible to reliably detect whether another node has failed But if you decide failure with consensus nodes can come to an agreement about which nodes should be considered alive With membership determined and agreed upon choosing a leader can be simply choosing the lowest numbered among current members Summary The consistency model linearizability where replicated data appears as though there is only one copy and all actions act on it atomically Causality is a weaker consistency model where not everything has to be in a single totally ordered timeline version history can be a timeline with branching and merging With causal ordering things like no two users can claim the same username still cannot be implemented distributedly which led to consensus A wide range of problem are equivalent to consensus linearizable compareandset register decides to set or abort based on comparison of given value and current value atomic transaction commit db deciding whether to commit or abort total order broadcast locks and leases deciding which client holds it membership coordination uniqueness constraint These are straightforward with a single leader but if the leader fails the system stops making progress To handle the situation we can wait for leader to recover 2PC manual failover choose a new leader automatically consensus problem If you find yourself wanting one of the above reducible to consensus and you want it to be fault tolerant try tools like ZooKeeper Not every system requires consensus leaderless and multileader replication systems typically dont use global consensus maybe its Ok when multiple leaders dont agree we may be able to merge branching version histories Part 3 Derived data Systems of records source of truth if there is any discrepancy between another system and the system of record system of records data is by definition the correct one Written once and typically normalized Derived data systems process transform existing data in some way If you lose it you can always regenerate it Eg cache Technically redundant but good for read performance Being clear about which is which helps bring clarity on a confusing system Whether the DB stores systems of records or derived data is up to your application and how you use such data Chap 10 Batch Processing The first two parts talk about request response in an online system triggered by user services Response time is important A different system is an offline system batch processing system taking in a large amount of input data runs a job to process it and may take a while Usually scheduled periodically Throughput is important Stream processing system is something in between a nearrealtime processing system Operates on events shortly after they happen MapReduce is a batch processing algorithm and was subsequently implemented in Hadoop CouchDB MongoDB Batch processing is a very old form of computing and MapReduce bears an uncanny resemblance to the electromechanical IBM cardsorting machines Batch processing with Unix tools awk print x uniq c sort r n sed grep xargs etc Powerful and performs well worth learning Linux sort automatically handles larger than memory workload by spilling to disk and automatically parallelizes sorting across multiple CPU cores A chain of Unix commands easily scales to a large dataset without running out of memory Note that mergesort has sequential access patterns that perform well on disks remember that optimizing for sequential IO was a recurring theme earlier Unix pipes reflects well the design philosophy of Unix make each program do one thing well to do a new job build afresh rather than complicating the old with features expect the output of every program to be the input to another yet unknown program dont clutter output with extraneous information dont insist on interactive input avoid stringently columnar or binary formats design and build to be tried early ideally within weeks dont hesitate to throw away the clumsy part and rebuild use tools in preference to unskilled help to lighten a programming task even if you have to detour to build the tools and expect to throw some of them out after using them automation rapid prototyping incremental iteration friendly to experimentation breaking down large projects into manageable chunks very much like Agile of todays Many Unix programs can be joined together in flexible ways Unix expects output of one to be input of another ie exposing a uniform interface to achieve this composability On Unix that interface is a file descriptor can be a device driver network socket communication channel to another program unix sockets stdin etc having all these share an interface is quite remarkable By convention many of Unix program will treat this sequence of bytes from a file descriptor as ascii Another characteristic of Unix tools is their use of stdin and stdout a program can read write files if it needs to but the Unix approach works best if the tool doesnt worry about file paths and instead interact with stdin stdout so that logic and writing are separated the program doesnt care about where the input is coming and where the output goes to somewhat similar to the ideas loose coupling late binding and inversion of control stdin and stdout have limitations eg program with multiple inputs outputs will be tricky and you cant pipe your output to a network connection etc Part of what makes Unix tools so successful is they make it easy to see whats going on input is immutable output can be piped to less for inspection you can write the output of pipeline stages to a file and use that as the starting point for the next stage allowing you to restart the later stage without rerunning the entire pipeline The biggest limitation of Unix tools is they only run on a single machine and this is where the likes of Hadoop come in MapReduce Similar to Unix tools but distributed MapReduce usually does not modify input and produces output as sequential file writes no random access and modification Reads and writes from distributed file system like Hadoops HDFS based on its proprietary counterparty GFS HDFS also follows a sharednothing architecture and a NameNode is in charge of keeping track of directory and block mapping like the single master in GFS HDFS scaled well to over tens of thousands machines storing over PBs of data Workflow Input is split into records For each record a Mapper handles each record independently and can generate any number of key value pairs from the record eg extracting some field from a record The MapReduce framework takes key value pairs generated by the Mapper collects all values belonging to the same key and calls the Reducer with an iterator over that collection of values the Reducer then produce output records Each input to a Mapper is typically hundreds of MBs in size the scheduler tries to run each mapper on a machine that stores a replica of the input file provided that the machine has enough spare RAM and CPU putting the computation near the data less copying over the network and better locality The number of Mappers is typically decided by the number of input blocks and the number of reducers is typically user specified To ensure same keys end up on the same Reducer the framework maps a hash of the key to the reducer task The key value pairs must be sorted since the dataset is likely too large to be sorted in memory instead each Map partitions its output by Reducer based on the hash of key then each of these partitions is written to a local sorted file using SSTable LSM trees Whenever a Mapper finishes writing its sorted output files the scheduler notifies the Reducer it can start fetching output sorted file from the Mapper The process of partitioning by reducer sorting and copying data from mappers to reducers is known as shuffle Reducer takes files from mappers and merge them together preserving the sorted order even if different Mappers produced the same keys for this Reducer Reducer is called with a key and an iterator that sequentially scans over records with the same key uses arbitrary logic to process these records and generate any number of output records which are written to a file on a distributed file system It is very common for MapReduce jobs to be chained into workflows Hadoop MapReduce does not have particular support for workflows chaining done implicitly eg by different directory names used by different MapReduce jobs This is slightly different from Unix pipes where output of one is immediately fed as input to the next over a small memory buffer This materialization of intermediate state has pros and cons MapReduce discards partial output of a failed job and dependent jobs can only start when their dependencies finish successfully A number of workflow schedulers are introduced for Hadoop to handle this dependency diagram These need good tooling support to manage complex dependency diagrams dataflows Reduceside joins and grouping Its common for a data record to have an association with another via foreign key in a relational model document reference in a document model or an edge in a graph model Join happens when you need both sides of the reference Denormalization can reduce the need for joins In a DB with index joins typically involve multiple indexbased lookups MapReduce jobs has no indexes conventionally it scans the entirety of input files Joins in the context of batch processing means resolving all occurrences of some association within a dataset An example problem for a MapReduce job is given user clickstream records identified by an user ID associate each record with user details stored in a remote users DB think denormalization stars schema is also related A common approach is for the job to take a copy of the user database and put it on the same distributed file system the map job is running on One set of mappers would go through partitioned clickstream records extracting the user ID as key and relevant info another set of mappers would go through the user DB also extracting the user ID as key and other relevant info The MapReduce framework would then partition mapper output by key in sorted order and user record with the same ID ends up on the same reducer adjacent to each other and the reducer can perform join logic easily Secondary sort can even make these key value pairs appear in a certain order This algorithm is known as a sortmergejoin The key emitted by mapper in this case is almost like an address designating which reducer this goes to MapReduce framework separates the physical communication and failure handling aspect from application logic Besides join another common pattern is groupby The simplest way to set this up is to let mappers use grouping key as key Grouping and joining look similar in MapReduce The pattern of bringing all records with the same key to the same place breaks down if very large amount of data are associated with the same key linchpin objects hot keys and the process of collecting all data related with that key leads to skew hot spot If join input has hot keys Pigs skewed join algorithm first runs a sampling to job to decide which keys are hot and mapper sends a record associated with a hot key to a randomly chosen reducer instead of a deterministic one records relating to the hot key would also be replicated over all reducers And you can add another MapReduce job to aggregate the more compact results that the randomly chosen reducers produced Hive takes a different approach to handling hot keys it requires hot keys to specified explicitly in table schema and it uses a mapside join for that key when joining Mapside joins The above performs join logic in the reducers and mappers prepare the input data This has the advantage of not needing to make any assumptions about the input datas properties structure Downside is sorting copying to reducers and merging reducer results can be expensive You can perform faster mapside joins if you know certain things about the data No reducers would be needed The simplest way broadcasthashjoin is when a large dataset is joined with a dataset small enough to be loaded entirely into memory of each mapper The mapper can then load the joindataset into memory and as it goes through its chunk of records it can perform join and produce joined output Pig Hive Impala etc all support this If the inputs to mapside joins are partitioned in the same way as user DB is then hash join can be applied to each partition independently Each mapper only needs to load the partition of DB that would contain records relevant to its input records This is a partitioned map join If input is not only partitioned the same way but also sorted the same way then a mapside merge join is possible and user DB partition does not have to fit entirely into mappers memory In the Hadoop ecosystem this kind of metadata about the partitioning of datasets is often maintained in HCatalog and the Hive metastore Note mapside join outputs chunks of output files sorted in the same order as the chunks of input and reduceside join is chunks of records partitioned and sorted by the join key Output of batch workflows Recall transaction processing workload with analytics workload have different characteristics Batch processing fits more in analytics but not quite analytics MapReduce was originally introduced to build indexes for Googles search engine and remains a good way to build indexes for Lucene Solr Google has moved away from this Building documentpartitioned indexes and building classifier systems and recommendation systems parallelizes very well Since querying a search index is a readonly operation these index files are immutable once created unless source documents change The output of such jobs is usually some database where a web interface would query and separate from the Hadoop infrastructure Writing directly to the DB from your Mapper or Reducer job may be a bad idea since Making a network request per record is not performant even if the client library supports batching MapReduce jobs often run in parallel and all mappers and reducers writing to the same DB concurrently may overwhelm it Finally MapReduce provides a clean allornothing guarantee for job output however writing to an external system from inside a job produces visible sideeffect and you have to worry about results of partial execution being available to the rest of the system MapReduce jobs output handling follows Unix philosophy by treating input as immutable and avoiding side effects such as writing to a DB This minimizes irreversibility makes rollback easier only code rollback is needed not database too Makes retrying a partial failed job easier The same set of files can be used as input for various jobs And like Unix tools this separate logic from wiring Comparing Hadoop to Distributed Databases Hadoop is like a distributed version of Unix where HDFS is the file system and MapReduce a sortedshuffling distributed implementation of Unix process The ideas of MapReduce has been present in socalled massively parallel processing MPP databases for a while But MapReduce distributed file system provides something much more like a general purpose OS Diversity of storage Some difference between MapReduce and MPP DBs include DBs require you to structure data according to a particular model Hadoop opens up possibility of indiscriminately dumping data into HDFS and only later figure out how to process it further The idea is similar to a data warehouse simply bringing data from various parts of a large organization together in one place is valuable and careful schema design slows this process down This shifts the burden of interpretting data to the consumer schemaonread the sushi principle raw data is better Diversity of processing models MPP databases are monolithic with query planning scheduling and execution optimized for specific needs of the DB eg supporting SQL queries If you are building recommendation systems and full text search indexes then merely SQL is usually not enough MapReduce provided the ability to easily run custom code over large dataset You can build SQL with MapReduce Hive did this and much more Subsequently people found MapReduce performed too badly for some types of processing so various other processing models have been developed over Hadoop The Hadoop ecosystem includes both randomaccess OLTP databases such as HBase open source BigTable with SSTable and LSM trees as well as MPPstyle analytic DBs like Impala Neither uses MapReduce but both use HDFS Designing for frequent faults MPP databases usually let user resubmit the entire query upon failure this is fine for typically seconds minutes long analytic jobs while MapReduce allows retrying at the granularity of a job MPP DBs also prefer keeping as much data in memory as possible while MapReduce is more eager to write to disk This is related with Googles environment of mixeduse datacenters in which online production services and offline batch jobs run on the same machines and every task has a resource allocation enforced using containers This architecture allows nonproduction lowpriority jobs to overclaim resources to improve the utilization of machines As MapReduce runs at low priority they are prone to being preempted by higher priority processes needing their resource relying less on inmemory states and more on writing to disk is preferable Current open source schedulers uses preemption less Beyond MapReduce MapReduce is only one programming model for distributed systems MapReduce provides a simple abstraction over a distributed filesystem but is laborious to use Pig Hive Cascading Crunch are abstractions over MapReduce to address the hardtouseness MapReduce is general and robust but other tools can be magnitudes faster for certain kinds of processing Materialization of intermediate state MapReduce chain jobs by writing the first to file and having the second pick up from where the file is written This makes sense if the result of first should be made widely available and reused as input to different jobs but not when output of one job is only ever used as input to one other job intermediate state This materialization of internal state differs from piping Unix commands This has following downsides A MapReduce job can start when all tasks in the preceding jobs have completed whereas processes connected by a Unix pipe can start at the same time with output consumed as soon as its produced This having to wait contributes to having more stragglers in execution and slows down the pipeline Mappers are often redundant they read back the same file that was just written by a reducer and prepare it for the next stage of partitioning and sorting if the reducer output was partitioned and sorted in the same way as mapper output then reducers could be chained together directly without interleaving with mapper stages Storing intermediate state in a distributed file system who replicates them is often overkill for temporary data Dataflow engines To address the above issues several execution engines for distributed batch computations were developed including Spark Tez and Flink They handle an entire workflow as one job rather than breaking it up into independent subjobs They explicitly model the flow of data through several processing stages hence they are known as dataflow engines Like MapReduce they work by repeatedly calling a userdefined function to process one record at a time on a single thread They parallelize work by partitioning inputs and then copy the output of one function over the network to become the input to another function Unlike MapReduce these functions operators dont have the strict roles of Map and Reduce but instead can be assembled in more flexible ways One option is to repartition and sort records by key like in shuffle stage of MapReduce This enables sortmergejoin and grouping as MapReduce would Another possibility is to take several inputs and to partition them in the same way but skip the sorting This saves effort on partitioned hash joins where partitioning is important but the order is not as hash randomizes it anyway For broadcast hash joins the same output from one operator can be sent to all partitions of the join operator This processing engine style offers advantages over MapReduce Expensive work like sorting only need to be performed where it is required No unnecessary Map tasks Because all joins and data dependencies in a workflow are explicitly declared the scheduler has an overview of what data is required where so it can make locality optimizations It is usually sufficient for intermediate state between operators to be kept in memory or written to a local disk which requires less IO than writing to HDFS MapReduce uses this optimization for Mapper output but dataflow engines generalize it to all intermediate state Operators can start as soon as input is ready no need to wait for for the entire preceding stage to finish before the next one starts Existing JVM processes can be reused to run new operators reducing startup overheads compared to MapReduce which launches a new JVM for each task They can implement the same thing as MapReduce and usually significantly faster Workflows implemented in Pig Hive or Cascading can switch from MapReduce to Tez or Spark with simple configuration changes Tez is a fairly thin library relying on YARN shuffle service for copying data between nodes whereas Spark and Flink are big frameworks with their own network communication layer scheduler and userfacing API Faulttolerance is easy in MapReduce due to full materialization of intermediate states The dataflow engines avoid writing intermediate states to HDFS so they retry from an earlier latest stage possibly from original input on HDFS where intermediate data is still available To enable this recomputation the framework must keep track of how a given piece of data is computed inputs and operators applied to it Spark uses Resilient Distributed Dataset RDD to keep track of ancestry of data while Flink checkpoints operator state When recomputing data its important to know whether computation is deterministic Partial result was computed and delivered to downstream then upstream fails and if the delivered results can vary between retries then the downstream needs to be killed and restarted as well Its better to deterministic but nondeterministic behavior creeps in via unordered container iteration probablistic algorithms system clock usage etc Returning to the Unix analogy MapReduce is like writing the result of each step to a temporary file dataflow engines look more like Unix pipes A sorting operation inevitably needs to consume the entire input before producing any output any operator that requires sorting will thus need to accumulate state In terms of job output storage like MapReduce HDFS is still usually the destination instead of building DB writes into user defined functions Graph and iterative processing Think graphs in analytics workload Eg PageRank estimate the popularity of a webpage based on what other web pages link to it Many graph algorithms are expressed by traversing one edge at a time joining one vertex with an adjacent vertex in order to propagate information and repeating until no edges to follow or some convergence This repeating until done cannot be expressed in plain MapReduce and is thus often implemented in an iterative style External scheduler runs a batch process to calculate one step of the algorithm Batch finishes scheduler checks whether the iterative algorithm has finished If not run another round of batch process This is often inefficient Bulk synchronous parallel model of computation has become popular as a result Apache Giraph Sparks GraphX and Google Pregel model In Pregel one vertex can send a message to another vertex and typically those messages are sent along the edges in a graph in each iteration a function is called for each vertex passing the function all the messages that were sent to that vertex a bit similar to the actor model if you think of each vertex as an actor except that vertex state and messages between vertices are faulttolerant and durable Fault tolerance in Pregel is achieved by periodically checkpointing the state of all vertices at the end of an iteration The framework decides which vertex executes on which nodes when a vertex sends messages to other vertices it simply sends a vertexID High level APIs and languages Since MapReduce the execution engines have matured by now the infrastructure has become robust enough to store and process petabytes of data on over 10k machines Physically operating batch processes at such scale has been considered more or less solved attention has turned to other areas improving programming model efficiency of processing and broadening the set of problems these technologies can solve Eg declarative query languages specialization for different domains Summary Unix tools and philosophy MapReduce and data flow engines Two problems each distributed batch processing frameworks need to solve Partitioning MapReduce partitions according to input file blocks output of mappers is repartitioned sorted and merged into a configurable number of reducer partitions post MapReduce dataflow engines try to avoid sorting unless required but otherwise take a broadly similar approach to partitioning Fault tolerance MapReduce frequently writes to disks and materializes internal states Dataflow engines perform less materialization deterministic operators heko reduce the amount of data that needs to be recomputed Join algorithms in MapReduce Sort merge joins Broadcast hash joins Partitioned hash joins Distributed batch processing engines have a deliberately restricted programming model callback functions such as mappers and reducers are assumed to be stateless and have no visible side effects Batch processing job derives some output from bounded immutable input and a job knows when it has finished reading the entire input Next chapter turns to stream processing where the input is unbounded and a job is never complete Chapter 11 Stream Processing In general a stream refers to data that is incrementally made available over time TCP stream stdin stdout etc A stream as a sequence of events which is similar to records in the bounded filebased input output case from previous chapter In batch processing a file identified by filename is read by multiple jobs analogously events grouped into a topic or a stream in stream processing is generated by one producer and consumed by multilpe consumers In principle a file or database can connect producers and consumers periodic polls by consumers which is expensive Or database triggers but specialized tools have been developed for delivering events notification Messaging systems A Unix pipe or TCP connection between producer and consumer would be a simpple messaging system but they connect onetoone Different systems have been developed for this pubsub model and to differentiate them its generally helpful to ask What happens if producers produce faster than consumer can consume Approaches are drop backpressure flow control as in TCP Unix pipes to block the producer or buffering in a queue What happens if the queue grows beyond memory limit Crash or dump to file system What happens if nodes crash or go offline Are any message lost Durability has a cost Not all losses are unacceptable Batch processing system also provide strong reliability guarantee in retrying failed jobs automatically Later well see how to do the same with stream processing Direct messaging between producers and consumers Without any intermediate nodes UDP multicast especially where low latency is important Application level protocol can optionally build retransmission on error Brokerless messaging like ZeroMQ implement pubsub over TCP IP multicast Producer making a direct RPC HTTP request to consumer if consumer exposes a service on the network webhooks with callback url when registering These generally require the application code to be aware of possibility of loss And when consumer is offline some protocol retries delivery but producer may break down from buffering up too much Message brokers Aka message queue a kind of DB optimized for handling message streams It runs a server with producers and consumers connected to it as clients producers write to it consumers read from it By centralizing data in the broker these systems can more easily tolerate clients that come and go Broker handles durability some are stored in memory others in persistent storage Generally allow unbounded queueing to accommodate slow consumers sometimes configurable to backpressure A consequence of queueing is consumers are generally asynchronous producer only cares about brokers response that it got the event not knowing consumer side of things Message brokers vs DB Some brokers can participate in 2PC protocols for distributed transactions making them more similar to DBs Key differences include Brokers typically keep the message until its delivered to consumers and are not geared towards long term storage DB only deletes when explicitly told to Due to the above brokers typically assume the working set is small DBs typically support secondary indexes and various ways to search for data while brokers often support some way of subscribing to a subset of topics matching some pattern The mechanisms are different Querying DB typically returns a pointintime snapshot of the data and the client is not notified of data changes By contrast message brokers do not support arbitrary queries but instead notify clients when data changes Standards like AMQP and JMS are implemented in RabbitMQ Google Cloud PubSub etc Multiple consumers When multiple consumers are present for the same topic two patterns of messaging are usually present Load balancing each message to one consumer for parallel processing Fanout each message to all consumer the streaming equivalent of having multiple jobs read the same input and not affecting each other The two can be combined eg fanned out to two groups where each group runs load balancing inside Acks and redelivery Consumer can crash while processing a message so a broker waits for consumers confirmation acknowlegding it has finished processsing a message to delete a message The ack could be lost making the broker redeliver an already processed message Due to the possibility of crashing a consumer may see brokers messages outoforder in a load balancing scenario a message being processed by a crashed consumer when redelivered to another consumer may already be behind the chronologically later messages the consumer has processed Even if the standard requires the broker to try to preserve ordering combination of redelivery and loadbalancing workload makes reordering possible One can use one queue per consumer not do load balancing to avoid this or its possible for this to not be an issue for messages that are not causally related Partitioned logs Due to the transience of data mindset in message brokers a key feature for batch processes replayable derived data reprocessing runs no risk of damaging input is not the case with AMQPJMSstyle messsaging Typically receiving a consumer ack and the broker deletes the message but we can do a hybrid of durble storage of DBs and lowlatency notification of messaging logbased message brokers Logbased message broker This is essentially partitioned and distributed tail f Producer produces to a log and consumer tails it For higher throughput the log is partitioned different partitions hosted on different machines A topic can then be defined as a group of partitions that all carry messages of the same type With each partition the broker assigns a monotonically increasing sequence number offset to each number as a partition is appendonly and messages within a partition are totally ordered There is no ordering guarantee across partitions Kafka Amazons Kinesis Streams and Twitters DistributedLog are logbased message brokers Google Cloud PubSub is architecturally similar but exposes a JMSstyle API Even though these message brokers all write to disk they are able to achieve throughput of millions of messages per second by partitioning across machines and fault tolerance by replicating messages The logbased approach trivially supports fanout as reads are independent and dont delete from the log Load balancing is achieved by instead of assigning individual messages to consumer clients in a group the broker can assign entire partitions to nodes in the consumer group Each client then consumes all the messages its assigned vs JMSstyle message brokers This coarsegrain load balancing has some downsides The number of nodes sharing the work of consuming a topic can be at most the number of log partitions in that topic because messages within the same partition are delivered to the same node even though messages of the same partition get to all nodes serving that partition one could let one node in that group ignore oddnumbered messages and another ignore evennumbered In general having more partitions is scalable If a single message is slow to process it holds up the processing of subsequent messages in that partition headofline blocking Thus in situations where messages may be expensive to process and you want to parallelize processing on a messagebymessage basis the JMSAMQP style of message broker is preferable For high throughput where each message is fast to process and ordering is important the logbased approach works well Consumer offsets Consumers consume a partition sequentially so all sequence numbers smaller than consumers current sequence number have been processed Thus the broker does not track acks for every message it only needs to periodically record the consumer offsets This reduced bookkeeping overhead helps scale logbased systems This offset is similar to the log sequence number in singleleader database replication Broker leader database consumer follower If one node in the consumer group fails another from the group picks up from the last recorded offset leading to some messages potentially processed twice Disk space Only append and youll eventually run out Over time older segments of the partition is deleted Eventually if a slower consumer cannot keep up itll have offsets pointing to a deleted segment This boundedsize buffer and discarding old messages is usually implemented as a ring buffer circular buffer usually quite large in size This contrasts with inmemory message storage and only spilling to disk when memory is used up When consumers fall behind Logbased with circular buffer is essentially a buffer drop approach If one consumer falls behind others are not affected When a consumer shuts down crashes it stops consuming resources only its offset remains Whereas in traditional message brokers you need to be careful to delete any queues whose consumers have been shut down as they otherwise continue accumulating messages in memory unnecessarily Replaying old messages In JMSAMQPstyle brokers processing ack is destructive it causes messages to be deleted In a logbased message broker consuming is just reading from a file it is readonly the offset moves but the consumer is in charge of the offset and gets to specify it to any point Logbased is then more similar to batch processing in the sense that derived data is clearly separated from input data the process is repeatable and input data is immutable This allows easier experimentation and recovery making logbased message brokers a good tool for integrating dataflow within an organization Databases and streams Log based message broker applies the idea of databases to message brokers the reverse is doable too The event in the message broker in question is now a DB write A replication log is a stream of database write events produced by the leader as it processes transactions State machine replication in total order broadcast is another case of a stream of events in which each replica processes the same set sequence of events in the same order to arrive at the same final state Keeping systems in sync There can be multiple copies of the same data stored to serve different needs in a cache a fulltext index a data warehouse etc With data warehouse this synchronization is usually performed by ETL processes often by taking a full copy of a database transforming it and bulk loaded into the data warehouse by a batch processing job If periodic dump batch processing is too slow an alternative is dual write the application code explicitly writes to each system when data changes sometimes concurrent updates to them Dual writes can have race conditions arising from multiple clients trying to write at the same time leaving different subsystems different views which wont converge over time Another is fault tolerance problem writing to one failed but another succeeded leaving the two subsystems permanently outofsync Ensuring both succeed or fail here is a case of the atomic commit problem which is expensive to solve eg with 2PC These problems could be avoided if only one leader exists among the subsystems This can be achieved with change data capture the process of observing all data changes written to a database and replicating the change to other systems with changes as a stream This essentially makes one database the leader and others are followers listening to fanout from a message broker The communication is asynchronous database does not wait for consumers response before committing the data DB triggers or change log parsing can be used to implement change data capture LinkedIn Databus Facebook Wormhole Kafka Connect framework all offer CDC for various databases Keeping the entire DB change log for this reason would be too much space and too much time to play through instead we use a snapshot offsets One can also do log compaction on records with a CDC whenever it sees the same key the previous value is replaced More and more DBs are exposing CDC API as opposed to retrofitting their implementation Kafka Connect is an effort to integrate CDC tools for a wide range of database systems with Kafka Event sourcing CDC is similar to event sourcing in domain driven design community While both having a log event sourcing applies the idea at a different level of abstraction CDC users use the DB in a mutable way low level log is extracted and its effects replicated writer tothe DB does not know its doing CDC underneath Event sourcing application logic is built on the basis of immutable events written to the log Events are designed to reflect things at the application level and not low level state changes Event sourcing is a powerful technique by making it easier to evolve application over time and understanding why something happened Replaying the event log gives the current state of the system Log compaction are handled differently in CDC and event sourcing systems CDC log events usually contain brand new values for a key and log compaction can discard previous events Event sourcing events are modeled at a higher level later events usually dont overwrite previous events The event sourcing philosophy is careful to distinguish between events and commands User request comes as a command which may fail integrity condition checks if command validation is successful it becomes a durable and immutable event fact A consumer of event streams is not allowed to reject events thus any validation of commands need to happen synchronously before it becomes an event eg by using a serializable transaction that atomically validates and publishes State streams and immutability Immutability of input files enables replaying in batch processing immutability also makes event sourcing and change data capture powerful States change is the result of events that mutated it over time If you store the changelog durably that simply has the effect of making the state reproducible and it becomes easier to reason about the flow of data through a system From this perspective the database is a cached subset of the log ie the latest record values in log and the truth is the log Immutability in DBs is an old idea An accountants ledger is immutable Immutable events also capture more information than just current state You can also derive different readoriented representations from the same log of events making it easier to evolve your application over time eg building new optimized view of some data Storing data becomes easier if you dont have to worry about its access pattern complex schema design indexing storage engines are results of access pattern Having an immutable log gives you a lot of flexibility by separating the form in which data is written from the form it is read by allowing several different read views This is known as command query responsibility segregation The traditional approach to database and schema design is based on the fallacy that data must be written in the same form as it will be queried Debates about normalization and denormalization become largely irrelevant if you can translate data from writeoptimized event log to a readoptimized application state It entirely makes sense to denormalize data in the readoptimized views as the translation process will keep data consistent with event log The biggest downside to event sourcing change data capture is building derived view is usually asynchronous Readyourwrite consistency cannot be achieved if user writes to log and reads from derived view You could have the log update and derived view update in one transaciton but this would require both to be in the same system or have distributed transaction support in heterogeneous systems or one could use total order broadcast Concurrency is also made simpler in that multiobject transactions can now be one event that describes the user action and different derived views build from it If the event log and application state processing an event for user on partition A only requires partition A of the application state then a straightforward singlethreaded log consumer needs no concurrency control for writes actual serial execution removes the need to define a serial order in a partition Limitation of immutability Maintaining an immutable history for all changes can be expensive for workloads with lots of updates and deletes on a small dataset in size and fragmentation Compaction and garbage collection becomes key for robustness There may also be regulatory cases requiring data to be deleted shunning excision one cant just append a delete event history actually needs to be rewritten True deletion is surprisingly hard due to replication and hardware storage mechanism Its more like making it hard to retrieve the data yet one still has to try Processing Streams How are streams used The above discusses where streams come from and how they are transported Common things that happen in processing the stream include writing to DB cache search index or some storage system push events to users realtime dashboard emails etc produce an output stream from inputs and pipe it to another Use case 3 is in many ways similar to the batch workflow dataflow engines with one crucial difference being stream never ends Implications include sorting no longer makes sense and sortmergejoins cannot be applied Fault tolerance mechanism also need to change batch job running for minutes can be restarted stream cannot simply be replayed from start which may be years ago Common uses include Complex events processing where queries are stored long term and events from input streams continuously pass through queries and find matching ones like Herald rules Stream analytics like complex events processing but usually measuring the rate stats of some type of events like GUTS Apache Storm Spark Streaming Kafka Streaming Google Cloud Dataflow Maintaining materialized views a derived alternative view for certain query patterns Difference from above being the view usually stretches back to the beginning of time as opposed to stats in a time window which Kafka Streams also supports Search on streams like complex events processing multievent patterns there is also complex search for individual events Elasticsearch offers this Index queries such that the number of queries run is lower than number of events times the number of queries RPC systems eg in the actor model for managing concurrency and distributed execution of communicating modules Reasoning about time The average over last 5 minutes can be surprisingly tricky A batch process rarely cares about system wall clock time the time at which the process is run has nothing to do with the time when the events happened On the other hand many stream processing frameworks use local system clock on the processing machine to determine windowing which is simple but can break down if there is significant lag between event occurrence and being processed Confusing event time and processing time can lead to bad data A tricky problem with defining window in terms of event time is that you can never be sure when you have received all of events for a particular window or whether there are more events to come delays stragglers outoforder Broadly you have two options ignore stragglers and track the amount of stragglers dropped and warn if that number gets high publish a correction you may also need to retract the previous output In some cases it is possible to use a special message to indicate from now on there will be no messages with a timestamp earlier than t but tracking such from multiple producers stamping with their own local time might be tricky Taking a step back which point in time should we use If we rely on users device clock they might be wrong drift etc We could log three timestamps device time at which the event occurred device time at which the event is sent to server in case user device was not connected for a long time server time of when the server receives the event Use the difference of the latter two to estimate the server time of the first one assuming network delay is negligible compared with the difference between device and server clocks Types of window Tumbling window Fixed length nonoverlapping 0 4 5 9 etc Each event belongs to exactly one window Hopping window Fixed length overlapping 0 4 1 5 etc This provides smoothing over several windows Sliding window Fixed length any events occuring within windowsize of each other would occur in the same window This is typically implemented with a buffer of events sorted by time and removing them when they expire Session window Variable length a window of all events in a session as defined by the application Stream joins Streamstream join eg correlating user search keyword and url click in the same session Variable delay in between Note that annotating url click with search query isnt enough as that does not include cases where search resulted in no click important for your clickthrough rate To achieve this stream processor requires states eg keyed by session ID and kept for an arbitrary 1 hour Streamtable join stream enrichment Consider caching a copy of the database into the stream processor Similar to hashjoin in mapside joins The cached copy likely requires updates which can be achieved with listening to change data capture This is actually similar to streamstream join where the CDC stream reaches back to the beginning of time Tabletable join materialized view maintenance Imagine the goal is to maintain a materialized view ones twitter feeds a timeline cache where a stream of tweets is joined with follow relationship the timeline corresponds to the join of tables in a relational database a view updated by stream change These all require the stream processor to maintain some state on one join input and query that state on messages from the other join input Time order of events that maintain the state can be important if the user updates his profile which activity events are joined with the old profile and which with the new If ordering of events across streams is undetermined the join becomes nondeterministic This issue is known as slowly changing dimension and is often addressed with explicitly versioning each time the user profile changes it is given a new version identifier and events include the version identifier to join on This makes join deterministic but log compaction is no longer possible as all versions are retained Fault tolerance Restarting for fault tolerance in a sideeffectfree batch processing achieves exactlyonce semantics more accurately effectivelyonce as each record is processed effectively once Microbatching and checkpointing Break the stream into small blocks and treat each like a miniature batch process Microbatching in Spark Streaming creates implicit tumbling window Or you can trigger checkpointing without a fixed time window which Apache Flink does Side effects will not be exactlyonce Atomic commit revisited St side effects are exactlyonce This is an atomic commit approach which is used in Google Cloud Dataflow and Apache Kafka This was efficient enough within the system itself not needing to be heterogenous keep the transactions internal by managing both state changes and messaging within the stream processing framework Overhead of transaction amortized by processing several input messages within a single transaction Idempotence Alternative to distributed transactions we can have operations be idempotent or made idempotent with some metadata eg Kafka stream offset and consumer of stream only update on offset values not seen before this requires replay on crash in the same order processing to be deterministic and no other node may concurrently update the same value When failing over from one processing node to another fencing may be required to prevent interference Rebuilding state after a failure States such as windowed aggregations counters averages etc can be kept in a remote datastore or local to the stream processor and replicated periodically Kafka streams replicate state changes by sending them to a dedicated Kafka topic with log compaction similar to CDC Just replay may be faster and the tradeoffs such as this depend on performance characteristic of your system Summary Stream processing is similar to batch processing on unbounded input From this perspective message brokers and event logs serve as the streaming equivalent of a filesystem JMSstyle message broker Broker assigns individual messages to consumers consumers acks broker then deletes Old message cannot be read after processed Logbased message broker Broker assigns all messages in a partition to the same consumer node and always delivers in the same order Parallelism through partitioning and consumers track their progress by checkpointing the offset of the last message they have processed Messages are retained on disk and can be reread if needed Logbased is like DB replication log and LSMtrees Particularly appropriate for derived view generation from input streams Where streams come from Change data capture Event sourcing Representing database as streams Purposes of stream processing processing streams section Reasoning about time Different types of stream joins Fault tolerance with microbatching checkpointing transactions or idempotent writes The Future of Data Systems Even so called general purpose database is designed for a particular usage pattern Know your usage pattern and how it maps to the tools you use Cobbling together different pieces to achieve certain functionality is important In my experience 99 of people only need X probably says more about the experience of the speaker than the actual usefulness of the technology Combining specialized tools by deriving data If it is possible for you to funnel all user input through a single system that decides on an ordering of all writes generate the rest derived via CDC or event sourcing then it becomes much easier to derive other representations of the the data by processing the writes in the same order Plus idempotence makes it easy to recover from faults CDC vs distributed transactions Both can achieve the goal of keeping data in different systems in sync distributed transaction usually provide linearizability which implies the likes of reading your writes while CDC are usually updated asynchronously and make no such guarantee In the absence of widespread support for a good distributed transaction protocol logbased derived data is probably the most promising approach for integrating different data systems However different levels of consistency guarantees can be quite useful Single leader partitioned if needed in which case order of events in two partitions can be ambiguous Geographically distributed likely requires a leader in each location which implies undefined ordering of events that originate in two different datacenters In formal terms deciding on a total order of events is known as total order broadcast which is equivalent to consensus Most consensus algorithms are designed for situations in which the throughput of a single node is sufficient to process the entire stream of events and these algorithms do not provide a mechanism for multiple nodes to share the work of ordering of events Batch and stream processing Batch processing has a quite strong functional flavor it encourages deterministic pure functions whose output depends only on input and has no side effects Inputs are immutable and outputs are appendonly This makes reasoning about dataflow as well as faulttolerance easy Reprocessing data for application evolution Stream and batch processing are both useful in schema evolution Derived views allow gradual migration you can maintain both versions of the derived view as you migrate its almost like railway gauge migration pythonpolyglot where different track distances are standardized by having a third rail such that trains for both gauges can run The lambda architecture Combining batch processing and stream processing say run Hadoop MapReduce and Storm sidebyside Stream processor consumes the events and quickly produces an approximate update to the view and the batch processor later consumes the same set of events and produces a corrected version This somewhat duplicated work has its shortcomings Recent work overcomes this by having batch computations and stream computations implemented in the same system Unbundling databases The similarities between operating system and databases are worth exploring At their core both are information management systems stored at file or records level Hadoop is presented earlier as somewhat like a distributed Unix OS hides away the hardware provides files pipes whereas distributed databases hides away the disk storage concurrency crash recovery and provides distributed transactions SQL etc The author would like to interpret NoSQL movement as wanting to apply a Unixesque approach of lowlevel abstractions to the domain of distributed OLTP data storage Composing data storage technologies Secondary indexes materialized views replication logs fulltext search indexes There are parallels between these database features and the derived data systems that people are building with batch and stream processors In a sense they are almost like elaborate implementations of triggers stored procedures and materialized view maintenance routines Federated database unifying reads One unified query interface for a wide variety of underlying storage engines and processing methods Unbundled database unifying writes Make it easy to plug together storage systems eg through change data capture and event logs is like unbundling a databases indexmaintenance features in a way that can synchronize writes across disparate technologies The author thinks the traditional approach to synchronizing writes requiring distributed transactions across heterogeneous systems is the wrong solution compared with change data capture The big advantage of logbased integration is loose coupling between the various components Outages are easier to isolate and handle and different components can be developed improved and maintained independent of each other The advantage of unbundling and composition only come into picture when there is no single piece of software that satisfies all your requirements Whats missing The tools for composing data systems are getting better but the author thinks a major part is missing an unbundled equivalent of Unix shell ie a highlevel language for composing storage and processing system in a simple and declarative way Like mysql elasticsearch which is like an unbundled way of CREATE INDEX Designing applications around dataflow Like formula in an excel sheet derived view that automatically updates when a dependent cell changes derived data view should have the same Application code as a derivation function triggers stored procedures userdefined functions which often came as an afterthought Separation of code and state Like web servers who typically put states in DB and have code hosted by a web server running in cluster management tools like Kubernetes Docker etc Thinking about applications in terms of dataflow implies renegotiating the relationship between application code and state management Instead of thinking DB as passive variable modified by the application we think much more about the interplay and collaboration between states state changes and code that processes them Comparing this with a microservice approach eg when a user makes a purchase of goods priced in one currency but paid in another currency the microservice approach the code that processes transaction would query an exchangerate service to obtain the current exchange rate in the dataflow approach purchase processing would be one stream processor exchange rate would be another stream processor and the above becomes a join local cached rate query timedependent join Observing derived state The read path and write path encompass the whole journey of data Write path caching eager evaluation Read path lazy evaluation The derived dataset materialized view cache is the place where the two paths meet and there is a tradeoff between the amount of work needing to be done at read or write time Stateful offlinecapable clients Ondevice state becomes a cached version a materialized view of that on the server The offline device once reconnected will behave like reading off of an offset in a logbased message broker Websockets EventSource API gave http server pushing capability who otherwise had been a simple polling protocol This extends the write path all the way to the end user in which case we need to rethink building of many of our systems moving away from requestresponse interaction and toward publishsubscribe dataflow Keep an open mind on both as a designer of data systems Reads are events too Its possible to have event log only store writes to the database but also possible to have it store read events as well In some cases reads queries contain information to be stored and analyzed Writing reads makes it easier to track causal dependencies Using stream processors to implement multipartition data processing Probably simpler to use a DB that provides multipartition query joins but more customizable and flexible to implement this with stream processors Aiming for correctness is no dirty writes an atomicity consistency linearizability and isolation guarantee is linearizability the strongest form for replica consistency The furthest in line in eventual readyourwrite consistencyprefixread causal does linearizability imply atomicity how does it not imply serializable isolation the weakest form of isolation read committed also implies no dirty writes Is atomicity any more than no dirty reads no dirty writes read committed are distributed transactions and replica consistency connected problems does full write broadcast and rwtotalnodes achieve linearizability does 2PC give you anything on top of synchronous singleleader replication"},{"title":"\"Fooled by randomness\"","href":"/notes/fooled-by-randomness","content":" Fooled by randomness Thoughts on fallibility of human knowledge by a former options trader Part I Luck is often mistaken for skills probability mistaken for certainty Croesus wanted to impress Solon with his fortune and asked Solon who the happiest man is Solon suggested one who lived a noble life and died for their duties Reason being You asked me a question concerning a condition of humankind happiness I reckon 70 years to be a long life Of those 26250 days no two will be the same We are wholly accident Good fortune is always mixed with misery In the journey of our lives there is an infinity of twists and turns and the weather can change from calm to whirlwind in an instant We can never know what might come next The gods are jealous and like to mess with mortals Sometimes we get a glimpse of happiness and then are plunged into ruin Yes you are fortunate wonderfully rich lord of many peoples But with respect to the question you asked I have no answer until I hear that you have closed your life happily Monte carlo random process alternate history Why me It is not natural for us to learn from history Hindsight bias Over a short time increment one observes the variability of the portfolio not the returns A trader making money does not mean he is good Similarly passing a particular interview does not mean you are good Dont get your causality reversed Our emotions may not be designed to understand such Ultimately news media is an entertainment industry We often mistake probability for expectation high chance of market going up does not mean you should long if in the rare case of it going down its going down by a lot This asymmetry skewness is often times counterintuitive considering that we are used to symmetric bell curves in everyday lives Consider this is market statistics even stationary as we observe the distribution itself may shift If the distribution is not stationary can past data even be used to forecast future performance if so to what extent Consider this statement I have just completed a thorough statistical analysis of the life of president Bush For fiftyeight years close to 21000 observations he did not die once I can hence pronounce him as immortal with a high degree of statisical significance Rare events do happen If the past by bringing surprises did not resemble the past previous to it then why should our future resemble our current past Extreme empiricism competitiveness and an absence of logical structure to ones inference can be quite a explosive combination A scientific hypothesis is falsifiable Newtons Laws are science and falsified astrology is not as its not falsifiable Karl Popper would claim there are only two types of theories those that are known to be wrong falsified and those that have yet been known to be wrong Part II Survivorship bias Ergodicity roughly under certain conditions very long sample paths would end up resembling each other Remember that nobody accepts randomness in his own success only his failure There is a high probability of the investment coming to you if its success is caused entirely by randomness Say an actively managed fund with an outstanding track record due to pure randomness Is it really a small world The world is much larger than we think it is just that we are not truly testing for the odds of having an encounter with one specific person in a specific location at a specific time Rather we are simply testing for any encounter with any person we have ever met in the past and in any place we will visit during the period concerned Think birthday paradox get 23 persons in a room there is about 50 chance of two having the same birthday Seems large no Remember we are testing for any pairs Consider backtesting particular strategies on historical data you might be fitting the rule on the data and the more you try the more likely by mere luck youll be able to find a rule that worked on past data eg correlating stock markets with length of womens skirts or finding that one instrument whose movement is perfectly correlated with weather in Ulan Bator You might be throwing monkeys at typewriters not telling them what book to write If you throw enough monkeys one of them will produce Illiad But how likely is he to then produce Odyssey Stock analysts have both a worse record and higher idea of their past performance than weather forecasters The sample size matters Our emotional apparatus is designed for linear causality for instance you study everyday and learn something proportional to your studies If you dont go anywhere you will be demoralized But the reality rarely gives us the satisfaction of a linear positive progression you may study for a year and learn nothing then unless you are disheartened and give up something will come to you in a flash This summarizes why there are routes to success that are nonrandom but few very few people have the mental stamina to follow them Those who go the extra mile are rewarded Most people give up before the rewards An example of biases in understanding probabilities a test of disease has 5 false positive the disease strikes 01 of the population We test people at random and someone turns out positive Whats the probability of someone actually having the disease By Bayesian formula its about 2 Usually smaller than our estimates On Bloomberg it acts as a safe email service a news service a historical data retrieving tool a charting system an invalubable analytical aid and a screen where I can see the price of securities and currencies I have gotten so addicted to it that I cannot operate without it as I would otherwise feel cut off from the rest of the world I use it to get in contact with my friends confirm appointments and solve some of those entertaining quarrels that put some sharpness into life Somehow traders who do not have a Bloomberg address do not exist for us Part III Wittgensteins Ruler when a criteria such as our opinions of something or a book review speaks more about the criteria itself rather than the subject being reviewed Randomness and personal elegance Flexibility around a schedule and happiness"},{"title":"\"Nineteen eighty four\"","href":"/notes/nineteen-eighty-four","content":"1984 Newspeak Ingsoc Doublethink Thought Police the Party Big Brother Oceania Eurasia and East Asia Airstrip One Ministries of Truth Love Plenty and Peace Goldstein two minutes of hate Inner party Proles Victory gin Chestnut Tree Cafe Mr Charringtons Shop Part I Winstons life as a forger in the ministry of truth Part II Winston and Julia Mr Charringtons place Part III OBrien Torture and rehabilitation"},{"title":"\"Philosophy ideas\"","href":"/notes/philosophy-ideas","content":" The Great Ideas of Philosophy Questions of Knowledge Conduct and Governance Did the Greeks invent it all What differentiates the works of Pythagoras Plato etc from those of Homer Sophocles Confucius Buddha etc In philosophy there is a critical disinterested skepticism towards the principles we hold and even mandates of God The pursuit would be wisdom itself not wisdom for the sake of something else And a philosophical realization made through skeptical analysis and debates would not end with an exclamation mark but with semicolon The Greek civilization of 6 Century BC would not have been more powerful than the Egyptians or Persians yet the latter were not considered to have laid the foundations of Western civilization How developed the thoughts are of a people country culture does not necessarily reflect how wealthy powerful the group is their ability to solve practical problems and influence others Nor would philosophy be the product of a leisurely Hellenistic upper class built upon labor of slaves and indentured servants Egyptians and Persians would have been better suited in that regard Greek religion tends to have an estrangement Gods are revered but aloof and indifferent towards our matters The fundamental questions of being would not be something you consult the Oracle for but rather left to reason about by ourselves We ourselves need to make order classification out of the chaos of this world The state would be religious but without a state religion Imagine us as puppets made by the Gods with various cords in it these cords reflecting pleasure pain and emotions pull us in different directions One cord is sacred and golden that of reason and calculations And when one follows this cord one is virtuous Plato The Laws Pythagoras and the divinity of number Abstract transcendance philosophy vs natural philosophy would be the focus of later Greek Aristotle philosophy Pythagoras rich family shrouded in mystery Learnt maths fractions calendar predicting Nile flood from his Egyptian travels Rumor has it he traveled and taught in Babylon India Spent most of his life traveling came back to Greece as a governor and founded his sect Pythagoras did poorly as a governor A philosophicallyguided state may not be well suited to the brutal reality Cynics are an important PreSocratic philosophical school Diogenes Nothing too excess Pythagoreanism abstract idea is the ultimate creator of material and physical realities and that idea is numbers hence divinity of numbers in particular 1 point 2 line 3 plane 4 solid tetrahedron special place in Pythagorean teaching sum is 10 also sacred in Pythagoreanism The lawfulness with which we enter social life is based off of numbers Also known for his theorem on right angle triangle and himself his schools contribution to discovery of harmonic structure of music harmony matches with something in the soul what creates a harmony is the relationship between notes part of the intended design of the cosmos Over time we wont exist but rectilinear triangle will mathematical abstractions will and will remain correct precise in describing the relationships in physical world Also believes in transmigration of souls death liberates something in us Pi e sqrt1 has no place in Pythagoreanism Sophia wisdom itself and Pythagoras considered himself a philosopher as he strived to and have befriended wisdom itself Elegance and simplicity expressed in mathematical formulaic equation What is there metaphysics A metaphysical question Real object perception and distortion what is beyond there If our senses always lie how would we know reality Aristotle notes that they have just completed a systematic consideration of nature the physical world and what is natural Centuries later people notes his work after the physics treatise as metaphysics after the physics natural science work Metaphysics deals with the subject of real existence What is there ontology being the branch of metaphysics that deals with reality and existence After change does the original still exist Cause and effect Are there minds and thoughts or just peculiar states of the brain are they merely terms hankering to superstitution Does what really exists vary being by being ie sensory apparatus What sort of being am I What am I made of made for Our modes of knowing epistemology Epistemological systematic organization and principle of facts questions are senses wrong If so what is right Abstract rationality of Pythagoras Customs Religion Epistemology is the study criticism and refinements of our modes of knowing Epistemic justification Is there heaven Is there such a thing as goodness Is there a moral reality and objectivity Is there real beauty Is there truth Is there right or wrong The conclusion we reach is more or less based on the method we choose often times blindly by habit social condition or conventional wisdom There tends to be a vicious circularity between the claim ontology and the methods epistemology Certainly the harmony and balance Greek perceived as beautiful is not the same as what medieval artists found beautiful in their depiction of human form Yet the Greeks philosophers were unaware of the shifting Democritus ultimate reality is but an incredibly large of atoms Atoms and the space in between the void Everything is ultimately reducible to that level What we see as flowers buildings animals are but ephemeral different in atomic composition The soul is a finer kind of atomics structure Earth air fire and water Can we even answer the ontological questions Beautiful or ugly True or false Moral or not Heraclitus no one descends twice in the same river Sees nothing but flux and change Protagoras preSocratic Founding father of Sophist thought Man is the measure of all things Judgment of any form must have some grounding and that grounding can only be the experiences of a lifetime we cannot take an epistemological position external to our own human ways of thought and feeling If there is a standard independent of human nature we cannot even comprehend it Hence is pursuing truth a misguided objective Each person is the measure of all things Truth is subjective We are not equipped to comprehend it even if there might be objective About the gods I cannot say if they are or how they are constituted in shape the unclarity of the subject and the shortness of human lives I can write you about what I see and hear and touch but I cannot write you about the gods Socrates spends much of his work disputing the Sophists much can be questioned about the claim Each man not only is not the measure of all things we are generally very poor in understanding ourselves With him philosophy becomes a humanizing and humanistic enterprise the human condition from which there is no retreat Greek tragedian on mens fate Dance is important in Homer Ovid and other classical authors Dance of cranes Theseus Chorus Dance Participants and preliterate history and moral thought Out of possibility of drama comes the dispute and dialogue Philosophy may have come from cultural activities such as drama The thought refining itself The human condition as understood by the dramatists Is Medea a murderess or are her motives irresistable Her sorcery practices are of Chthonic and preOlympian When we surrender reason to passion chaos ensues What about the trial of Orestes The debate on Nomos norm natural law Antigone Is the law of kings higher or the nature that sisters should bury brothers Euripides heroic characters are very much human At the end of day character is destiny Aristotles view on women vs that of Greek tragedians Aristotles view on tragedy bad things happening to good people If theres an all seeing and all loving god why such evil and injustice around us Herodotus and history Herodotuss Histories attributes war to irreconcilable differences in value Legions are moved with words and symbols Histories describes various peoples dress weapon food economonies and religious belief to give a full perspective on events of historical significance as opposed to mythology where such accounts on humanistic details are often glossed over Herodotus would suggest to account for events of historical significance perspectives on sociology pyschology beyond mere chronology is required And his teaching or that of history would be meaningful beyond ethnicity and calls onto the roots of humanity itself Histories has clear distinctions for Herodotuss own opinions opinions he has heard and facts His work however has many inaccurate accounts to establish some morals on the question of conduct Eg the emblematic meeting between Croesus and Solon where the happiest men are established to be Kleobis and Biton two sons who yoked themselves to their mothers cart such that their mother a priestess would not be late for the festival of Hera They died peacefully after the deed Solon would claim only after ones story is complete can one be judged whether their lives being happy or not and human happiness is not dependent on wealth Socrates on the examined life Teaching of Socrates is preserved by Zenophon and Plato broad shoulders who claims to be the mere scribe but his works are much more a portrayal of his and Socratess thoughts via Socratess mouth than mere description Socrates lived in a time when Athens lost the Peloponnesian War against Sparta and as a loyal faithful soldier of Athens his thoughts are very often on the practical side what went wrong with Athens in matters of educating the youth of conduct and government The Dialogues are conducted by the losing side which often explains him taking a Spartan view on the matters above Socrates described himself as a gadfly asking those confident in their thoughts reminding us of the gadfly sent by Zeus to unseat Bellerophon from Pegasus an analogy for unseating the confident writers He was well trained by his Sophist teachers sophistical teaching known for their skills in debate and rhetorics and to expose the ignorance of the interlocuter via whimsical and deft conversations Yet Socrates aimed to defeat the skepticism and cynicism claiming that there is something we can truly know and knowing that our methodology of knowing is sound contrasting his thoughts from those of Pythagoras St Augustine would consider Socrates the only true philosopher one committed to living and dying by his own philosophy the inquiries he perceived as true and the cause of reason not like Pyrrho Pyrrhonism a school of skepticism Socrates was found guilty of failure to respect the Gods despite him being a reverential person and corrupting the youth The Symposium which has inquires on the nature of love ended with Socrates returning home alone Neither charges were truthful and Socrates had the choice of death or exile ostracism of at least 10 yrs on which Pericles was also charged and choose the former hemlock poisoning in defense of his thoughts and the rule of law the public expression of rational thoughts reason without passion A Homeric ending think Hector breaker of horses where upon death he uttered will you remember to pay the debt Socrates would teach the unexamined life is not worth living What is wrong with the unexamined Socrates would describe it as a screen on which events are laid out and not a lived life as prisoners living in a cave watching parapets on which the shadows of puppets are cast where all they see is shadows and illusion Examine in the sense of interpretation of meaning of events integration of experience subject to the refinements through self criticism and introspection and making the thought whole This connects back to the motto at the Oracle of Delphi know thyself meaning knowing what it means to be a human being Beyond the biology constituents etc The Socratic agenda would argue against skepticism and cynicism and realizing the interconnectedness between matters of knowledge conducts and governance Dealing first with knowledge and establishing a good philosophical basis that there is something we can know one kernel of truth we can know if we cant know anything as the skepticists claim then the life is uselessly examined prejudices and self deceptions If it is possible to know something know that you know it and know the methodology of knowing it then we can inquire into what kind of life is right for beings such as ourselves Happiness and pleasure Are they the same for all people all cultures What about our values Are they just relative opinions and prejudices What about governance how should we be governed Are the core precepts of Athenian democracy to be questioned Socrates would question Athenian democracy which Aristotle would later defend claiming the collective wisdom is more likely to make good decisions as opposed to the strength of one and the weaknesses are likely to cancel each other out Plato and the search for Truth To be able to arrive at any truth one first defeats the skepticism claim that nothing can definitively be known Platos Meno deals with the search for Truth where Socrates converses with Meno a noble from an area with strong Sophist presence on whether virtue comes from teaching or nature Socrates in Meno instructs by guiding the barbarian servant boy of Menos to discovering the Pythagoras theorem And claims knowledge is a form of reminiscence where through philosophical guidance one is no longer clouded by the sensory systems of the material world and recalls the Truth that the soul had always known Hence Plato establishes that Truth is different from facts where the former is eternal not sensory and cannot come by perceptions and the latter is ephemeral material and as Heraclitus would claim always in flux as the material world would be Facts are various forms of rectilinear triangles drawn on the sand flux ephemeral and material and Truth is Pythagorean theorem that a2 b2 c2 is the true form of rectilinear triangle Aristotle would argue this is Platonic and not Socratic Platos work is also divided by later scholars into early mid and late where some ideas came earlier are later revised How is Truth acquired One has it intuitively The gift of rationality Plato draws much from Pythagoras in illustrating Truth with his discovery and in transmigration of souls where he would claim with death the soul is liberated Plato considers mathematical Truth an example of knowledge we know for certainty and where the skepticists are wrong but the debate would continue with evolution on both sides To Plato the problem then becomes identifying mathematicslike Truth and it can be discovered by the dialectical approach to achieve something abstract and uncover what one has always possessed but became clouded by the material world To this end it is also thought that the discovery of Truth has naught to do with experience and the experience of many do not necessarily draw us any closer to Truth than the experience of one On this ground Socrates would question the basis of Athenian democracy Can virtue be taught Platos Protagoras Problem of conduct to each his own Vicious hedonistic Is there truth to ethics morality Is there moral objectivity The Socratic Platonic school would combat the skepticists in the above domain claiming the problem of conduct is not to be solved at the level of merely personal desire The same argumentative approach to knowledge should be applied How life should be lived What kind of life is right for beings such as ourselves Platos Protagoras Setting Protagoras is in town a young 30s Socrates put several questions to Hippocrates not the physician who woke him up to this news Socrates then raises the questions to Protagoras who eloquently discourses on virtue conduct etc Socrates and Protagoras then engaged in Socratic dialectical method and hard to say who had the upper hand What do you want to be that makes you want to study under Protagoras Protagoras claims moral excellence What is virtue Justice temperance courage wisdom etc what are they Parts of virtue Parts in what sense Is virtue teachable Both men agree knowledge is the greatest and there must be knowledge first before there is virtue Knowledge known in some nonsensory way How do we teach anybody anything We teach by providing sensible visible ostensive objects Anything taught by showing cannot be universal What do you point to to say that is virtue to teach virtue The teaching of technique vs the teaching of Sophia You cant show the universal but you can show an instance of it Perhaps we can teach virtue by pointing to persons practicing virtue Point Leonidas at Thermopylae to a one year old and he wouldnt be able to comprehend Socrates argues to the conclusion virtue cannot be taught as such Perhaps the knowledge wisdom attained by philosophical reflection is the grounding of virtue This is teachable The oneyearolds arent ready for it many arent ready for it in their entire lives The students must have been prepared to be receptive of such lessons then you may find such persons resonating to the virtuous act when it presents itself What kind of preparation is necessary for one to be considered ready for instructions by the virtuous This is discussed in The Republic Theaetetus is another piece where Protagoras is a central figure and claims men is the measure of all things I will do things that please me At the end of the day the problem of conduct is a problem of principle something universally right The Socratic approach to knowledge would claim each man is not the measure of all things There is a measure of all things and its the task of each man to understand that measure and apply it properly Platos Republic Latemiddle dialogue Russell on the Republic fascist and imperialistic Foundational work in political science Tied to the considerations of statecraft are observations on moral justice weakness of human hence somewhat foundational in psychology Sepheles near death and ponders on wealth Polymarchus what it means to be just is to pay ones debt Can an unjust person be happy and make great progress Thrasymachus strong should prey on the weak Glaucon what is goodness Three classes Things that are good in and of themselves Harmless pleasures Things that are good in themselves but also for the consequences that arise from such good Knowledge health Pursued for the same of their results Gymnastics money making Where is justice placed Socrates the first where things are pursued for their intrinsic value not for consequences Glaucon Adeimantus Those seem just may not be just Virtue may arise purely from fear of punishment and desire of reward not for the sake of being virtuous Example of Ring of Gyges invisibility used to commit unjust Being just may be for reputation not for the sake of justice Socrates suggests to inspect the state instead so that we can seek justice on an enlarged scale rather than on each flimsy individuals No claim can be made towards politics without understanding human nature Guardians Elimination of greed no payments Shared partners Marriage lottery in festivals but actually chosen breeders Breed the guardians Pure eugenics Problem of knowledge Allegory of the cave Being shackled and faced with parapets with projections shadowy illusions Each man his own measure Imagine one breaks out discovers the outside and realized what theyve perceived as true are merely halluciatory experience This one goes back to share his experience the others wouldnt be able to comprehend and instead thought him as blinded by the light Being bound by our material selves resisting transcendance believing whatever we see and only what we see Ignorance is darkness See the reality behind the appearance of shadowy illusions To appreciate comprehend Leonidas at Thermopylae one needs to be preconditioned The eyes and ears never record the truth and what they do pick up will never be parlayed into the truth Being experientialist while missing the point of life not even understanding life has a point There are some things that only be seen under the light of philosophical examination the wise mans guidance Problem of knowledge then gets us back to the search of truth of the relationship as the relationships constitute the true form of things What is a good government is then answered in terms of relationship between the ruled and the rulers laws where authority is vested and for what purpose Justice is also perceived as the the harmonious relationship among the rational the passionate the emotional dispositions of the soul Also behavioral theory eg children has to be protected from vulgar music but exposed to martial music How should we behave How should we live our lives We are corporeal inclined towards pleasure and avoid pains We are like charioteers standing on two horses a good horse and a vicious one a metaphor also found in India A will capable of resolving us to follow the right course The will itself cannot decide what the right course is desire knows only one course to fulfill itself How then do we discover the right course through rational power Supremacy of reason in determining what we ought to do Reason as shown in mathematical proportion harmony balance The guide and goal of life Avoid excess The Pythagorean numerological balance is the resemblance of truth Hippocrates of Kos and Greek medicine Contemporary of Socrates Plato Aristotle whose father was a physician movement in the direction of natural science Secularize and analyze a subject that is generally related with religion and customs Greek medicine is close to modern practices in its being scientific realizes diet hereditary disposition as causes for conditions Health as related with the state of the body as opposed to religion and customs Few or perhaps none of the writings of Hippocratic school were by Hippocrates himself Pythagoras sect of medical views treat the body with vegetarianism ritual performance musical harmony and exercises Hellenic civilization objectified itself and the natural world for the purpose of critical scrutiny Secularizing of knowledge as opposed to realizations brought by prophets Hippocratic writers are religious but they hold the religious account constant They provide a science that is liberated from religious orthodoxy Hippocratic medicine is not necessarily superior to Egyptian medicine which dates even earlier but it is based on an entirely different set of suppositions Once a society confers upon a selected group ultimate epistemological authority on core questions arising from the problem of knowledge the nearly inevitable result is philosophical paralysis positions become quite hardened and the only work left for scholars is to interpret the words of the wise The debate is no longer about knowledge and truth but how a text or holy maxim is to be understood Empirical Empiricist The Greek word observation Science is an empirical enterprise Hippocratic writers describe themselves as empirical as their treatment is based off of clinical observation naturalizing the natural world How is apoplexy understood Eastern Christianity influenced by Greek thoughts perceives it as a viral attack on the body empirical Western Christianity much more theological theory driven perceives it as retribution for the victims wickedness divine spiritual intervention Hippocratic writers correctly identified the brain as the central organ that processes sensory information being correct whereas Aristotle was not heart while brain controls the temperature of the blood The celebrated figures of this age Socrates Hippocrates Aristotle etc were a conspicuous minority This would be an uncommon group in any age The mass of the people has a deep suspicion towards philosophers The perspective developed by these few was not widely shared There was a perfectionist idea wide spread in Hellenistic civilization In rhetorics tragedy physical competitions architecture works of art etc Similarly in mathematics and philosophy Hellenism bequeathed this pursuit to later societies Aristotle on the knowable Aristotle has so much original hence so much criticism from those who came after His role in logic biology physics natural science political science and metaphysics are fundamental He spent 20yrs in Platos Academy where some small fraction of his teachings are present in Platos dialogues Aristotles Metaphysics proposed many thoughts different from Socratic and Platonic view It offered a presentation of preSocratic and Socratic schools of thoughts serving as a good account of the history of philosophy and Hellenic thoughts in general Aristotle usually inspects the historical work done what is left wanting and goes from there On senses Metaphysics opens with All men by nature desire to know An indication of this is the delight we take in our senses This sets a fundamentally different tone from a Platonic dismissiveness towards senses Nature produces nothing without a purpose and senses do not exist to deceive creatures Perception must be the starting point of knowledge On classification Aristotle proposes different perceptual modes of knowledge Whats the fundamental power Greek word Dynamics in which a living thing has life Nutritive A living creature has the means to absorb nutritions from its surroundings to sustain itself Reproductive A living species can reproduce Locomotive Some living creatures have the power to move Some plants dont Sensation The ability to act knowingly consciously Animals possess this Rationality The soul psyche power of reason a special rationality cognitive power to allow beings to grasp universal propositions all men must die vs that men died Our laws is another example of generality Aristotles view on knowing also differs from Platos As opposed to Platos Meno in which he talks about facts and Truth with the latter being awakening something we inherently know via dialectical methods Aristotle claims there are two modes of knowing eg the angles of a triangle sums up to 180 By measurement This is by experience and factual By definition This is episteme and appeals to generality On the cause Happy is the man who knows the cause of things Whats the cause of a statue On the material level theres the stone It couldnt have existed otherwise A statue possesses certain features to differentiate it from just any stone This is the cause on the formal level Then theres the efficient cause of one blow after another by the sculptor where each blow is the cause for the next Finally how do you know where to hit the material You have to know what you aim to achieve Then the ultimate final cause is that intelligent design Final in time first in conception Unless you have an intelligent plan to begin with otherwise nothing will come about Greek telos A teleological explanation identifying the purposes plans designs and goals Teleological does not assume sentient beings Evolutionary theory is teleological where wings mating behavior etc serve specific purposes We do not understand something fully unless we know all four modalities The number of things we know are based on the questions we ask does it exist If it does to what degree In what relation does it stand to other things What is it for The central point of Aristotelian program are these four causes questions in domain of knowledge politics ethics etc In politics what is the polis for In ethics what kind of being am I How do the actions of my mine either realize what is potential within me or stultify such potentials What am I here for my potentials How do I live my life to honor the central fact of my being Living things of the universe fit in a plan Nothing with pattern or design comes about accidentally If the art of shipbuilding were in the wood ships would exist by nature Ask the ultimate question what is it for To know in this sense is to comprehend far more than anything conveyed by the mere composition of the object What are atoms for Knowing all are made of atoms is pure materiality and does not rise to the level of episteme Aristotles explanation is usually universal by and large in general and deterministic Eg his view on scientific theory needs to be general where the explained event is a particular instance"},{"title":"\"Sapiens\"","href":"/notes/sapiens","content":" Sapiens Cognitive revolution What separates homo sapiens from homo erectus or neanderthal The former is the predominant human race while the DNA of the latter can hardly be traced to modern human beings some neanderthal DNAs were able to be found to modern day human beings Homo sapiens were hunter gatherers foragers Ache society in Paraguay are hunter gatherers living to this day with the tradition of killing off the old and the infants who cant keep up with the tribe Yet do we live in a happier society than them The cognitive revolution saw homo sapiens brains being able to process something purely fictional like laws or the United States of America This power to process something imaginary lets us grasp far more things than whats encoded in our DNA The course of natural evolution runs much slower than that of our cognitive revolution In turn such understandings are not preserved by the species its not in our DNA rather it dies as the individual bearing it dies Cognitive revolution allows homo sapiens to band together in much larger numbers under shared myths eg that of religion or nationalism and collaborate in ways far more flexible than whats encoded in our DNA Agricultural revolution Did homo sapiens domesticate the wheat or did wheat domesticate homo sapiens Our bone structures were not evolved naturally to take care of wheat nor our lifestyle Is the success of a species to be judged by the average of individual happiness or by how many replicas of its DNA it was able to reproduce Homo sapiens especially after the agricultural revolution are hugely successful in terms of the latter but may be questionable in terms of the former There is no going back in the agricultural revolution The collective myths Objective subjective intersubjective Something prevalent in the minds of a multitude of subjects does not make it objective The myth of modern time could be considered as that of consumerism and romanticism broadening our horizon via consuming different kinds of experiences Eg that of traveling abroad An Egyptian noble would probably not consider travelling to Babylon with his wife a desirable form of entertainment theyd much rather be building an expensive tomb perhaps Summerian writing Catalogging A scribe system Scientific revolution We know that we dont know Falsifiability Monarchs social elites and the like believe in the benefits of technological advancement and find their investment in such worthwhile Imperialism an amalgam of different cultures and without clear definition of borders Think Athenian Roman British empires Capitalism whereas those before Adam Smith would have us believe the economy pie is limited in size and an increase of my share comes at a cost of decreasing those of others capitalism would have us believe the pie grows by reinvesting earnings into growth By encouraging consumption and reinvestment the economy grows Humanism that of the liberal individuality socialism collectivism and evolutionary facism branches Religion Animism Polytheism is generally acceptive to foreign beliefs deities eg the Romans didnt mind adding the Egyptian Isis to their Pantheon What about Roman prosecution of Christianity It was mostly a halfhearted effort and early Christianity sometimes come with political insurgence Polytheism usually features one ultimate power to whom there is no use in praying eg as in Greek mythology each god has their speciality and take an interest in human affairs but they all are subject to fate which has no particular interest in humanity or gods Or in Norse that of Ragnarok Monotheism is far more exclusive and usually features one ultimate power who takes an interest in human affairs eg the love of God for human By the way one key difference in Protestantism vs Catholicism is in how to ascend to heaven after death Protestants would claim that Gods love is so great that one can go to heaven as long as one has faith Catholics would emphasize the importance of deeds church activities etc Missionary Natural order religion like Buddhism Craving leads to lives being trapped in the cycle of karma By giving up craving treating pain and joy with indifference one can achieve nirvana the ultimate inner peace World War II arguably is very much a war of religions rather than different sects of a traditional religion such as Hussites vs Catholics or Sunnis vs Shiites its liberal humanism vs evolutionary humanism and Cold War being liberal humanism vs social humanism Seeing whatis often times make us take whatis as granted there is no other way Ones lifespan is usually so short and horizon so limited that there is every other way which one may not have the eyes to see"},{"title":"\"Starting strength\"","href":"/notes/starting-strength","content":" Starting Strength neuromuscular properly performed fullrange motion barbell exercises are essentially the functional expression of human skeletal and muscular anatomy under a load Barbell Posterior chain training Good technique in barbell training is defined as the lifters ability to keep the bar vertically aligned with the balance point midfoot Femur thigh bones Tibia lower leg bones Calf lower leg muscles Ham string upper leg muscles Adductors skeletal muscles located in the thigh"},{"title":"\"Story of human language\"","href":"/notes/story-of-human-language","content":" Language Language is not the same as communication imitation both in the latter animals can do but the former seems unique and genetically programmed to homo sapiens A gradual evolution 100k years ago Chomsky hypothesis language is something we are genetically programmed with as opposed to culture societyspecific phenomenon Speaking a language is not particular to certain areas all humans speak some languages not the same can be said for writing The ability to learn new languages degrades as one gets older another characteristic of something a species is genetically programmed to do There are over 6000 spoken languages and over 200 written Language comes in various forms some exotic features include clicks Khoisan languages of Botswana and Namibia X C Q pronounced with a tongue click where having one to four clicks before a word mean different things or having one word that others would express in a long sentence Language change over time The writing changes slower and the sound changes happen faster typically in following ways Assimilation least effort to make the different sounds Example being inpossibilis in Latin to impossibilis as m is closer to the following p Consonants devolve they become softer or omitted over time Examples being Latin into French Italian Spanish Portuguese Romanian The change of t and d k and g removal of consonant sound etc aqua Latin agua Spanish eau French Vowels devolve and shift over time The Great Vowel Shift starting in the 13th Century the vowels shifted up ah ay name as nahmeh to its modern day pronouciation is a result of this shift and devolve the last vowel Modern written English still reflects how the language was pronounced before the shift oo as in food was a long ou but now an u feed was an ae but now i The language we speak today also shifts eg aw ah in some areas They started out as accents and gradually the general populace talk in different ways Meaning of words too shift awesome and lame in the 60s vs now With the sounds devolving over time a language does not become fumbling of vowels over time as new words are constantly being created Some languages work with tones others with suffixes prefixes Tones presumably could have come from words devolved over time Imagine pa pas pak meaning different things in a language over time the consonant devolves and the only thing differentiating these would be tone Grammatical words and concrete words Walk eg is concrete nepas eg is grammatical Grammaticalization over time concrete ones devolve into grammatical ones Example being the French pas where it used to be that ne by itself means negation and pas had a concrete meaning of a step like il ne marche pas used to mean he doesnt walk a step Over time this concrete meaning devolved and pas was appended meaning general negation with ne ne can be downplayed in Spoken French imagine someone else transcribing this language they can derive the impression that pas alone means negation Suffixes and prefixes and tones too are results of grammaticalization Latin has different verb forms for I speak you speak and he speaks which is resulted from the phrase speak I will etc and the Iyouhe will parts were different They were glossed over over time and the verb speak with different suffixes became prevalent Rebracketing is another common sound change where the boundary of two words became different My Ellie turned into Nelly My Edward turned into Ned all one turned into alone Semantic change Semantic drifts meaning change over time too Silly used to mean blessed and the German root still means blessed Silly Virgin Mary means blessed innocent Virgin Mary in the 13th Century Gradually it shifted to mean innocent in Shakespeares time Innocent is connected with needing some compassion and in our time it means not very bright Nice used to mean finegrained small pieces as in nicety but means pleasant in our time In this way bear to carry a burden shares the same root as birth th is grammaticalization of another word like warmth for warm and transfer prefer borrowed from Latins protoIndoEuropean root of bear fer Semantic widens and narrows meat used to mean all food but has specifically come to mean animal flesh Bird used to be specific and foul was general but now the semantic of bird is widened Word order change Modern languages have all combinations of subject verb object order Both SOV and SVO are very common historically change from SOV to SVO has been common while the other direction has been rare Modern German Turkish old English biblical Hebrew are all SOV languages while modern English and modern Hebrew are SVO Welsh Irish Gaelic Polynesian languages are VSO OVS is rare some South American native language uses it An Australian native language has no fixed order Suffix and context differentiate subject and object Language studies words sounds grammar and meaning change over time Patterns of change Language is constantly morphing not for good or bad Some directions of change are predictable eg th in English to t f or s Some semantic decay can be seen in lielay inferimply hitherthitherwhither thou lookstthee look is shotshotten spitspitten in modern English Earlier pronounciation books recorded sound became available only in the 1870s suggested dismaydismiss should be pronounced dizmaydizmiss and balcony has emphasis on co Modern English says a house is being built while 19c English may say a house is building The Roman Empire is unique in its efforts of Romanization ancient empires eg that of the Persians rarely impose that their language be the only one taught in conquered territory Latin uses noun suffixes to indicate object and doesnt have to a the etc Modern Romance languages dropped these noun suffixes Some common patterns of change eg grass in Latin herba h is usually fragile in Romance language French herbe Spanish hierba pronounced yierba Italian erba Portuguese relva Romanian iarba English is protoGermanic and has undergone lots of changes too Shakespeare writes in a language that is very different from what is spoken today Due to semantic drifts Shakespeare translated to modern foreign languages can often be easier to understand Eg wherefore meant why not where wit meant knowledge not as in witticism humor is then fragile Jane Austens language is also different from ours eg you are come at last or it would would not it is perfectly normal in her time Archaic Chinese evolved as well several branches of modern Chinese can sound like completely different languages 7 major Chinese languages in mandarin cantonese shangainese fujianese taiwanese gan xiang hakka language kejiahua In English present progress tense vs present habitual tense is pretty unique nice the ing form of a verb like I am eating is not present in languages like French or German In Semitic languages of AfroAsiatic language family Hebrew Arabic Ethiopian etc Triconsonantal root with vowel variations is rather unique In Japanese Southeast Asian languages and Chinese some features include numeral classifier sounds like la ba me to help convey meaning no distinction between he she it in sound generally more contextual than English ProtoIndoEuropean Studies showed the similarity between Sanskrit Latin and ancient Greek none of these are spoken now They trace back to the same ancestor a protoIndoEuropean language which is the ancestor of most languages in Europe except Hungarian Basque Finnish and a few others Iran and upper twothirds of India lower third was Dravidian language family This was not a written language and its original forms can be deduced from variations in todays descendant languages sound and more so in grammar ProtoIndoEuropean is thought to have started from Kurgan language of southern Russian steppes others may claim Anatolia Turkey as genetic traces would suggest as well as certain patterns living descendants of this language tend to have eg common words in descendants for horse wheels but not oak vine or palm trees Germanic branch of IndoEuropean has common deviations from Latin including p f pater to father t th etc This is described in Grimms Law Latvian and Lithuanian the Baltic branch of IndoEuropean is thought to have undergone more conservative changes and remain closer to the IndoEuropean root More so in Slavic languages too Armenian and Albanian although IndoEuropean in origin have borrowed more from neighboring languages and have fewer words in common with the other branches of IndoEuropean languages Comparative reconstruction decuding the original form of a protoIndoEuropean word from several modern day extinct descendants The general methodology seems to combine the more uncommon features from later variants some common forms of decay patterns of change have been noted as those especially if appearing in multiple descendants in different geolocations are more unlikely to be the results of coincidental change the multiple descendants all went through Tracing language families Tracing the source of a language family usually the area with the largest diversity Austronesian languages spoken east of Australia Polynesian Indonesia Malaysia and Madagascar is thus thought to have originated from Taiwan as three of the four sub families in Austronesian have their presence in Taiwan Similarly Bantu languages Swahili Xhosa etc spoken south of the Sahara are thought to have started from Cameroon and east Nigeria Khoisan languages click languages spoken in southwestern Africa may have been the descendent of the first human language as its families are very diverse and the raison detre of some features like clicks would otherwise be hard to deduce Native American languages are extremely diverse in regions like California and further south but not in Alaska even though Archaeological evidence points to native Americans having migrated across the Bering Strait This correlates with climate change theory in that native Alaskans languages are those spoken by the people from down south who repopulated that area after the ice age Cases for and against a protoworld language Language dates back 150k to 50k years writing only dates back 6k the further back the more difficulty in comparative reconstruction The protoworld language theory may be on to something in that there might have been one language that was the ancestor of all or a protoeuroasian ancestor but so far the theory and deduction seem shaky Some similarities are purely by chance eg in English and Japanese while others are harder to explain eg a dialect of Nepal and a Papua New Guinea language Often times a grammatical structural similarity demonstrates a higher likelihood of two languages sharing the same root than pure pronunciation coincidences Dialects Each language is a bundle of different variants What became canon for a country is often times more geopolitical than intrinsic to the language itself Italian of the Tuscany variant English of the London variant French of the IledeFrance variant became canon not because they are superior to that of Sicily Kent or Provencal in any way rather from decisions of the past eg nationalism in forging a unified national identity one particular variant is picked and standardized that of the ruling class with the most political military power became canon Such canon changes over time court Russian was once Ukrainian due to the power center being in Kiev USSR time court Russian becomes the Moscow dialect and Ukranian was considered a peasant dialect fast forward to modern times where Ukrainian is considered its own language Written records suggest 500 years ago it wouldve been really difficult for a London English speaker to understand a Kent English speaker even though the grammar underlying structure are very much shared the pronunciation can be fairly different Similarly those we consider different languages today sometimes sit better as variants of the same bundle Scandinavian languages Swedish Danish and Norwegian are quite similar to each other Erdu and Hindi are quite similar grammar and sometimes vocabulary but backed by different writing systems Erdu uses an Arabiclike writing Hindi uses Devanagari writing system and considered different due to historical enmity Moldovan and Romanian are similar when Moldova was an SSR of the Soviet Union its tasked to come up with a Moldovan language backed by Cyrillic alphabets as opposed to Romanias Latin alphabets but the underlying are very similar On the other hand different seven dialects of Chinese are often as different as French Spanish is Or as different as Arabic spoken in Morocco vs in Syria In the Chinese and the Arabic case the writing system is the same but pronunciation can be vastly different and sometimes even grammar They are considered dialects more often due to geopolitical nationalism reasons than anything intrinsic to the language itself Dialect continuum its sometimes hard to decide the line between one dialect or another as each one differs with the one next door only by a little but looking far enough the two become distinct enough These then become a collection of dialects whose diversification happens on a continuous scale Diglossia A condition where two languages or two variants of the same language are used in the same geolocation by the same people usually divided into a highlow variants Not that the high is superior in any sense just that having different variants spoken in different contexts is a common phenomenon Examples include standard Arabic as the Quran is written in and Egyptian Arabic which is used colloquially and very different Hochdeutsch and Swiss German English h and French l in old Quebec French h and Russian l in Anna Karenina in English itself where some words are more formal while others more colloquial but they mean the same thing kids children bag parcel go back return etc Some states have triglossia eg Java Island where high middle and low Javanese may sound very different and used in different contexts Diglossia is often the effect of one version of the language being written in works of great importance and that version becomes fixed Uusally the H version is a historical snapshot of the language Eg standard Arabic being what the Quran is written in and Egyptian and other variants of Arabic kept on changing Writing slows down the pace a language changes it might be easier for us to converse with Shakespeare than him with a person in the 11th century The H is not necessarily always more complicated than the L it is so in some cases eg spoken French dropping ne in nepas but in spoken Chinese dialects there can be more tones than Mandarin Writing is almost orthogonal to speaking the former being something one goes through formal training to acquire and the latter being more of a biological instinct The written language is almost always very different from the spoken one more composition more structure bigger vocabulary longer etc The start of Hebrew Bible may be a good example of when writing mimicing the spoken language as the former was not yet too developed The written language in Gibbons work on the other hand would demonstrate a style that is very distinct from a spoken language Not that either is superior in any way as both can be elegant English as it is today is influenced by rules people like Lowth introduced somewhat arbitrarily Examples include peculiarities like you were was arent I am not I to whom to who meaning of double negative aint got nothing These peculiarities are often based on their notion of Latin and Greek being the superior languages any change decay in a language is undesirable and that grammar should always be logical Borrow and mixing Languages borrow and mix both words and grammar English in particular is a bastard language in the sense that most of the vocabulary today is borrowed 1 percent from old English and the rest from old Norse dating back to the Viking invasions Norman dating to William the Conqueror and Latin Greek dating to later when English became a language of learning The shorter and more widely used words often are different from other languages and rooted in old English while the longer ones for something specific and complex are usually quite similar with French etc This borrowing creates interesting effects like doublets of words and the high low variant eg beef and cow pork and pig There isnt a language quite like English having a level of similarity like those between different Slavic languages or Portuguese with Spanish the closest is probably Dutch but thats still a long way off Grammar too are borrowed An example being the IndoEuropean language like Hindi coming into contact with Dravidian language in the south and being impacted by its SOV ordering Codeswitching switching languages midsentence following a particular set of rules is a sign of a bilingual person with dual cultural background and another process in which grammar of one language can be introduced into another and new languages be born Eg a language in Ecuador that is a mix of Quechua and Spanish How the modern education teaches languages books on grammar memorizing words speaking after some teacher is not how language has been learnt in the past And the learning process of hearing and repeating in the past presumably contributed to the much borrowing and mixing we see today as well Languages mix and borrow in a symbioticlike fashion The branches of a bush mix and form a net Language areas where languages of different family draw features from each other and became closer in terms of grammar and words An example is the Bulkan area where Romanian is of Romance subfamily of IndoEuropean Albanian of Albanian subfamily and Bulgarian of Slavic subfamily yet Romanian adopted having thea behind the noun unlike other Romance languages but like Albanian and Bulgarian got thea in the first place which is unlike other Slavic languages Another area is Sinosphere where TaiKadai Laotian and Thai AustroAsiatic drew from SinoTibetan Complexities in languages Languages naturally develop complexities that are not quite necessary for being able to clearly express For example has done perfect tense in English event happened in the past having impact on current a peculiar feature to mostly European languages gender masculine and feminine forms factual marker needing to differentiate a fact you see heard or hearsay with an ending to the word etc Such usually comes from grammaticalization There is no defined standard what is needed to clearly express in a language but each language picks up a few features such as tones endings factual markers tenses etc All languages are complex but not equally so The case usually is that the more secluded the language the more complicated it gets An example being Tsez a language spoken by tens of thousands in the Caucasus where the sound involves waggling the back of your throat has four genders and variations on verb and adjectives when they dont start with a consonant Children has amazing imitation skills in picking up these complexities in languages and beyond a certain point such a language would be very difficult to learn Languages simplify due to external influence eg a particular group of adults learning the language Real Javanese was formed around the time of different ethnic groups coming to learn the language as was Mandarin Chinese which was deemed simpler than Chinese dialects for its reduced variants in tones and possible consonant endings Mandarin came around a time when groups of Tibetans Mongolians etc adults came to northern China to learn the language The Mandarin simplification was not as extreme as that of Real Javanese ProtoGermanic and English German English Dutch Faroese and Frisian are derived from protoGermanic family One of the characteristics of this family compared with other branches of IndoEuropean is its emphasis is usually on the first syllable making endings particularly vulnerable to being dropped Modern English keep very few case endings including s and more commonly used ones like him whom etc ProtoGermanic traces two thirds of its words back to protoIndoEuropean and the other third is unknown including words like sheep One theory attributes the rest to Semitic influence There were records of Semitic speaking sailors in Northern Europe Some similarity in words exist and sometimes the verb transformation reflects the triconsonantal root with vowel variations seen in Semitic languages eg think thought Grims law could also be related with Semitic influence Old English is much like German also being SOV the change from old English likely was due to invasion of old Norse speakers They settled in learnt the language halfheartedly and following a learning methodology very different from todays and resulted in changes simplifications in the English grammar Language and culture psycology of a people SuppearWarf hypothesis claims grammar channels the way people think This is luring but unproven The current belief is that language is largely not indexed to culture and its transformation is unconnected with culture Warfs example was standard average European language Germanic Romance languages vs native American language Hopi The latter has one word for all flying objects except birds differentiates water you use and water you see doesnt have tense which supposedly reflects native Americans cyclical view of time The theory largely thought the likes of English being crude and Hopi sophisticated the typical trend in linguistic study of his time Examples for the theory includes empirical observations but not proofs Honorific distinctions in Japanese Korean reflecting the respect hierarchies in these cultures Also more often than not French speakers think of tables as female in cartoons due to gender markers in the language Speakers of a language that does not differentiate bluegreen glu seem to take more effort differentiating these two colors and those who have two yellows take less effort to differentiate different yellows Problems with the hypothesis include Warf being wrong about Hopi whose view of time is not cyclical but recordbased and the language does have some time markers And counter examples include inalienable possessive marking usually not found in languages spoken by the capitalist world but rather those in the Amazon jungles Another being fly crawl float are all move with different particles as opposed to their own words in Navajo a traditionally nomadic people Navajo differentiates handling of objects depending on the shape and form of something this fact did not seem to make the speakers more likely to differentiate objects by form and shape as opposed to color and size Something else does The theory of languages reflecting culture has never been proven Pidgin A particular group of people learning a language to get by also resulting in a much simplified combination of the two languages Them making systematic mistakes which evolved into implicit rules in the bastard version of the language This includes Cantonese traders speaking English where Pidgin came from 18th Century native American English a RussoNorsk language spoken by Russian traders spending their summers in Norway The RussoNorsk language in particular has no markings verb conjugations one preposition no gender very simple pronunciation 300 words with a few for different alcohol Creoles Every language traces back to something and Creoles are instances where new language gets created from pidgins Note that Creole doesnt specifically refer to Creoles of European and native American origins When pidgin speakers do not go home and keep speaking their pidgins over time due to the need to express themself in nuanced ways a language gets built back up from stripped down versions that is a pidgin having a full language is part of being human Example include Tokpissen business talk in Papua New Guinea its Lingua Franca British South Sea plantation brought over population from New Guinea and this language is developed propagated back to the islands and became a way people from different islands speaking different aberiginal languages can communicate Lousiana French Creole Haitian English Creole Guyana English Creole West African and dying Indian Portuguese Creole Uganda Arabic Creoles are all examples of such Most Creoles are created from colonization and slave trade They differ from dialects of the language in that the grammar and how the vocabulary are used are different enough that listeners to a Creole cannot tell what the original language was and how it evolved to the point of the Creole Saramarkan Creole in Suriname Carribean Creole resulted from slave trade has a hybrid European vocabulary Slaves on the planatation run by the English then the Dutch with a mix of Portuguese Jews fleeing the inquisition and an Angola West Africa influence developed this language It developed simple tones something no pidgins have a particular grammar that involves different forms of to be and nuances that are hardly necessary for conveying meaning unambiguously and a marker indicating something fallen being thrown finishing its trajectory Hawaiin English Creole has a grammar that developed from the Chinese Japanese and Phillipino planatation worker pidgin Bioprogramme hypothesis Bickerton claims if there is a universal grammar Creole languages would reflect this grammar without all the complexities of a traditional language developed over time almost all of which has eg either tones or prefixes suffixes This is again controversial Creoles are often times more streamlined than traditional languages that developed maybe accidental gunks over time and could reflect what a language needs or even the first language if there was one Creole is a continuum concept as languages and dialects there is the language that gave it its words a deepest Creole and many variants in between Afrikaans is eg more streamlined than Dutch eg no gender but not a full Creole Black English A dialect of English not any bastardization nor a Creole or an African language Slangs are just the icing its also a unique sound system th and f and grammar dropping to be be being a habitual marker not a present marker Language death Today we are at a point where there are fewer languages than any other point in human history Over 10k languages became about 6k and 96 of worlds population speak one of the big 20 languages Chinese Spanish English Hindi Arabic Portuguese Bengali Russian Japanese Lahnda Punjabi Javanese German Korean French Telugu Indian Marathi Indian Turkish Tamil Indian Vietnamese Erdu Projection suggests 500 of the 6k languages would remain in 2100 Among the 300 native American languages over 200 are not passed on or spoken by anyone any more The process of death for a language is usually it reverting to pidgin vocabulary shrinks structure melts away finally it devolves into dust think immigrants children It takes about one generation for a language to not be passed on and die Why should one care Is diversity always nicetohave Languages may project things abbout our culture and mind assumption which is a complex issue Language revival artificial languages Once it needs revival its already dead Known efforts include Celtic languages like Welsh Gaelic Irish Gaelic Breton New Zealand Maori Hawaiian Mohawk In the near future chance of all people speaking English as their first language is very unlikely for most as a second language To preserve all 6K languages is also unlikely urbanization hastened the death of smaller languages due to them not being written and sometimes not seen as a language Also languages are hard especially dying small ones in reality there is little context to use them Hebrew saw a successful revival effort from being only for liturgical uses to an everyday language this has to do with Israel being a special situation and Hebrew being well preserved in literature Artificial languages including sign languages are created since 1800s with a universal language goal in mind Most have died Volapuk invented by a Bulgarian priest is one such example based on Romance and Germanic languages and very complex The creator thought the complexity in languages a necessity and not accidental Esperanto is still sortof alive this day with 1 million speakers Its based off of the creator witnessing animosity between Russian Polish Yiddish and Germanspeaking ethnic groups Creator also has the notion that Romance and Germanic languages should be the base for a universal language Esperanto is much easier with 16 rules nouns end with o adjectives end with a verbs with i present tense with as past with is future with os conditional us command u etc Finale The series try to focus more on how change is inherent to a language with the opinion that systems are more important than collections and individual etymology Languages are dynamic symbiotic and hallmarks of being human"},{"title":"\"The mythical man month\"","href":"/notes/the-mythical-man-month","content":" The mythical man month Why is programming fun Joy of making things Making things useful for other people The fascination of fashioning complex puzzlelike objects of interlocking moving parts and watching them work in subtle cycles Joy of always learning sprung from the nonrepeating nature of the task The delight of working in such a tractable medium Its fun because it gratifies creative longings built deep within us and delights sensibilities we have in common with all men The woes of programming One must perform perfectly Other people set ones objectives etc Dependence upon others Finding nitty little bugs is just work Work over which one has labored can be obsolete upon before completion Menmonth as a metric men and months are interchangeable commodities only when a task can be partitioned among many workers with no communication among them eg reaping wheat or picking cotton which is apparently not true for system programming Authors usual estimation for software projects planning testing unit testing and early integration test integration and system test Adding manpower to a late software project makes it later Brookss law Team division Best performers and worst performers may have a 101 productive ratio Many would prefer a small team of elites Though the cruel fact is for a system large enough albeit more efficient it would probably still take too long for the small team of elites making the product obsolete by the time its complete One effective way could be to organize the software team as a surgical team Mills concept The surgeon chief programmer defines the specs designs the system and is able to do any part of the work Needs great talent and experience The copilot is the alter ego of the surgeon table to do any part of the job but less experienced His main function is to share in the design as a thinker discussant and evaluator He serves as insurance of disaster to the surgeon The administrator surgeon is in charge but his effort should not be wasted on money personnel space etc The administrator takes care of that The editor edits the documentation the surgeon writes An administrator secretary and an editor secretary The program clerk maintaining technical records The toolsmith The tester The language lawyer master of the programming language in question The whole system is the product design of one mind or at most two acting uno animo As opposed to the conventional team where each part of the system is the design and implementation of one person Specialization of function and a lack of the division of the problem and a superiorsubordinate relationship are key in this concept In a larger team the conceptual integrity of each piece should be maintained Meanwhile a sharp distinction need to be made between architecture design and implementation Aristocracy democracy and system design Conceptual integrity is the most important consideration in system design as determining factor of a systems easetouseness Designing or architecturing especially setting the external specifications the manual of the product is not more creative than implementation the design of implementation The former process is an aristocracy which requires no apology Finishing architecturing does not necessarily block implementation design efforts The secondsystem effect What happens when architect proposes something more than the implementation can achieve resolved by two parties communication An architects first work is apt to be spare and clean He knows he doesnt know what hes doing so he does it carefully and with great restraint Once done building the first system this second system is the most dangerous a man ever designs the general tendency is to overdesign the second system using all the ideas and frills that the were cautiously sidetracked on the first one and extra caution is usually required when designing the second system Passing the word The manual describes what the user sees does it should refrain from describing implementation details whose freedom should be left to the implementers Pros and cons of formal definition or implementation as definition prose definition in the manual Having effective weekly architecture meetings annual supreme courts for remaining minor decisions Telephone log QA of the architect Why did the Tower of Babel fail Communication is key Communication happening informally via meetings and via a workbook Workbook keeps the external specs the internal implementation design and documentations etc Treelike organization in large software systems Possible relationship between the producer and the technical director Thinkers are rare doers are rarer and thinkerdoers are the rarest Calling the shot Effort required is roughly proportional to size of program15 Ten pounds in a fivepound sack Size control budget memory resident space as well as harddrive usage define exactly what a module should do as you place limit on its size the system architects must maintain continual vigilance to ensure continued system integrity Programming manager totalsystem useroriented Representation of data is key to managing spacetime tradeoffs the essence to programming The documentary hypothesis Required documents What objectives specification first to come last to finish When How much Where Who Plan to throw one away Prototype Be prepared to redesign and reimplement Plan ahead to build a throwaway you will anyhow Design and be prepared for change Structure an organization for change Program maintenance is an entropyincreasing process Hence at some point itll be a one step forward and one step back process Sharp tools Toolmaker of each surgical team tools documentation system performance simulator Gradual adoption of highlevel languages and interactive programming The Whole and the Parts General paradigms to avoid bugs Testing the specification externally Topdown design Structured programming avoid gotos Hatching a Catastrophe Milestones need to be concrete Fuzzy milestones boosts chronic schedule slippage which is a morale killer and leads to catastrophe One characteristic of programming teams hustle running faster than necessary moving sooner than necessary trying harder than necessary The other piece is late anyway is no excuse PERT chart critical path scheduling identifying scheduling dependencies Plans and Controls group on a large software project watchdog of the schedule Boss those higher than the first line managers should encourage truthful status reports and try to refrain from acting on them unless necessary The Other Face What documentation is required Overview purpose environment domain and range input and output functions realized and algorithms what it does input and output formats operating instruction how to use and what to do if abnormal options what choices are given to user running time accuracy Every program shipped should have small test cases that serves as proof of working More thorough test cases should include mainline cases barely legitimate cases edge cases within the domain of input data barely illegitimate cases Flow charting every detail statement is not useful flowchart system diagram on a higher level could be better for describing the internals of a system To be able to reasonably document something we should try to minimize the burden of documentation Comment your code name your variables structure your code so that its easy to read selfdocumenting code Document your code as you are writing it No silver bullets Claim no single improvement in technology management would cause the software development productivity simplicity or reliability to increase by an order of magnitude The making of a great designer"},{"title":"\"2011 july in gepai\"","href":"/posts/2011-july-in-gepai","content":" 201915 458 5 7 confidencecourage 7 20117B 20088 5 7 "},{"title":"\"2018\"","href":"/posts/2018","content":" 2018 December 23 Hudson River Trading Library Floor 58 4 World Trade Center 2018 has been an eventful year Leaving much to be pondered and appreciated Three visits to Chicago Paseo de la Reforma and Osiris As we wandered by the snowcovered shores of Lake Michigan in January I wouldnt have known two more trips to the same city await me just a few months away A much welcomed coincidence as most have been Clouded by a sense of crisis and self doubts there was a window of about a month that I said yes to every recruiter who reached out 7 interviews were scheduled 5 of which onsite And by the grace of Lady Fortuna 5 offers ensued Some from companies that I havent previously heard but gradually learnt to appreciate IMC Akuna in Chicago and Zoox in the valley Others perhaps beyond my dreams Google and Hudson River Trading In the latter of which I currently sit overlooking downtown Manhattan on a starless Sunday night right before Christmas and this time again in a different mindset I rarely thought myself a strong candidate having less than 300 leetcode questions under my belt to start with It now seems laughable how I frantically checked my phone on the way back from Teotihuacan to Ciudad de Mexico expecting the decisive call from Google Pacing restlessly up and down Paseo de la Reforma looking for a quiet spot to pick up the call and desparately trying to take in all the details of feedbacks knowing that I have made the cut became fond memories of the trip along with the foolhardy effort of pointing at the stormy skies to capture the weapon of Zeus and enjoying a hearty meal under the calm morning sun on the following day Two weeks later I cleared an entire day of interviews in World Trade Center In Room Osiris overlooking the Statue of Liberty where in between the rounds I had the leisure of taking in panoramic views of the Hudson River glittering in dazzling summer sun Why they named the room after the Egyptian deity was beyond me but what a fitting name nonetheless him of rebirth and of Nile inundation thus of life and harvest Arguably I find it hard to judge if Im worthy of the harvest thus sown Prudence often advised against judging especially when not knowing the complete picture or being able to perfectly assess the variance in the minute details Worthy or not I am grateful for everything I was gifted with and see myself willing to commit my passion and efforts into what is deemed right May our path stays guided by prudence and such willingness last 22 Vain and jealous as it may sound the 22 was my last straw if Im to be completely honest with myself The choice and the mortal fear"},{"title":"\"2019 on habits\"","href":"/posts/2019-on-habits","content":" On Habits Bookkeeping Often enough we found ourselves not being able to pick up notes or research results done longer than a few months ago To ease future lookup and improve future efficiency we should follow certain bookkeeping conventions Work logs research results profiling outcome historical binaries should go into designated location on a shared drive or public git with personal projects with folder names indicating subject and style of work done each folder should have a readmemd recording what this is and why what we tried the current status results as well as comments on anything in the folder that is not immediately obvious These should include failed experiments where we should also record why the idea is given up what the conclusions are and current plan is It seems good to also have one centralized and shared place to record overall status tracking and projects we wanted to take on for atwork and personal ones The scale and time horizon of these nowadays appear big enough to justify another and hopefully last formal bookkeeping attempt Coding design habits A common theme we are seeing more often with personal work is having to rework things previously done procrastinating too long in getting simple features in eg meaningful logging using designated modules division of components meaningful argparse or opting for brute force approach as opposed to setting tools up for longer term eg consistent autoformatting Software architecture recommendation we received was to generally prioritize going well over going fast and my past behavior also seemed to lean generally towards the latter Then as an overall principle in personal projects I should consider spending more time on design think and research more before you start writing knock out problems we consistently get confused about follow known best practices and if something takes too much effort to follow invest into ways of automation This echoes back to an earlier realization regarding what to do when encountering unfamiliar vocabulary in English literature Finding a balance seems nontrivial yet the tendency of glossing over seemed to have harmed overall efficiency and quality in recent observations Knowledge compaction consolidation It seems almost cliche that we often see ourselves repeatedly making similar realizations Completely forgetting and relearning something from scratch is fine and often times would come with new and improved insight but having to relearn too often or having knowledge retained in memory for too short appeared to have hurt our overall effectiveness This appears to be a compounded effect of the following subpar bookkeeping hoping to be better addressed in item1 not understanding the subject matter well enough in the first place hoping to be better balanced in item2 not having an established compaction schedule Historically we had issuetracker implemented but the habit was never formed and the strict curve in the first iteration appeared too much overhead to actually implement Compaction often seems less cool than picking up new things which makes it more critical for us to establish well thoughtout procedures to carry them out Personal finance If ones life would be long enough for law of large numbers to kick in presumably following an established hopefully backed investment strategy would pay off This is something we should get into habit of doing for longer term our financial well being Until dollar cost averaging on a total stock index has been found to be ineffective should we just follow this strategy Health Ergonomics exercise cleanliness tidying up your room good sleeping habits all matter Think in the longer term invest in the future Daytoday Empirical observation would suggest most biggest factors in loss of efficiency are usually distractions and illness Distraction comes in the form of not wanting to start and often correlated to not having rested well enough Itd be useful to identify instances of each and regulate our behavior in reasonable ways"},{"title":"\"Addressing self\"","href":"/posts/addressing-self","content":" Addressing self Nov 10 2018 Preface It always surprises me how futile the attempts were to refrain from looking back especially on a fine Saturday morning in office overlooking the Central Park and having the entire floor to myself Often times the thoughts revolve around how much has changed and the naivete of my past and current self that the regular feeling of deja vu started to convince me I was trapped in a labyrinth making little progress in terms of real realizations Perhaps such is the process of making realizations however we dont enter the labyrinth emptyhanded We dive in this time equipped with a ball of thread like what Theseus was given in an attempt to jot down the fine details of our thoughts through which we derive the core values and principles External conditions Recent events reinforced the realization that much of my fate is not in my own hands For the first time in my last five years as an immigrant to this country I saw how limited my freedom is in pursuing what I deemed worthy due to the restrictions in switching employers and traveling abroad This may be a moment when I should swallow my pride fear and anxiety and accept it for what it is as the serenity prayer would preach On one hand to live and learn is but the best option we have and on the other hand there is something to be salvaged out of every misfortune and it is exactly through such we evaluate ourselves steer our goals and harden our resolve In these endeavors know that we do not bow to the whims of external conditions I hold firm the belief that ones true excellence is decoupled from external conditions The goal is to master my own fate The way there is through the pursuit of knowledge And in the pursuit I aim to stay true to the virtues below and to weather the storms with faith and temperance Pursuit of knowledge We often times pondered what it means to really know something and among all the arts in this world what would be worth knowing The majority of my life was spent as a student yet among the things I was taught I cannot say I learnt most of them One can read a book audit a course labor in pratices yet all of these are ways of obtaining knowledge mostly irrelevant with evaluating whether our goal of learning something is achieved I often times see these feats from those considered masters of their arts To be able to explain it to a community both from a high level and indepth in order to cater to different interest and skill levels of the audience To be able to connect the dots when something seemingly irrelevant to the untrained eyes is brought up To be able to apply what they know to produce something tangible Perhaps this is better phrased as to internalize something and to grasp the underlying principles or confluences by the analogy of some crossdisciplinary scholars Our pursuit of knowledge is then characterized by the same feats Let us not rest in the comforts of having read something but one step further of pondering discussing applying and teaching what we have read as true tests of our internalization of such knowledge In the field of engineering this can come in the forms of understanding the design alternatives building something with it having seminal discussions with likeminded people and many others For all of which we should seek and create opportunities if we find them lacking When reflecting daily what we have achieved make sure to apply the same criteria of internalization Openmindedness The topic of what is worth internalizing is a subjective and mercurial one I appreciate the beauty in most trades be it various fields of liberal arts sciences and engineering skills in particular sports interpersonal relationships and many other techniques Such diversity is what makes here an incredible place for the curious whose efforts are fueled by the freedom to pursue these It is my goal to number among the curious To be ready and excited waking up each morn thinking about the new knowledge we are going to internalize and the new challenges we are going to tackle To take in think and act with an open mind and be open and flexible to change without losing sight of what we set out to achieve or failing to adhere to our principles To reflect strengthen what we did well and readily acknowledge where I was wrong and to not judge others for our differences in the perception of what is worth internalizing Compassion gratefulness and humility I was gifted with a loving and supportive family We were not affluent in our wealth but never lacked the means to afford food lodging or education Throughout my life I was fortunate enough to learn from many whom I admire and appreciate Such cannot be said for many others not for those from a poor rural area where I taught in 2011 and not even for my parents who grew up in turbulent times It is not due to their lack of endeavors the cases usually suggest quite the opposite but simply a factor of luck To such fortune and those who participated in weaving it we show gratitude And to cherish these that we arguably arent worthy of we work and learn hard and pay it forward Do not ridicule or easily discredit others it could be just a difference in habits belief temperament and priorities Most in life are burdened with their own struggles be it career health relationships and many others Do not think that the troubles you face are the biggest adversity or decide what few accomplishments you have deserve the attention and praise of all or boast what little you own and realize Listen attentively show compassion be supportive learn from them and heartily congratulate them for their achievements Greed vanity and ignorance often cloud our judgment do not fall victim to the temptations of such Courage determination and discipline Readily jump into actions Perseverance rituals habits The wisest men follow their own directions Fragility of life To clear the unworthy out of your mind and to learn like you will not be able to tomorrow Opportunities choices and tradeoff The beauty of computer science"},{"title":"\"Bloomberg 2017 2018\"","href":"/posts/bloomberg-2017-2018","content":" Bloomberg 2017 2018 11232018 The coming week marks the end to my first job as a fulltime software engineer working with Bloomberg LP In the last year and a half I was gifted with the opportunity to work in an incredible environment with many talented people for which I remain forever grateful I was fortunate enough to have forged a bond with a few among them hopefully one that will last beyond my now limited days in the company The decision Before transitioning from a student researcher to a professional engineer I did not picture myself looking for new opportunities or quitting a job in the first two years I knew there would be problems and things that I dislike but as engineers we are hired to deal with them and in doing so we develop our core skill set namely that of problem solving It would probably sound hypocritical that I claim I still hold such beliefs despite being a leaver in the first two years However there is often times a plethora of other factors behind a decision and I hope to justify mine from these ones below Growth As a junior engineer growing my skill set is the topmost concern There is a tactician and a strategist in everyone of us the tactician seeks the best approach to our battles at hand and throws our entire personality at them while the strategist looks ahead and behind carefully planning the next campaign making sure its fought at the right time and place so that we could reap the heftiest benefits As the strategist surveys the battleground he is not optimistic about the outlooks In seeking growth we strive for the areas we want to grow in to line up with our efforts at work Yet most of the time we find the work highly specialized in particular and minute areas that we doubt if anything we gain is of strategic value or could count as transferrable skills If we couple that with legacy frameworks inherited from a long past by software engineering standards the outlook dims further Legacy systems arent necessarily evil and everything has its lifetime Plus we were taught to be skeptical about the new and shiny technologies and to focus on grasping the fundamentals But add debt atop the legacy questionable design decisions from ten years ago friction in being able to effectively develop and iterate and the issue becomes worse Now this may make a great proving ground for a skilled veteran and Id be far too delusional to consider count myself among such Its comforting that the company is actively looking to address many of these but it also takes time to overcome the inertia Embracing change Being curious is mentioned as a highly desirable trait in software engineers by many sources Seeing the woes of our development today sometimes I cant help but wonder how others do it differently And what could be better than to experience them firsthand Without seeing a contrast its even difficult to appreciate what we enjoy here Over the year I saw myself becoming accustomed to bad practices and getting satisfied from just getting by And come to think of it this would be the exact state we fear settle for the suboptimal for knowing only the superficial or becoming stale The environment it assimilates us it lures us to its comforts dulls our senses and weakens our resolve I am aware the young are often criticized for their lack of perseverance in pursuing one thing for a prolonged period of time I myself may very well be subject to this blame Yet the other side of it is that we live but once the twentyfifth year of our lives perhaps the best period to expand our horizons and embrace change What I learnt C Financial market Woes in software architecture Soft skills Going forward It pains to state that from my downsampled observation quite a few who left are considered the best and brightest among us I mean no offense to those who stayed I admire your dedication and loyalty Nor dare I number myself among their ranks Now that we are given a similar opportunity to start fresh at a place well regarded by many I hope we make sure to treasure such Proactivity Deliver Critical path and prioritization Advocate for yourself sell and speaking up"},{"title":"\"Engineering\"","href":"/posts/engineering","content":" Recurring themes in software engineering This year marks the end of a decade since I wrote my first line of code and two years and a half since I became a professional software engineer Along the process some recurring themes are discovered lessons learnt realizations made and this summary intends to be a discourse on these observations on a metaengineering level ie a collection of commonalities and general methodology The hope is that when faced with an engineering problem one can see the alternatives easier make better judgment calls and think architecturally when it is needed Tradeoffs There is no magic It is all tradeoffs It is so between timespace in an algorithm biasvariance in statistics and noisereduction vs detailssharpening when post processing a photo As is with most choices a small company vs a big one moving fast vs architecting something well having an abstraction that covers all vs individual solutions tailored to each even being compensated more or less to a degree Anything reasonably sophisticated would likely involve making nontrivial tradeoffs and arguably this decisionmaking element ie design strategizing is where the most dynamics and excitement lie Know the approach identify its tradeoffs map them to first principles and be able to draw upon them when designing your own systems Prioritization is another form of tradeoff A practical system cannot be perfect in architecture Looking at MapReduce in 2019 perhaps few would think a fixed framework of mappers and reducers with a sorted shuffle step forced in between and materialization of transient states is anywhere near general enough for distributed processing workflow Yet it worked well and was general enough at least for a time for some particular workload ie building search indexes Know your use case and its bottlenecks and make tradeoffs from there Simplicity A complex system that works is invariably found to have evolved from a simple system that works The inverse proposition also appears to be true a complex system designed from scratch never works and cannot be made to work Simplicity comes from knowing what things our system needs to do and not being overambitious do one thing and do it well single responsibility plan for some changes but not all of them Unix programs principle are good embodiments of such as is a RISC instruction set GFS uses one inmemory master for directory storage and failover is tackled by a different component Simple to reason about and simple to operate Why make something more complicated than it needs to be There may be a valid reason for that to account for some possibility of future changes Yet such concerns often times are misguided into premature optimization We thought we need multithreading because we didnt understand concurrency well enough We thought we need a highavailability distributed NoSQL database because thats what everyone talks about We thought we need blockchain because we never understood our problem well enough That said there is a fine line here which a later section will cover How can a system be simple if the business problem it models itself is complex to start with Well we break it down into pipelines compositions of components and cheat by hiding the details of each behind abstractions Abstraction generalization and specification Abstractions make complicated things seem simple Its like coding in assembly C or Python or having TCP on top of IP Abstractions take freedom away from the user you are tied to the particular ways the abstraction approaches a problem and as a result performance is often sacrificed specific optimizations for some users particular cases cannot be made Performance is overrated Well shame on me for saying that as a person paid to build highfrequency trading platforms But this does seem the prime reason leading to premature optimization ie the likes of I thought this would run faster in hypothetical scenarios ABC Make datadriven decisions and know that for most of our cases something should be as fast they as they need to be anything faster than that might be making sacrifices elsewhere maintainability operability evolvability cognitive overhead etc think twice if those are worthy sacrifices Yet often enough they are worthy sacrifices And specification comes in As the problem evolves two uses cases thought to share the same logic start to diverge the abstraction becomes over ambitious a square is not a rectangle or slow Holes are punched encapsulation violated patches that hurt maintainability goes in and it becomes about time for specifications to be introduced and abstractions refactored These are parts of the natural life cycle of engineering and do not necessarily indicate a bad design architecture yet it is at times like these our barriers are best tested Dependencies and coupling Interface segregation Dependency inversion Plan for change Problems evolve Software evolves When choosing how to store your data did you consider how easy itd be for the schema to evolve When deploying your system did you consider how easy itd be to upgrade When modularizing your code did you consider what are the likely ways for it to evolve and how well each are insulated from changes in the other on the likely paths of evolution Are the ones likely to change at the same time for the same reason near each other Are the above considerations even important to your problem Do not consciously leave traps for the future person nor should you prepare for armageddon in your code Whats the criteria Think hard and experience will tell What experience brings When someone amazes us with their ability to think outside the box and to come up with innovative solutions it often times is their box being unfamiliar to us Before we learn to think outside a box learn to think inside one In some sense experience may seem like the ability to simulate the situation accurately without needing to have actually experimented or gauging the counterfactual Our repertoire of knowledge and toolkit is that box Knowledge comes in many levels Knowing certain API and what happens underneath the hood give you an edge Knowing how to troubleshoot an inhouse system gives you more Knowing their whys and why nots is even better and Being able to create new ones combining first principle and triedandtrues of others makes you an architect There will always be the young ambitious and eager person with just too much time on your team who will outdo you in the first few levels of knowing Yet the last ones come with experience the tradeoffs are firmly implanted through your training that you will deduce sometimes without having to experiment what will work and what will not Think hard at the boxes weve built and at the ones we are given dissect them take them apart and rebuild them in different ways and from such gain experience Devils in the detail With experience we make empirical claims Know that any such can be dangerous especially in that they often times come with assumptions we didnt fully realize ie the details When not sure about something experiment When sure about something it does not hurt to also experiment Do the mental exercise but always be prepared to get your hands dirty and write the code Undeterminism One level of indirection Lixia a prominent researcher in Internet architecture and my advisor once brought up the idea that one level of indirection is a powerful tool in engineering Over time we saw many falling into this pattern DNS offers a level of indirection for addressing such that application level need not hard code IP addresses which would be bound to a specific physical interface Normalization denormalization in database schema design is a level of indirection when storing enumerables eg neighborhood in someones profile do we store them as plain text in the user table or an ID that can be cross referenced with a dedicated mapping to plain text A message broker is a level of indirection to handle producers coming and going have them both talk to the broker as opposed to directly to each other A pointer reference is one level of indirection with these the underlying object can now be mutated inplace or shared by many There is however no universal answer to the question should we or should we not apply a level of indirection Common sacrifices we make include efficiency due to needing to follow an extra link for gains such as decoupling different layers unifying different representations and not needing to have changes propagate through the codebase Things to consider include if this should be modeled as a multilayer situation if there is one source of truth that everything else adheres to etc Anecdotally another thing she brought up as an important tool in our engineering kit is randomization example being TCPs random initial sequence number for crash recovery Im sure venturing more unto the undeterministic will yield more realizations for such Prototype and iteration MVPs and convincing Beyond coding Testability Maintenance Operational Documentation Communication Work ethic Advocation Seeing it through The first principles Rectilinear triangle Truth and knowledge Internalization of knowledge Routine and habits"},{"title":"\"July 2019\"","href":"/posts/july-2019","content":"July 28 2019 Hudson River Trading Library Floor 58 4 World Trade Center This week saw three core developers leaving the company due to performance reasons I do not know the detailed circumstances of each yet it always seems demoralizing to see folks being asked to leave be it a recent hire of two seasons or a seasoned hand of several years Back at school or in my yearandhalf at Bloomberg the thought of being forced onto something seemed so distinct that only now has it begun to dawn on me what that means for someone international and perhaps worse for someone who has not experienced a serious downturn This gets me pondering about my own work If nothing else being the latest member on a large team with degrading performance is worrying especially when my work is more individual and replaceable What is really the role of an engineer What are our core values What are the tradeoffs Which is the critical path from here for myself my work and the team and the company Engineers are problem solvers first and foremost What problem to solve Prioritize the one blocking the critical path One can bitch about code style violations poor integration testing nonexistent continuous integration all day but is that the weakest link Spotless code that does the wrong thing is still worse than a piece of crap which gets the job done When we complain about such perchance it is the one thing we are good at not the weakest link in our chain In other words we are twisting the problem to fit our strength not adapting to solve the problem Similarly one can boast skills in distributed system machine learning software architecture or simply writing good code but if they do not get you any closer to a solution what use is there of your training in such say profound art of dragonslaying Hence we should first realize the problem on the critical path the one that blocks progress Whose critical path Unfortunately in my line of work at its current stage that of my employers I am paid as an individual contributor to make the companys problems go away Great if they line up with my interest and priorities if not in the long run we should reconcile the two either leave for something different or bend either towards a blend of boths A mismatch here would be a common cause for stagnation in ones growth The reality in locating the weakest link however is often times murky as what one faces is always unique A true visionary may know the problem better than the client he serves with tact and resolve he would guide others into his view Yet being too stubborn in his pursuit he could become trapped in fixing any problem on a solution he knows well To navigate such ground the most useful seems to be to observe communicate and reflect Gather the facts apply our expertise and make a judgment call Outside work the same would apply only that we ourselves would be in control of the critical path what do we want to pursue where do we want to improve and why The balancing act The solution almost always is a compromise a tradeoff between multiple aspects or conflicting goals It is so between time vs space in an algorithm bias vs variance in statistics and noise reduction vs details when post processing a photo As is with most choices a small company vs a big one moving fast vs architecting something well having an abstraction that covers all vs individual solutions tailored to each Or personal traits being cautious or daring lenient or aggressive reaching out for help or diving deep oneself Know the problem know your tradeoff make a judgment call and know what you are sacrificing Know that the problem and our experience both evolve Evaluate your situation and reflect know when to dive deep and exploit also when to take a step back go wide and explore Values In finding the right balance we saw common traits shared by good problem solvers Face your worst fears Seek and embrance change Try Take risks No excuses Persistence Take a break Daily quarterly yearly Clear your mind Appreciate the problem appreciate others Learn and grow HRT has a large pool of talented folks It is perhaps time to acknowledge no matter how hard I try I will not be as good as someone else at things I care about Yet someone better in strategy may be worse in tactics another better in both may be worse in execution Find the right blend for yourself and the ground for you to shine Ebb and flow No matter how hard we try we fall into ruts Know where we have come from to gauge where we want to be Looking back the flows include first year of undergrad when we did well in school and spent the summer in memorable activities summer of 2013 in the US and the following year at remap during which we dived hard to understand NDN from the 2nd quarter at grad school till the first half year at Bloomberg when we established good rituals and laid the foundation for the next steps the last quarter at Bloomberg till the first quarter at HRT when we saw our skills sharpened acompanied by good fortunes in seeking a change The ebbs followed each flow where sophomore and junior year during which we saw excessive amount of time spent on games subsequent years at remap when learning slowed down to a slog and our vision of future became clouded after first performance evaluation at Bloomberg when our effort seemed misguided and the growth aspect seemed hindered after the initial thrill at HRT expires when job security started to become a concern Disatisfaction and lack of confidence in our skills started to rise Some among pessimism jealousy cynicism procrastination and a disinterest in everything typically accompany an ebb whereas confidence creativity and a stream of projects and hobbies usually come with each flow Inspecting our recent predominant mindset gives a good clue of the state we are in Understand its part of the natural cycle but seek to overcome it Reflect on the problems our approach and tradeoffs adhere to our guiding values Appreciate learn and grow Wake up with one problem we want to work on one ritual we want to strengthen today Go to bed satisfied knowing weve made good progress"},{"title":"\"Memory and bigtable\"","href":"/posts/memory-and-bigtable","content":"October 28 2019 Trump Plaza Jersey City A recent conversation drew me to realize the surprising similarity between the way our memory works and that of a log structured merge tree In my pseudoscientific language idea credit Sophia our shortterm memory would be like a memtable longterm memory would be like an SSTable sleep would be the process of committing a memtable to an ondisk SSTable it would also be the process where compaction happens our memory is keyed on some vague and flexible index and some operations seem incredibly faulty in that indexes clash and the values become intertwined in unpredictable ways the query process would be similar in that memtable is queried first and the latest SSTable the next latest etc memories like memtables and SSTables are appendonly in that we cannot unsee something already seen They are also immutable in regular flows not including the random faults of the mind once committed we may even also keep a Bloomfilter like structure as an optimization to the query process to be able to quickly answer something with that does not ring a bell at all the notes we keep are like logs in such a system to recover from eg a crash before commit from memtable wed need additional information like our physical notes other peoples memory of the same event can also serve as backup mechanism redundancy there definitely is also a layer of LRU cache that is associated to our shortterm memory too Like keeping hot content at the tip of tongue whats a good way of formalizing this Takeaways Sleep is important Consciously schedule improve review may help the compaction process We can probably create some interactive experience to model this process and bring in the idea of reference counting where something is forever lost in our memory history when nothing remembers it Or maybe this would be an excellent demonstration of a person inexperienced in one field trying to project his misguided thoughts onto another field where he thought hed be more knowledgeable in what bias is this called"},{"title":"\"Random realizations\"","href":"/posts/random-realizations","content":" Random realizations quotes from conversations with friends Although we are young and the cost of making suboptimal choices is relatively small but with each choice there still is a cost Now that I think of it miss the chance to go to Tsinghua CMU Google once we probably miss it for life The impact magnifies over time leading one down different paths in lifemaybe diverging farther from the goal we set out to achieve if there ever were one At some point one settles down becomes satisfied and loses the mettle When that day comes and I think back I just want to reassure myself I did everything I could and worked hard for everything I believed right At a game as well as at other things a big difference is whether there is a purpose in mind Today Ive a goal to improve my skills What do I want to learn and internalize today How do I achieve that Did I manage to achieve it There were times in games when I thought why dont you noobs get good then I realized its just different life priorities Some play to have fun others play to win Im in the middle of figuring out why I play and along that process we found a few things that might make a difference Now is just as good a time as any to do those After a while the absurd becomes the norm hourlong build times nonexistent coding convention hideous undefined behaviors floating around dependency loops and the like The silent desperation takes over and one starts to think it must be the same everywhere else and not worth the trouble to seek changes This too unfortunately is what comes with experience Zoom in zoom out dont lose sight of what we set out to achieve and embrace changes On how we do things and more importantly on how I perceive them and take them on With travel comes change habits uprooted and reestablished We may fear losing the key initiatives inspirations insights we just had or simply fear the unknown It is at such moments we should reiterate the core values we believed in what we set out to achieve stay confident in who we are and seek opportunities within the changes The difference often comes from that final push one that goes just beyond the critical mass and tips the balance in your favor One can give up thinking the tasks all but impossible or complain about the inability of others Yet know that more often than not failure comes from ones own resolve falling short or ones inexperience in seeing the merits and contributions of others Much like the timevalue of money using which we discount future cashflows to derive a present value there is a timevalue of knowledge skills achievements metaphysically all assets as well Different branches of knowledge have instrinsically different discount curves mapped to a multidimensional space of evaluation criteria The key would then lie in identifying the critical path and giving it all you got now You should know when to stop And when you are about to stop push one step further You never know if just one step would make all the difference if you dont take it The biggest asset of a company or perhaps any human construct is its people Make sure they are treated as such and make sure they are given the right environment and resources to realize their potentials Looking at my documents since 2013 there are at least three dozen named notes thoughts realizations plans and the like Most of them are scratches mind dumps and early studies that never came to fruition They struck me with the realization that almost none of my ideas now is completely new They all trace back to somewhere although the source may seem long forgotten Work on your bookkeeping taxonomy and consolidate often Learn like you will not be able to tomorrow That may actually happen Do your best to learn from them rather than be intimidated by them Reach out Ask Praise Appreciate Voice your thoughts Right wrong good bad its all a matter of perspective Keep an open mind Know and appreciate that your perspective may not be one shared by others Question and improve that of your own realize and appreciate those of others Maybe its this particular feeling of having ones mind inspired that drew me close as it longed for food for thought It almost reminds me of an old friend and the way she used to kindle ideas from these ashes Come to think of it perhaps I was naive and biased while she was right all along In hindsight perhaps her decisions were for the best and delivered in a very tactful manner while my thoughts and feelings misguided what I dont do well in convey your thoughts in a confident and clear way Some fear seems to have been holding us back I would think Im not a cynic not an inconsiderate person not overly vain and not terrible at certain things are those merely blissful illusions that the mind chose to weave for me When to settle for a compromise and when to aim only for the very best how we balance Learn something new different every 30 60 90 360 days Improve in certain ways Change forming habits a conscious influence and a dedicated effort This mind seems to have lost some of its most treasured liveliness creativity imagination daringness humor could one blame it on the burden of real life Did things really take a downturn since this job What exactly is this burden Immigration Work Future Desire for a breakthrough Routine Is this something we should actively work to thwart Our perspectives are often shaped by the limited knowledge and experience we have We take what we are used to for granted and any development longer than the lifespan or the radius of a human being can be hard for the particular being to comprehend Realize this and keep an open mind Sapiens Languages First try to make progress then seek breakthroughs Are you running away from your problems What better time than now to face and overcome them Much like a civlization an individual goes through periods of rise and fall ebb and flow Some started out brilliant at school early on in their career but later see their talents and potentials buried in the daily routine shackled by tradition Keep a mindset of change and growth and enjoy the process Probablistic statistical The unstructured life is not worth living for some Things we are systematically bad at De javu and is math useful is math olympiad useful Is skepticism trying to take the easy way out"},{"title":"\"Thanksgiving 2018\"","href":"/posts/thanksgiving-2018","content":" Thanksgiving 2018 11202018 It used to bewilder me why in China we dont seem to have a holiday dedicated to appreciating the God and those around us Perhaps we never celebrated a harvest or the arrival of new immigrants like the first Puritans did 400 years ago One so vital upon which the very survival of the colonies is contingent Ive not been one to express gratitude verbally or much to my distress not many other positive feelings or compliments either for that matter Yet I do find myself constantly reflecting on the people and events of the past Usually no matter if those brought joy anger or pain at the moment I experienced them by the time of the reflection I feel gratitude towards the much treasured memories we shared or the bitter lessons they taught It is almost once again this time of the year and in typing these down I would hope to make up for my lack of words No particular order would do these justice perhaps the cliche alphabetical one is but the best I can do To Adam and Jochen for their excellent advice on career and on what I could have done better To Buyu for putting up with my nonchalance and ambiguity which I am now ashamed of To Chongshan Dongjie Zhongxia and Jian for all the fun in our first reunion in four years And dragging me to a net cafe not something Im particularly proud of but another first nonetheless To Fei Li and Sergii for their willingness to help and share their knowledge and the time they generously invested in me To Janani for always being encouraging and comforting to talk to Often times I dont see myself share this much with friends A change for the better I suppose To Jiajun and Josh for sharing their knowledge and motivating each other on the similar path we undertook To Jian Ken and others of Jump for the many Friday nights we spent discussing papers technologies of mutual interest Perhaps one of those was my finest moment this year To Joseph David Erin Kevin Kai Kim and many others at HRT for the fun interviews and events Also for inspiring me to keep improving to remain humble and not get satisfied with my few achievements To Joseph for being an outstanding conversationist and lunch pal To Nikhil and Shawn for their encouragement and support on my decision to leave earlier A shame the way it turned out To Ning for the gifts and few interactions in a foreign office In the past year those days if not now were perhaps the ones during which I was most troubled To Nzo for always being there to bounce ideas with Even those evil unethical and silly ones Perhaps not many are lucky enough to share such an acquaintance To Ponjo for being a genuine friend someone to share happiness experience or simply programming techniques with To Sam Eduardo Rayna Jason and others who bombarded LinkedIn Inbox for reaching out and reminding me how much more there is to explore out there To Setty for the day hikes we went on the trips to Chicago and CDMX the arguments we had and the many sentiments we shared From my disappointment in Februrary the anxiety in April and May the exhilaration in July till September and the deep concerns now To Sophia for helping me realize how much I should treasure the opportunities now and wanting to work harder to be worthy of what luck has graciously gifted me with To Yao for keeping my ego in check at work To those I met in my travels this and the last year for making me feel at home in foreign lands To my parents always And to the many unnamed for your kindness patience support or simply your smile at a stranger in distress I was treated by this world with kindness for that I constantly remind myself to pay it forward I find myself again at a crossroads perhaps making questionable decisions Even if the path may turn out to be a dead end work hard learn stay humble and appreciate"},{"title":"\"Turkey 2019\"","href":"/posts/turkey-2019","content":" Islam A better place Hi phesiz ki Antalya dnyann en gzel yeridir Without a doubt Antalya is the most beautiful place in the world Having spent an entire day traversing from citys numerous beaches to vast Anatolian hills I can see Atatrks exclamation being quite reasonable Seeing locals young and old swimming in tranquil waters of the Mediterranean and basking on a rocky shore on a warm September day was not something I had expected when planning the trip nor was the delicate old city or the panoramic escalators leading to its natural harbor Yet the city did leave a brush of shadows upon my mind Coming back from a night photography session at the Duden falls my host Sahar Sara asked me to guess where she was from I would not have suspected her not being a native having experienced how diverse Turkish people can look When she brought up Iran something about her looks did click elegant and shapely very much like the few Persian ladies I met back in New York What do you think its like to migrate from 40 degrees hell to 20 degrees I hope to see you again in a better place someday she replied I thought to myself perhaps a better place I know not nor do I find it likely that our paths would ever cross again I offered to help in any way I can she smiled gently and asked only for my best wishes With that we parted and my last stop awaits Hers as well as those of many others would be an ordeal I cannot presume to comprehend I ask of myself to cherish what I was given to work hard to be worthy of it to listen and not judge to treat the world with kindness and help those in need Atatrk Varner"}]